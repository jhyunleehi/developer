# Elastic stack

## 개요

### 배경

* 2004년 샤이 배논(Shay Banon)은 Compass 라는 이름의 오픈소스 검색엔진을 개발하였습니다. 
* 2010년 샤이는 Compass를 Elasticsearch라고 이름을 바꾸고 프로젝트를 오픈소스로 공개하였는데, 곧 수많은 검색 개발자들에게 인기를 얻으며 급속도로 성장하기 시작했습니다. 
*  Elasticsearch 프로젝트는 2012년에 창시자 샤이 배논과 함께 스티븐 셔르만(Steven Schuurman), 우리 보네스(Uri Boness) 그리고 아파치 루씬 커미터인 사이먼 윌너(Simon Willnauer) 4인의 멤버에 인해 네델란드 암스텔담에서 처음 회사로 설립.
*  Logstash, Kibana와 함께 사용 되면서 한동안 ELK Stack (Elasticsearch, Logstash, Kibana) 이라고 널리 알려지게 된 Elastic은 2013년에 Logstash, Kibana 프로젝트를 정식으로 흡수하여 한 지붕 아래에서 함께 개발을 해 나가고 있습니다. 
* 2015년에는 회사명을 Elasticsearch 에서 Elastic으로 변경 하고, ELK Stack 대신 제품명을 Elastic Stack이라고 정식으로 명명하면서 모니터링, 클라우드 서비스, 머신러닝 등의 기능을 계속해서 개발, 확장 해 나가고 있습니다.

### Elasticsearch 

Elasticsearch는 Elastic Stack의 심장이다. 기본적으로 모든 데이터를 색인하여 저장하고 검색, 집계 등을 수행하며 결과를 클라이언트 또는 다른 프로그램으로 전달하여 동작하게 합니다.

  Elasticsearch는 뛰어난 검색 능력과 대규모 분산 시스템을 구축할 수 있는 다양한 기능들을 제공하지만, 설치 과정과 사용 방법은 비교적 쉽고 간편합니다.

기존 관계 데이터베이스 시스템에서는 다루기 어려운 전문검색(Full Text Search) 기능과 점수 기반의 다양한 정확도 알고리즘, 실시간 분석 등의 구현이 가능합니다.

#### 오픈소스

Apache 2.0 라이센스로 배포되고 있고 Elastic Stack의 모든 제품들은 (https://github.com/elastic) 깃헙 리파지토리 에서 소스들을 찾을 수 있습니다.

루씬이 자바로 만들어졌기 때문에 Elasticsearch도 마찬가지로 자바로 코딩이 되어 있습니다. 루씬은 하둡을 개발한 더그 커팅(Doug Cutting)에 의해 처음 만들어졌지만 Elasticsearch 엔지니어들 중에는 루씬 커미터들이 다수 있어서 루씬을 매우 깊은 레벨에서 다루고 있고, Elastic 사의 루씬 커미터인 개발자들이 실제로 루씬 프로젝트의 절반 이상의 기능을 개발에 기여하고 있습니다.

#### 실시간 분석

Elasticsearch의 가장 큰 특징 중 하나는 실시간(real-time) 분석 시스템 입니다.   Elasticsearch는 하둡 시스템과 달리 Elasticsearch 클러스터가 실행되고 있는 동안에는 계속해서 데이터가 입력 되고, 그와 동시에 실시간에 가까운 (near real-time) 속도로 색인된 데이터의 검색, 집계가 가능합니다.

#### 전문(full text) 검색 엔진

루씬은 기본적으로 역파일 색인(inverted file index)라는 구조로 데이터를 저장합니다. 루씬을 사용하고 있는 Elasticsearch도 마찬가지로 색인된 모든 데이터를 역파일 색인 구조로 저장하여 가공된 텍스트를 검색합니다.

이런 특성을 전문(full text) 검색이라고 합니다.

* JSON 문서 기반 Elasticsearch는 내부적으로는 역파일 색인 구조로 데이터를 저장하고 있으나, 
* 사용자의 관점에서는 JSON 형식으로 데이터를 전달합니다. 
* JSON형식은 간결하고 개발자들이 다루기 편한 구조로 되어 있어 색인 할 대상 문서를 가공 하거나 다른 클라이언트 프로그램과 연동하기에 용이합니다.
* 또한 key-value 형식이 아닌 문서 기반으로 되어 있기에 복합적인 정보를 포함하는 형식의 문서를 있는 그대로 저장이 가능하며 사용자가 직관적으로 이해하고 사용할 수 있습니다. 
* Elasticsearch에서 질의에 사용되는 쿼리문이나 쿼리에 대한 결과도 모두 JSON 형식으로 전달되고 리턴됩니다.
*  다만 JSON이 Elasticsearch가 지원하는 유일한 형식이기 사전에 입력할 데이터를 JSON 형식으로 가공하는 것이 필요합니다. CSV, Apache log, syslog등과 같이 널리 사용되는 형식들은 Logstash에서 변환을 지원하고 있습니다.



### Logstash

조던 시셀(Jordan Sissel)에 의해 탄생된 Logstash는 원래 Elasticsearch와 별개로 

* 데이터 수집과 저장을 위해 개발된 프로젝트
* Logstash가 출력 API로 Elasticsearch를 지원하기 시작하면서 많은 곳에서 Elasticsearch의 입력 수단 으로 Logstash를 사용
* Logstash는 JRuby로 되어 있습니다
* Apache 2.0 라이센스를 따르고 있어 자유롭게 사용이 가능합니다.

 Logstash는 데이터 처리를 위해서 크게 다음과 같은 과정들을 거치게 됩니다. : 입력, 필터, 출력



### Kibana

Elasticsearch는 Restful 한 속성과 JSON 문서 기반의 통신을 지원하기에 http 프로토콜을 이용해 어떤 클라이언트와도 손쉽게 연동이 가능합니다. 그래서 개발자들이 Elasticsearch와 연동되는 다양한 시각화 도구를 개발하였고, 이 중 라시드 칸(Rashid Khan)이 개발한 Kibana 라는 시각화 도구가 큰 인기를 끌었습니다.

  Kibana는 Elasticsearch를 가장 쉽게 시각화 할 수 있는 도구입니다. 검색, 그리고 aggregation의 집계 기능을 이용해 Elasticsearch로 부터 문서, 집계 결과 등을 불러와 웹 도구로 시각화를 합니다. 

#### Discover

Discover는 Elasticsearch에 색인된 소스 데이터들의 검색을 위한 메뉴입니다. 검색 창에 질의문을 통해 데이터를 간편하게 검색, 필터링 할 수 있으며, 검색된 데이터의 원본 문서를 확인하거나 보고 싶은 필드만 선택해서 테이블 형태로 조회가 가능합니다. 시계열(time series) 기반의 로그 데이터인 경우 시간 히스토그램 그래프를 통해 시간대별 로그 수도 표시됩니다.

#### Visualize

 Visualize는 aggregation 집계 기능을 통해 조회된 데이터의 통계를 다양한 차트로 표현할 수 있는 패널을 만드는 메뉴입니다. 영역차트, 바차트, 파이차트, 라인차트 등 다양한 시각화 도구들의 사용이 가능하며 여기서 만들어진 패널들을 조합해서 대시보드를 만들게 됩니다.

#### Dashboard

Visualize 메뉴에서 만들어진 시각화 도구들을 조합해서 대시보드 화면을 만들고 저장, 불러오기 등을 할 수 있는 메뉴입니다. 다른 메뉴들과 마찬가지로 검색 창에 쿼리를 입력하거나 시각화 도구들을 클릭해서 조회할 데이터들의 필터링이 가능하고, URL로 대시보드를 다른 사람들과 공유하거나 json 형식으로 내보내고 불러오기 등이 가능합니다

### Beats

독일에서 어느 두 개발자들이 Packetbeat라는 프로그램을 이용해 네트워크 패킷을 스니핑하여 Elasticsearch에 저장하여 모니터링 하는 시스템을 이용한 것, 개발 중이던 Logstash 원격 수집기 프로젝트는 중단되었고 그 역할을 Beats가 대신 실행하게 되었습니다.

#### 1. Metricbeat

  Metricbeat은 실행시켜놓기만 하면 시스템에서 실행중인 프로세스들의 정보와 이 프로세스들이 소모중인 CPU, Memory 등에 대한 상태들을 수집해서 Elasticsearch에 적재하고 손쉽게 이것들을 모니터링 할 수 있는 시스템을 만들 수 있습니다.

#### 2. Filebeat

  사용자들이 가장 필요로 하는 기능은 파일의 내용을 수집하는 기능입니다. Web log 또는 machine log 등이 저장되는 파일 경로를 지정하기만 하면 Filebeat은 해당 경로에 적재되는 파일을 읽어들이며 새로운 내용이 추가될 때 마다 그 내용을 Elasticsearch로 색인합니다.

#### 3. Libbeat

  먼저 Beats 팀은 처음 개발한 PacketBeat 프로그램에서 Elasticsearch로 전송하는 부분만을 따로 추출하여 일종의 공통 라이브러리로 만들었습니다. 특정한 데이터를 수집하는 부분만 코딩 하고 나면 데이터를 JSON 문서로 변환하고, 데이터가 유실되지 않게 관리하고, Elasticsearch로 전송하는 역할은 이 공통 라이브러리인 Libbeat이 담당하게 됩니다.

#### 4. Packetbeat

  가장 처음 존재했던 Beat이 바로 Packetbeat 입니다. 설치된 시스템에 유통되는 패킷들을 스니핑 해서 Elasticsearch에 적재시키는 기능을 합니다.

#### 5. Heartbeat

  다른 Beats 들도 이름에서 손쉽게 어떤 역할을 하는지 짐작이 가능한데, Heartbeat은 Beats 의 종류이면서도 심장 박동과 동일한 단어이기 때문에 재미있습니다. 역시 이름에서 유추할 수 있듯이 Heartbeat은 다른 프로세스들의 가동 시간 등을 모니터링 합니다. ICMP, TCP, HTTP 프로토콜 등을 통해 Ping 명령으로 원격의 프로세스의 가동 여부를 확인하는데, 동작은 단순하지만 다양한 시스템을 동시에 모니터링 할 때 매우 유용합니다.

#### 6. Functionbeat

  가장 최근에 추가된 Functionbeat은 요즘 유행하는 마이크로 서비스 아키텍쳐(MSA)와 같은 FaaS 클라우드 기반의 시스템에서 서버리스 프레임워크를 이용하여 클라우드 인프라를 모니터링 합니다. 다른 Beats 들과 달리 수집을 위한 데이터가 있는 시스템에 설치되는 것이 아니라 Lamda와 같은 기능으로 배포됩니다.



## 시작하기

### Downloads & install

#### 1. download

https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.0.0-linux-x86_64.tar.gz

https://www.elastic.co/kr/downloads/

```
$ sudo groupadd  elastic
$ sudo adduser  -g elastic elastic


$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.10.2-linux-x86_64.tar.gz
$ wget https://artifacts.elastic.co/downloads/kibana/kibana-oss-7.10.2-linux-x86_64.tar.gz

$ tar xfz elasticsearch-7.11.1-linux-x86_64.tar.gz
$ tar xfz kibana-7.11.2-linux-x86_64.tar.gz
$ You can use the `bin/elasticsearch-reset-password` tool to set the password for the elastic user.
$ ~/bin/elasticsearch 
```

#### 2. 설정

* elasticsearch.yml 설정

 ~/config/elasticsearch.yml  편집

```sh
$ curl  -XGET http://localhost:9200
{
  "name" : "ubuntu20",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "3ubvZgKaQRCRnzhDC5A5oQ",
  "version" : {
    "number" : "7.11.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "ff17057114c2199c9c1bbecc727003a907c0db7a",
    "build_date" : "2021-02-15T13:44:09.394032Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

*  cluster, node name 변경

```
$ curl  -XGET http://localhost:9200
{
  "name" : "ubuntu20",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "3ubvZgKaQRCRnzhDC5A5oQ",
  "version" : {
    "number" : "7.11.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "ff17057114c2199c9c1bbecc727003a907c0db7a",
    "build_date" : "2021-02-15T13:44:09.394032Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}

```



* kibana  :5601 port

```
$ ./kibana
...
  log   [13:40:17.648] [info][listening] Server running at http://localhost:5601
  log   [13:40:18.169] [info][server][Kibana][http] http server running at http://localhost:5601
^C  log   [13:41:06.397] [info][plugins-system] Stopping all plugins.
  log   [13:41:06.398] [info][kibana-monitoring][monitoring][monitoring][plugins] Monitoring stats collection is stopped


```

### file beat

#### 1. download, install

* download filebeat

```sh
$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.11.1-linux-x86_64.tar.gz
$ tar xzvf filebeat-7.9.3-linux-x86_64.tar.gz
```

#### 2. filebeat config

* config filebeat.yml

```yaml
output.elasticsearch:
  hosts: ["localhost:9200"]
setup.kibana:
  host: "localhost:5601"
```

#### 3. module config

* enable and configure data collection modules

```sh 
./filebeat modules list
./filebeat modules enable system nginx
```

* ngix.yml 설정

``` yaml
$ cat ~/filebeat/modules.d/nginx.yml 
# Module: nginx
# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.11/filebeat-module-nginx.html

- module: nginx
  # Access logs
  access:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    var.paths: ["/var/log/nginx/access.log*"]  

```

* system.yml

```yaml
$ cat system.yml
# Module: system
# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.11/filebeat-module-system.html

- module: system
  # Syslog
  syslog:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    var.paths: ["/var/log/syslog*"]  

  # Authorization logs
  auth:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:
```



* Setup assets

```
$ ./filebeat setup -e
...
Loaded machine learning job configurations
2022-03-03T10:18:36.448+0900	INFO	eslegclient/connection.go:99	elasticsearch url: http://localhost:9200
2022-03-03T10:18:36.449+0900	INFO	[esclientleg]	eslegclient/connection.go:314	Attempting to connect to Elasticsearch version 7.11.1
2022-03-03T10:18:36.450+0900	INFO	eslegclient/connection.go:99	elasticsearch url: http://localhost:9200
2022-03-03T10:18:36.451+0900	INFO	[esclientleg]	eslegclient/connection.go:314	Attempting to connect to Elasticsearch version 7.11.1
2022-03-03T10:18:36.518+0900	INFO	fileset/pipelines.go:143	Elasticsearch pipeline with ID 'filebeat-7.11.1-nginx-access-pipeline' loaded
2022-03-03T10:18:36.538+0900	INFO	fileset/pipelines.go:143	Elasticsearch pipeline with ID 'filebeat-7.11.1-nginx-error-pipeline' loaded
2022-03-03T10:18:36.538+0900	INFO	eslegclient/connection.go:99	elasticsearch url: http://localhost:9200
2022-03-03T10:18:36.540+0900	INFO	[esclientleg]	eslegclient/connection.go:314	Attempting to connect to Elasticsearch version 7.11.1
2022-03-03T10:18:36.602+0900	INFO	fileset/pipelines.go:143	Elasticsearch pipeline with ID 'filebeat-7.11.1-system-auth-pipeline' loaded
2022-03-03T10:18:36.628+0900	INFO	fileset/pipelines.go:143	Elasticsearch pipeline with ID 'filebeat-7.11.1-system-syslog-pipeline' loaded
2022-03-03T10:18:36.628+0900	INFO	cfgfile/reload.go:262	Loading of config files completed.
2022-03-03T10:18:36.628+0900	INFO	[load]	cfgfile/list.go:129	Stopping 2 runners ...

```

* Start Filebeat

```sh
# usermod -a -G sudo elastic
$ sudo chown root filebeat.yml 
$ sudo chown root modules.d/nginx.yml
$ sudo chown root modules.d/system.yml
$ sudo chown -R root  /home/elastic/filebeat/module/nginx
$ sudo chown -R root  /home/elastic/filebeat/module/system 
$ sudo ./filebeat -e
```

* view data in Kibana

http://localhost:5601



### Filebeat 다루기

* OSS 버젼을 다운 받아야  라이선스 이슈가 발생하지 않는다.
* https://www.elastic.co/kr/downloads/past-releases#filebeat-oss
* 

#### 1. Filebeat config

custom log 를 elasticsearch에 적제 하는 방법은 3가지가 있다.

1. FileBeat 내에서 dessect 프로세서 사용: https://www.elastic.co/guide/en/beats/filebeat/current/dissect.html 
2. LogStash를 설치하고 grok 패턴을 사용하십시오: https://www.elastic.co/guide/en/logstash/master/plugins-filters-grok.html 
3. ElasticSearch 수집 파이프라인 사용 https://www.elastic.co/guide/en/elasticsearch/reference/master/grok-processor.html 

나는 FileBeat내에서 dessect 프로세서를 사용하여 이것을 처리한다. 

##### 기본 log를 처리를 위한  module 

* filebeat는 기본적으로 common 서비스 들의 log을 분석할 수 있는  module들을 제공한다.
* custom log라면 입맛에 맞게 설정을 해야 한다. 
* filebeat.yaml 에  module를 사용하겠다고 정의한다. 

```yaml
# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s
```



##### Configure inputs

```yaml
filebeat.inputs:
- type: log
  paths:
    - /var/log/system.log
    - /var/log/wifi.log
- type: log
  paths:
    - "/var/log/apache2/*"
  fields:
    apache: true
  fields_under_root: true
```



* 기본 설정 : filebeat.yml

````json
filebeat.prospectors: 
  - input_type: log 
      paths: - /path/to/file/mylogfile.log 
output.logstash: 
  hosts: [ "localhost:5044" ]
````

##### 

* systemd  설정



#### 2. Processors를 이용한 Filter 처리 방법

libbeat library에서 제공하는 processor의 기능 (애네 들은 용어 정의를 그냥 입맛대로 막하네)

* reducing the number of exported fileds
* enhancding events with additional metadata
* performing additionanl processing and decoding

##### 1. 예시: Drop event 

* debug 로그는 제거

```yaml
processors:
  - drop_event:
      when:
        regexp:
          message: "^DBG:"
```

* test event를 제거

```yaml
processors:
  - drop_event:
      when:
        contains:
          source: "test"
```

##### 2. 예시 : json 데이타

* data

```
{ "outer": "value", "inner": "{\"data\": \"value\"}" }
```

* 설정

```yaml
filebeat.inputs:
- type: log
  paths:
    - input.json
  json.keys_under_root: true

processors:
  - decode_json_fields:
      fields: ["inner"]

output.console.pretty: true
```

* 결과

```json
{
  "@timestamp": "2016-12-06T17:38:11.541Z",
  "beat": {
    "hostname": "host.example.com",
    "name": "host.example.com",
    "version": "8.0.1"
  },
  "inner": {
    "data": "value"
  },
  "input": {
    "type": "log",
  },
  "offset": 55,
  "outer": "value",
  "source": "input.json",
  "type": "log"
}
```



##### 3. processor 정의

https://www.elastic.co/guide/en/beats/filebeat/current/defining-processors.html



#### Dissect string

* The `dissect` processor tokenizes incoming strings using defined patterns.

```yaml
processors:
  - dissect:
      tokenizer: "%{key1} %{key2} %{key3|convert_datatype}"
      field: "message"
      target_prefix: "dissect"
```



##### 예시: dissect 

```text
"321 - App01 - WebServer is starting"
"321 - App01 - WebServer is up and running"
"321 - App01 - WebServer is scaling 2 pods"
"789 - App02 - Database is will be restarted in 5 minutes"
"789 - App02 - Database is up and running"
"789 - App02 - Database is refreshing tables"
```



* processors

```
processors:
  - dissect:
      tokenizer: '"%{service.pid|integer} - %{service.name} - %{service.status}"'
      field: "message"
      target_prefix: ""
```

* 결과

```json
"service": {
  "pid": 321,
  "name": "App01",
  "status": "WebServer is up and running"
},
```



#### 4. elasticsearch.servics

```
root@ubuntu20:/etc/systemd/system# cat ./multi-user.target.wants/elasticsearch.service
[Unit]
Description=Elasticsearch
Documentation=https://www.elastic.co
Wants=network-online.target
After=network-online.target

[Service]
Type=notify
RuntimeDirectory=elasticsearch
PrivateTmp=true
Environment=ES_HOME=/usr/share/elasticsearch
Environment=ES_PATH_CONF=/etc/elasticsearch
Environment=PID_DIR=/var/run/elasticsearch
Environment=ES_SD_NOTIFY=true
EnvironmentFile=-/etc/default/elasticsearch
WorkingDirectory=/usr/share/elasticsearch
User=elasticsearch
Group=elasticsearch

ExecStart=/usr/share/elasticsearch/bin/systemd-entrypoint -p ${PID_DIR}/elasticsearch.pid --quiet

# StandardOutput is configured to redirect to journalctl since
# some error messages may be logged in standard output before
# elasticsearch logging system is initialized. Elasticsearch
# stores its logs in /var/log/elasticsearch and does not use
# journalctl by default. If you also want to enable journalctl
# logging, you can simply remove the "quiet" option from ExecStart.
StandardOutput=journal
StandardError=inherit
# Specifies the maximum file descriptor number that can be opened by this process
LimitNOFILE=65535
# Specifies the maximum number of processes
LimitNPROC=4096
# Specifies the maximum size of virtual memory
LimitAS=infinity
# Specifies the maximum file size
LimitFSIZE=infinity
# Disable timeout logic and wait until process is stopped
TimeoutStopSec=0
# SIGTERM signal is used to stop the Java process
KillSignal=SIGTERM
# Send the signal only to the JVM rather than its control group
KillMode=process
# Java process is never killed
SendSIGKILL=no
# When a JVM receives a SIGTERM signal it exits with code 143
SuccessExitStatus=143
# Allow a slow startup before the systemd notifier module kicks in to extend the timeout
TimeoutStartSec=75
[Install]
WantedBy=multi-user.target
# Built for packages-8.0.0 (packages)

```



### 클러스터 구조

####  cluster

![img](D:\Code\dev\img\cluster)



#### 인덱스와 샤드

Elasticsearch 에서는 단일 데이터 단위를 **도큐먼트(document)** 라고 하며 이 도큐먼트를 모아놓은 집합을 **인덱스(Index)** 라고 합니다. 인덱스라는 단어가 여러 뜻으로 사용되기 때문에 데이터 저장 단위인 인덱스는 **인디시즈(indices)** 라고 표현하기도 합니다. 

* 데이터를 Elasticsearch에 저장하는 행위는 **색인**, 
* 도큐먼트의 집합 단위는 **인덱스** 라고 하겠습니다.



```
$ curl -XPUT "http://localhost:9200/books" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  }
}'
```



```
$ curl -XGET http://localhost:9200/books
{"books":{"aliases":{},"mappings":{},"settings":{"index":{"routing":{"allocation":{"include":{"_tier_preference":"data_content"}}},"number_of_shards":"5","provided_name":"books","creation_date":"1646110042085","number_of_replicas":"1","uuid":"bL8m2PYkScGIXIsXSZ-lew","version":{"created":"7110199"}}}}}elastic@ubuntu20:~/els/config$ 
```







![img](D:\Code\dev\img\shard)



## REST API

### ElasticSearch Commands Cheat Sheet

#### curl

* list all indexes

```json
curl -X GET 'http://localhost:9200/_cat/indices?v'
curl -X GET 'http://localhost:9200/sample/_search'
curl -X GET http://localhost:9200/samples/_search?q=school:Harvard
curl -XGET --header 'Content-Type: application/json' http://localhost:9200/samples/_search -d '{ "query" : { "match" : { "school": "Harvard" } }}'
curl -X GET http://localhost:9200/samples
curl -XPUT --header 'Content-Type: application/json' http://localhost:9200/samples/_doc/1 -d '{ "school" : "Harvard"}'
 
curl -XPUT --header 'Content-Type: application/json' http://localhost:9200/samples/_doc/2 -d ' { "school": "Clemson"}'
curl -XPOST --header 'Content-Type: application/json' http://localhost:9200/samples/_doc/2/_update -d '{ "doc" : {"students": 50000}}'

curl --user $pwd  -H 'Content-Type: application/json' -XGET https://58571402f5464923883e7be42a037917.eu-central-1.aws.cloud.es.io:9243/_cluster/health?pretty

curl -X GET 'http://localhost:9200/(index)/_search'?pretty=true

```



* 예시

```json
GET _cat/indices
GET metricbeat-7.10.2/_search 
GET metricbeat-7.10.2/_search
{
  "query":{
    "match":{
      "host.hostname": "so-prometheus-exporter"
    }
  }
}

GET metricbeat-7.10.2/_search?q=host.hostname: "so-prometheus-exporter"

GET _search
{
  "query": {
    "match_all": {}
  }
}
```

* curl 예시

```json
$ curl  -X GET http://192.168.56.189:9200/_cat/indices
green open filebeat-7.10.2                     lf4FiHvwSwGLsqPHud5ahg 1 1      721      0 845.9kb 423.1kb
green open metricbeat-7.10.2-2022.02.10-000011 ijNHrgZqRW-3mIr0FhFknw 1 1 46216697      0  28.8gb  14.3gb

$ curl  -X GET http://192.168.56.189:9200/metricbeat--7.10.2/_search
$ curl  -X GET http://192.168.56.189:9200/metricbeat-7.10.2/_search?q=host.hostname:"so-prometheus-exporter"
```



#### RESTFul

- 입력 : `PUT http://user.com/kim -d {"name":"kim", "age":38, "gender":"m"}`
- 조회 : `GET http://user.com/kim`
- 삭제 : `DELETE http://user.com/kim`



#### kibana dev tools

* http://localhost:5601



#### CRUD

  Elasticsearch에서는 단일 도큐먼트별로 고유한 URL을 갖습니다. 도큐먼트에 접근하는 URL은 

```
http://<호스트>:<포트>/<인덱스>/_doc/<도큐먼트 id> 
```

구조로 되어 있습니다. 

* curl로 해도 되지만  그래도 kibana dev tools를 사용하여 테스트 해본다. 

```sh
$ curl -XPUT "http://localhost:9200/my_index/_doc/1" -H 'Content-Type: application/json' -d'
{
  "name": "Jongmin Kim",
  "message": "안녕하세요 Elasticsearch"
}'
{"_index":"my_index","_type":"_doc","_id":"1","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":0,"_primary_term":1}
```



##### PUT

```json
PUT my_index/_doc/1
{
  "name":"Jongmin Kim",
  "message":"안녕하세요 Elasticsearch"
}
```

 처음으로 도큐먼트를 입력하면 결과에 `"result" : "created"` 로 표시가 됩니다. 동일한 URL에 다른 내용의 도큐먼트를 다시 입력하게 되면 기존 도큐먼트는 삭제되고 새로운 도큐먼트로 덮어씌워지게 됩니다. 이 때는 결과에 `created`가 아닌 `updated`가 표시됩니다.

* 결과

```json
{
  "_index" : "my_index",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
```



##### GET

```json
GET my_index/_doc/1
```

* 결과

```
{
  "_index" : "my_index",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 3,
  "_seq_no" : 2,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "name" : "Jongmin Kim",
    "message" : "안녕하세요 Elasticsearch"
  }
}
```



##### POST

  POST 메서드는 PUT 메서드와 유사하게 데이터 입력에 사용이 가능합니다. 도큐먼트를 입력할 때 POST 메서드로 `<인덱스>/_doc` 까지만 입력하게 되면 자동으로 임의의 도큐먼트id 가 생성됩니다. 도큐먼트id의 자동 생성은 PUT 메서드로는 동작하지 않습니다.

```json
POST my_index/_doc
{
  "name":"Jongmin Kim",
  "message":"안녕하세요 Elasticsearch"
}
```



* 결과: id 값이 자동으로 생성된다. 

```json
{
  "_index" : "my_index",
  "_type" : "_doc",
  "_id" : "SGbvQ38BvWqIjT4Ix3FL",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 4,
  "_primary_term" : 1
}

```



* 오류: pretty=true

```
GET  my_index/_doc

{
  "error" : "Incorrect HTTP method for uri [/my_index/_doc?pretty=true] and method [GET], allowed: [POST]",
  "status" : 405
}
```



##### DELETE

```json
DELETE  my_index/_doc/2
```

* 결과

```json
{
  "_index" : "my_index",
  "_type" : "_doc",
  "_id" : "2",
  "_version" : 2,
  "result" : "deleted",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 5,
  "_primary_term" : 1
}
```



##### 벌크

```json
POST test/_bulk
{"index":{"_index":"test","_id":"1"}}
{"field":"value one"}
{"index":{"_index":"test","_id":"2"}}
{"field":"value two"}
{"delete":{"_index":"test","_id":"2"}}
{"create":{"_index":"test","_id":"3"}}
{"field":"value three"}
{"update":{"_index":"test","_id":"1"}}
{"doc":{"field":"value two"}}
```

* PUT test/_doc/1 {"field": "value one"}
* PUT test/_doc/2 {"field":"value two"}
* DELETE test/_doc/2 
* PUT  test/_create/3  {"field":"value three"}
* PUT test/_doc/1  {"doc":{"field":"value two"}}



##### 예시

```
GET _search
{
  "query": {
    "match_all": {}
  }
}
GET _cat/indices
GET .kibana/_search
GET filebeat-*/_search
{
  "query":{
    "match": {
     "so.log_type": "ERRO"
    }
  }
}

GET filebeat-*/_mapping 

GET phones/_search
{
  "query": {
    "range": {
      "price": {
        "gte": 700,
        "lt": 900
      }
    }
  }
}

GET phones/_search
{
  "query": {
    "range": {
      "date": {
        "gt": "2016-01-01"
      }
    }
  }
}

GET phones/_search
{
  "query": {
    "range": {
      "date": {
        "gt": "31/12/2015",
        "lt": "2018",
        "format": "dd/MM/yyyy||yyyy"
      }
    }
  }
}

DELETE sobeat-*/
```





### 검색 API

Elasticsearch의 진가는 쿼리를 통한 검색 기능에 있습니다. 검색은 인덱스 단위로 이루어집니다. `GET <인덱스명>/_search` 형식으로 사용하며 쿼리를 입력하지 않으면 전체 도큐먼트를 찾는 **match_all** 검색을 합니다.

#### URI 검색

_search 뒤에 `q` 파라메터를 사용해서 검색어를 입력할 수 있습니다. 이렇게 요청 주소에 검색어를 넣어 검색하는 방식을 **URI 검색**이라고 합니다. 

``` json
GET test/_search?q=value
GET test/_search?q=field:"value two"
```

* 결과: field의 값이 "value"에 해당하는 값 1건이 hits되어 조회 된다. 

```json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.7985077,
    "hits" : [
      {
        "_index" : "test",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.7985077,
        "_source" : {
          "field" : "value two"
        }
      }
    ]
  }
}
```



#### body data 검색

``` json
GET test/_search
{
  "query": {
    "match": {
      "field": "two"
    }
  }
}
```



* 결과:  "two"라는 스트링이 들어간 모든 필드가 조회 된다.  만약 " value two" 이렇게 하면 value가 들어가거나 two가 들어가 모든 필드가 조회된다. 

```json
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.6931471,
    "hits" : [
      {
        "_index" : "test",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.6931471,
        "_source" : {
          "field" : "value two"
        }
      }
    ]
  }
}

```



## Full Text Query

* data 입력

```json
POST my_index/_bulk
{"index":{"_id":1}}
{"message":"The quick brown fox"}
{"index":{"_id":2}}
{"message":"The quick brown fox jumps over the lazy dog"}
{"index":{"_id":3}}
{"message":"The quick brown fox jumps over the quick dog"}
{"index":{"_id":4}}
{"message":"Brown fox brown dog"}
{"index":{"_id":5}}
{"message":"Lazy jumping dog"}
```



#### match_all

```json
GET my_index/_search
```



#### match

```json
GET my_index/_search
{
  "query": {
    "match": {
      "message": "dog"
    }
  }
}
```



* or 조건:  quick or dog 조건으로 검색

```json
GET my_index/_search
{
  "query": {
    "match": {
      "message": "quick dog"
    }
  }
}
```

* and 조건 : quick and dog 조건으로 검색

```json
GET my_index/_search
{
  "query": {
    "match": {
      "message": {
        "query": "quick dog",
        "operator": "and"
      }
    }
  }
}
```

* 문장으로 검색

```
GET my_index/_search
{
  "query": {
    "match_phrase": {
      "message": "lazy dog"
    }
  }
}
```





#### 복합 Query

* must : 쿼리가 참인 도큐먼트들을 검색합니다. 

* must_not : 쿼리가 거짓인 도큐먼트들을 검색합니다. 

* should : 검색 결과 중 이 쿼리에 해당하는 도큐먼트의 점수를 높입니다. 

* filter : 쿼리가 참인 도큐먼트를 검색하지만 스코어를 계산하지 않습니다. must 보다 검색 속도가 빠르고 캐싱이 가능합니다.

```json
GET <인덱스명>/_search
{
  "query": {
    "bool": {
      "must": [
        { <쿼리> }, …
      ],
      "must_not": [
        { <쿼리> }, …
      ],
      "should": [
        { <쿼리> }, …
      ],
      "filter": [
        { <쿼리> }, …
      ]
    }
  }
}
```

* 이렇게 사용

```json
GET my_index/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "message": "quick"
          }
        },
        {
          "match_phrase": {
            "message": "lazy dog"
          }
        }
      ]
    }
  }
}
```







#### 범위 Query

```json
POST phones/_bulk
{"index":{"_id":1}}
{"model":"Samsung GalaxyS 5","price":475,"date":"2014-02-24"}
{"index":{"_id":2}}
{"model":"Samsung GalaxyS 6","price":795,"date":"2015-03-15"}
{"index":{"_id":3}}
{"model":"Samsung GalaxyS 7","price":859,"date":"2016-02-21"}
{"index":{"_id":4}}
{"model":"Samsung GalaxyS 8","price":959,"date":"2017-03-29"}
{"index":{"_id":5}}
{"model":"Samsung GalaxyS 9","price":1059,"date":"2018-02-25"}
```



range 쿼리는 `range : { <필드명>: { <파라메터>:<값> } }` 으로 입력됩니다. range 쿼리 파라메터는 아래의 4가지가 있습니다. 

- gte :  (Greater-than or equal to) - 이상 (같거나 큼)
- gt : (Greater-than) – 초과 (큼)
- lte :  (Less-than or equal to) - 이하 (같거나 작음)
- lt :  (Less-than) - 미만 (작음)



```json
GET phones/_search
{
  "query": {
    "range": {
      "price": {
        "gte": 700,
        "lt": 900
      }
    }
  }
}
```



#### 날짜 검색

```json
GET phones/_search
{
  "query": {
    "range": {
      "date": {
        "gt": "2016-01-01"
      }
    }
  }
}
```







## 집계-Aggregations



 Aggregation 에는 크게 **Metrics** 그리고 **Bucket** 두 종류가 있습니다. Aggregations 구문이나 옵션에 metrics 이거나 bucket 이라고 따로 명시를 하지는 않습니다. 

* Aggregation 종류들 중 숫자 또는 날짜 필드의 값을 가지고 계산을 하는 aggregation 들을 metrics aggregation 이라고 분류하고, 
* 범위나 keyword 값 등을 가지고 도큐먼트들을 그룹화 하는 aggregation 들을 bucket aggregation 이라고 분류 합니다.

  다음에 주로 사용되는 **Metrics** 와 **Bucket** Aggregations 들을 설명하기 위해 아래의 데이터를 먼저 **my_stations** 인덱스에 입력하도록 하겠습니다. 처음 한 줄을 카피 해서 필드명은 그대로 두고 필드값만 수정하면 약간 더 수월하게 입력할 수 있습니다.

```json
PUT my_stations/_bulk
{"index": {"_id": "1"}}
{"date": "2019-06-01", "line": "1호선", "station": "종각", "passangers": 2314}
{"index": {"_id": "2"}}
{"date": "2019-06-01", "line": "2호선", "station": "강남", "passangers": 5412}
{"index": {"_id": "3"}}
{"date": "2019-07-10", "line": "2호선", "station": "강남", "passangers": 6221}
{"index": {"_id": "4"}}
{"date": "2019-07-15", "line": "2호선", "station": "강남", "passangers": 6478}
{"index": {"_id": "5"}}
{"date": "2019-08-07", "line": "2호선", "station": "강남", "passangers": 5821}
{"index": {"_id": "6"}}
{"date": "2019-08-18", "line": "2호선", "station": "강남", "passangers": 5724}
{"index": {"_id": "7"}}
{"date": "2019-09-02", "line": "2호선", "station": "신촌", "passangers": 3912}
{"index": {"_id": "8"}}
{"date": "2019-09-11", "line": "3호선", "station": "양재", "passangers": 4121}
{"index": {"_id": "9"}}
{"date": "2019-09-20", "line": "3호선", "station": "홍제", "passangers": 1021}
{"index": {"_id": "10"}}
{"date": "2019-10-01", "line": "3호선", "station": "불광", "passangers": 971}
```

* kibana:5601에서 Management>StackManagement>kibana>Index Patten> create Index pattern 구성 
* create panel에서  "passenger" 필드를 이용한 kinbana panel 구성

![image-20220302103543049](D:\Code\dev\img\image-20220302103543049.png)



### Metric aggregations

#### min,max,sum,avg

가장 흔하게 사용되는 metrics aggregations 은 **min**, **max**, **sum**, **avg** aggregation 입니다. 순서대로 명시한 필드의 **최소**, **최대**, **합**, **평균** 값을 가져오는 aggregation 입니다.

```json
GET my_stations/_search
{
  "size": 0,
  "aggs": {
    "all_passangers": {
      "sum": {
        "field": "passangers"
      }
    }
  }
}
```

* 결과

```json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "all_passangers" : {
      "value" : 41995.0
    }
  }
}
```



#### stats

*  **min**, **max**, **sum**, **avg** 값을 모두 가져와야 한다면 다음과 같이 **stats** aggregation을 사용하면 위 4개의 값 모두와 count 값을 한번에 가져옵니다.

```json
GET my_stations/_search
{
  "size": 0, 
  "aggs": {
    "passangers_stats": {
      "stats": {
        "field": "passangers"
      }
    }
  }
}
```

* 결과

```json
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 5,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "gangnam_passangers" : {
      "value" : 29656.0
    }
  }
}
```

#### cardinality

  필드의 값이 모두 몇 종류인지 분포값을 알려면 **cardinality** aggregation을 사용해서 구할 수 있습니다. Cardinality 는 일반적으로 text 필드에서는 사용할 수 없으며 **숫자** 필드나 **keyword**, **ip** 필드 등에 사용이 가능합니다. 사용자 접속 로그에서 IP 주소 필드를 가지고 실제로 접속한 사용자가 몇명인지 파악하는 등의 용도로 주로 사용됩니다. 다음은 my_stations 인덱스에서 **line** 필드의 값이 몇 종류인지를 계산하는 예제입니다.



#### percentiles, percentile_ranks

  값들을 백분위 별로 보기 위해서 **percentiles** aggregation 의 사용이 가능합니다. 먼저 passangers 필드에 percentiles aggregation 적용 한 것을 확인 해 보겠습니다.



### Bucket aggregations

  Bucket aggregation 은 주어진 조건으로 분류된 **버킷** 들을 만들고, 각 버킷에 소속되는 도큐먼트들을 모아 **그룹으로 구분**하는 것입니다. 각 버킷 별로 포함되는 도큐먼트의 개수는 **doc_count** 값에 기본적으로 표시가 되며 각 버킷 안에 metrics aggregation 을 이용해서 다른 계산들도 가능합니다. 주로 사용되는 bucket aggregation 들은 **Range**, **Histogram**, **Terms** 등이 있습니다.

#### range

  **range** 는 숫자 필드 값으로 범위를 지정하고 각 범위에 해당하는 버킷을 만드는 aggregation 입니다. **field** 옵션에 해당 필드의 이름을 지정하고 **ranges** 옵션에 배열로 **from**, **to** 값을 가진 오브젝트 값을 나열해서 범위를 지정합니다. 다음은 passangers 값이 각각 1000 미만, 1000~4000 사이, 4000 이상 인 버킷들을 생성하는 예제입니다.

```json
GET my_stations/_search
{
  "size": 0,
  "aggs": {
    "passangers_range": {
      "range": {
        "field": "passangers",
        "ranges": [
          {
            "to": 1000
          },
          {
            "from": 1000,
            "to": 4000
          },
          {
            "from": 4000
          }
        ]
      }
    }
  }
}
```

#### histogram

  histogram 도 range 와 마찬가지로 숫자 필드의 범위를 나누는 aggs 입니다. 앞에서 본 range 는 from 과 to 를 이용해서 각 버킷의 범위를 지정했습니다. histogram 은 from, to 대신 **interval** 옵션을 이용해서 주어진 간격 크기대로 버킷을 구분합니다. 다음은 passangers 필드에 간격이 2000인 버킷들을 생성하는 예제입니다.

```json
GET my_stations/_search
{
  "size": 0,
  "aggs": {
    "passangers_his": {
      "histogram": {
        "field": "passangers",
        "interval": 2000
      }
    }
  }
}
```



*  결과

```json
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "passangers_his" : {
      "buckets" : [
        {
          "key" : 0.0,
          "doc_count" : 2
        },
        {
          "key" : 2000.0,
          "doc_count" : 2
        },
        {
          "key" : 4000.0,
          "doc_count" : 4
        },
        {
          "key" : 6000.0,
          "doc_count" : 2
        }
      ]
    }
  }
}
```





#### date_range, date_histogram

  **range** 와 **histogram** aggs 처럼 숫자 외에도 날짜 필드를 이용해서 범위별로 버킷의 생성이 가능합니다. 이 때 사용되는 **date_range**, **date_histogram** aggs 들은 특히 시계열 데이터에서 날짜별로 값을 표시할 때 매우 유용합니다. **date_range** 는 **ranges** 옵션에 `{"from": "2019-06-01", "to": "2016-07-01"}` 와 같이 입력하며 **date_histogram** 은 **interval** 옵션에 `day`, `month`, `week` 와 같은 값들을 이용해서 날짜 간격을 지정할 수 있습니다. 다음은 **date_histogram** 으로 **date** 필드를 **1개월** 간격으로 구분하는 예제입니다.



#### terms

  앞에서 살펴 본 **(date_)range**, **histogram** 은 모두 숫자, 날짜를 가지고 구간을 나누는 aggregation 이었습니다. **terms** aggregation 은 **keyword** 필드의 문자열 별로 버킷을 나누어 집계가 가능합니다. keyword 필드 값으로만 사용이 가능하며 분석된 text 필드는 일반적으로는 사용이 불가능합니다. 다음은 my_stations 인덱스에서 **station.keyword** 필드를 기준으로 버킷들을 만드는 예제입니다.

```json
GET my_stations/_search
{
  "size": 0,
  "aggs": {
    "stations": {
      "terms": {
        "field": "station.keyword"
      }
    }
  }
}
```



* 결과

```json
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "stations" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [
        {
          "key" : "강남",
          "doc_count" : 5
        },
        {
          "key" : "불광",
          "doc_count" : 1
        },
        {
          "key" : "신촌",
          "doc_count" : 1
        },
        {
          "key" : "양재",
          "doc_count" : 1
        },
        {
          "key" : "종각",
          "doc_count" : 1
        },
        {
          "key" : "홍제",
          "doc_count" : 1
        }
      ]
    }
  }
}
```

