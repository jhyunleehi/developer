# 영역 1. 데이터 수행

## 1. 데이터 수집 수행

영역 1의 첫 번째 작업 설명인 데이터 수집 수행을 시작하겠습니다. 시작하기 전에 잠시 기본 사항으로 돌아가서 기본 사항 강의에서 언급한 데이터 엔지니어링 수명 주기에 대해 이야기해 보겠습니다.

#### 데이터 생성, 저장, 수집 , 변형 및 제공 단계
데이터 생성, 저장, 수집, 변형 및 제공 단계가 있다는 것을 기억해야 합니다. 그러면 이 작업 설명의 초점인 수집 전의 처음 두 단계를 빠르게 살펴보겠습니다.



데이터 엔지니어링 수명 주기의 생성 단계부터 시작하겠습니다. 생성은 데이터가 발생하는 위치, 즉 소스 시스템입니다. 사물 인터넷(IoT) 디바이스, 트랜잭션 데이터베이스, 애플리케이션 메시지 대기열 등이 될 수 있습니다. 생성 단계에서 데이터 엔지니어는 소스 시스템의 데이터를 사용하지만 해당 소스 시스템을 제어하지는 않습니다.

 

소스 시스템의 작동 방식, 데이터 생성 방식, 데이터의 빈도 및 속도, 생성되는 데이터의 유형을 이해해야 합니다. 이 단계에서 고려해야 할 몇 가지 질문은 다음과 같습니다. 데이터의 특성은 무엇입니까? 소스 시스템에서 데이터가 유지되는 방식은 무엇입니까? 중복 데이터가 존재합니까? 해당 소스 데이터의 스키마는 무엇이고 스키마는 변경됩니까?



데이터 엔지니어링 수명 주기의 다음 단계는 저장입니다. 소스 데이터를 저장할 적절한 솔루션을 선택할 수 있어야 합니다. 이 단계는 영역 2: 데이터 스토어 관리에서 다를 것입니다. 여기서는 저장이 두 번째 단계이고 소스 시스템의 데이터를 AWS로 수집하려면 다양한 사용 사례와 다양한 AWS 스토리지 서비스를 이해해야 한다는 점을 이야기하고 싶습니다.

 
#### 데이터 수집 
데이터 엔지니어링 수명 주기의 세 번째 단계이자 이 작업 설명의 초점인 수집으로 넘어가겠습니다. 데이터 소스, 소스 시스템의 특성, 해당 데이터가 소스 시스템에 저장되는 방식을 이해하고 수집된 데이터를 저장하기 위한 계획 및 설계를 마친 후에는 해당 데이터를 수집해야 합니다. 고려해야 할 몇 가지 질문은 다음과 같습니다. 수집되는 데이터의 사용 사례는 무엇입니까? 이 데이터는 수집된 후 어디로 이동합니까? 데이터가 수집되는 빈도 및 볼륨은 어떻게 됩니까? 데이터의 형식은 무엇입니까? 등입니다.



이 단계에서 고려해야 할 두 가지 주요 데이터 통합 개념이 있는데. 배치 또는 스트리밍과 푸시 또는 풀입니다. 이 시험에서는 배치, 스트리밍, 푸시 및 풀 각각의 개념 및 차이점을 이해해야 합니다.

 

또한 데이터 수집 파이프라인의 재처리를 고려해야 합니다. 재처리는 파이프라인 로직에서 실패, 업데이트 또는 변경이 발생할 경우 데이터를 재처리하는 데 도움이 됩니다. 데이터 수집 파이프라인에서 재처리를 구현하기 위해 고려할 사항은 무엇입니까? 저는 항상 이벤트 기반 방식으로 데이터 수집 파이프라인을 설계할 것을 권장합니다. 새로운 데이터가 도착하거나 데이터가 변경될 경우 데이터를 수집하고 이벤트를 시작하는 데 사용할 수 있는 AWS 서비스에는 무엇이 있습니까?

 

몇 가지 유용한 서비스는 Amazon S3, Amazon Kinesis 또는 Amazon EventBridge입니다. 다양한 시뮬레이션 시나리오 및 재처리 시나리오를 사용하여 데이터 수집 파이프라인의 재처리를 테스트하여 멱등성을 검증하고 데이터 손실, 중복 또는 무결성 문제 없이 일관되게 데이터를 재처리하는 것이 모범 사례입니다.

 

이는 파이프라인이 데이터 중복 또는 불일치 없이 동일한 데이터를 여러 번 처리하는 데 도움이 됩니다. 또한 파이프라인 내에 데이터 처리의 진행 상황 및 상태를 추적하여 마지막으로 성공적으로 처리된 데이터 포인트를 결정하는 체크포인트 메커니즘을 구성합니다. 그러면 오류 또는 중단이 발생할 경우 파이프라인이 해당 포인트부터 처리를 재개할 수 있습니다.

 

데이터 수집 파이프라인 내에 데이터 버전 관리도 추가합니다. 원시 또는 중간 데이터를 Amazon S3와 같은 확장 가능한 내구적 스토리지 서비스에 저장해야 합니다. 그러면 규정 준수 요구 사항, 데이터 거버넌스 정책 및 다시 재생을 위해 일정 기간 동안 데이터를 보존할 수 있습니다. 그런 다음 데이터 수집 파이프라인 내에 로깅 및 모니터링을 추가하여 진행 상황을 추적하고, 실패를 식별하고, 파이프라인 동작을 분석하기 위한 관련 로그, 오류 및 지표를 캡처합니다. 마지막으로 AWS CloudFormation 또는 AWS Cloud Development Kit (AWS CDK) 같은 코드형 인프라 도구를 사용하여 데이터 수집 파이프라인의 배포 및 구성을 자동화합니다. 이 영역의 마지막 작업 설명에서 IaC에 대해 자세히 알아볼 것입니다.

 

데이터 수집을 수행하기 위한 단계를 살펴보겠습니다. 데이터 수집 프로세스는 생산자에서 시작됩니다. 생산자는 데이터베이스부터 모바일 디바이스 또는 애플리케이션에 이르기까지 모든 것이 될 수 있습니다. 예를 들어 생산자가 소스 시스템에서 데이터를 생성하면 수집 도구가 데이터를 수집합니다.

 

수집된 데이터는 소비자로 직접 이동할 수 있습니다. 소비자는 Amazon Elastic Compute Cloud (Amazon EC2)에서호스팅되는 애플리케이션부터 AWS 데이터베이스 서비스, AWS Lambda와 같은 컴퓨팅 서비스에 이르기까지 모든 것이 될 수 있습니다.

 

데이터를 AWS로 수집하는 또 다른 예를 살펴보겠습니다. 이 예에서는 애플리케이션이 파일을 S3 데이터 레이크에 기록합니다. 이 데이터 레이크에는 데이터 과학 팀과 비즈니스 인텔리전스 또는 데이터 분석가 팀이 액세스해야 합니다. 애플리케이션은 지속적으로 데이터(고객 상호 작용 및 고객 리뷰)를 캡처한 다음 해당 데이터를 탭으로 구분된 값(TSV) 파일 형식으로 S3 버킷에 기록합니다. 데이터 과학자 또는 기계 학습 엔지니어가 이 원시 데이터를 탐색할 수 있도록 Amazon S3에 기록된 데이터를 변환하는 파이프라인을 어떻게 설계하시겠습니까?

 

데이터는 S3 버킷에 저장되므로 Amazon Athena에서 표준 SQL을 사용하여 데이터를 쿼리 및 분석할 수 있습니다. 이를 어떻게 구성합니까? 먼저 S3 버킷의 TSV 데이터를 Athena에 등록한 다음 데이터세트에 대해 임시 쿼리를 실행합니다. 그러나 TSV 데이터를 Apache Parquet와 같이 보다 쿼리 최적화된 열 기반 파일 형식으로 변환할 수도 있습니다.

 

하지만 비즈니스 인텔리전스 팀이 데이터 웨어하우스에서 해당 데이터의 하위 집합을 필요로 한다면 어떻게 합니까? TSV 데이터를 Amazon Redshift에 삽입하고 Amazon Redshift Spectrum을 사용하여 S3 데이터 레이크 내 데이터에 대한 데이터 웨어하우스 쿼리를 결합할 수 있습니다.

 

새 데이터가 S3 데이터 레이크에 도착하면 Athena 테이블을 업데이트하기 위해 어떤 AWS 서비스를 사용할 수 있습니까? 한 가지 옵션은 AWS Glue와 AWS Glue 크롤러를 사용하는 것입니다. 이 예에 대해 질문을 하나 더 해보겠습니다. 여러 소스로부터 대량의 파일을 수신하고 있다고 가정해 보겠습니다. 매일 모든 파일이 단일 gzip 파일로 병합 및 압축됩니다. 이 파일은 S3 버킷에 업로드됩니다. 그런 다음 Amazon Redshift 클러스터에 로드됩니다.

 

Amazon Redshift 클러스터에 대한 복사 프로세스를 더 빠르게 실행하도록 구성을 업데이트하려면 어떻게 해야 합니까? 잠시 기본 사항으로 돌아가보겠습니다. COPY 명령은 Amazon Redshift 대규모 병렬 처리 아키텍처를 사용하여 Amazon Simple Storage Service (Amazon S3) 버킷의 파일로부터 병렬로 데이터를 읽고 로드한다는 점을 알아야 합니다. 따라서 파일 수가 Amazon Redshift 클러스터 내 슬라이스 수의 배수가 되도록 gzip 파일을 더 작은 파일로 분할할 수 있습니다. 그러면 Amazon Redshift가 데이터를 병렬로 슬라이스에 로드할 수 있으므로 전체 로드 시간이 향상될 수 있습니다. 데이터를 여러 파일로 분할하고 테이블에 배포 키를 설정하여 병렬 처리를 사용할 수 있습니다.

 

이 시험에서 좋은 결과를 얻으려면 데이터 분석을 위한 일반적인 사용 사례에 대해 잘 알고 있어야 하며, 각 사용 사례와 관련된 AWS 서비스, 다양한 AWS 서비스를 구현하는 방법, 여러 서비스 중 특정 서비스를 선택해야 하는 경우를 알고 있어야 합니다.

 

예를 들어 Amazon S3는 데이터 레이크에 적합합니다. 데이터 레이크의 장점은 사전 정의된 스키마가 필요하지 않다는 것입니다.

데이터 웨어하우스 또는 레이크 하우스가 필요한 경우 Amazon Redshift 및 Amazon Redshift Spectrum을 사용할 수 있습니다. 한 가지 장점은 데이터를 불필요하게 이동할 필요 없이 정형 및 비정형 데이터를 대상으로 SQL 및 복잡한 분석 쿼리를 실행할 수 있다는 것입니다.

 

빅 데이터 처리의 경우 Amazon EMR은 데이터 엔지니어링, 데이터 과학 개발 및 협업을 위해 방대한 양의 데이터를 쉽고 빠르게 처리할 수 있습니다.

 

실시간 분석의 경우 AmazonKinesis는 데이터 레이크에 들어오는 스트리밍 데이터를 수집, 처리, 분석하고 실시간으로 대응할 수 있으며 Amazon Managed Streaming for Apache Kafka(Amazon MSK)는 스트리밍 데이터를 실시간으로 수집하고 처리합니다.

 

운영 분석의 경우 Amazon OpenSearch Service는 애플리케이션 모니터링, 로그 분석 및 클릭스트림 분석을 위해 거의 실시간으로 데이터를 검색, 탐색, 필터링, 집계 및 시각화할 수 있습니다.

 
#### 데이터의 5V :  다양성, 볼륨, 속도, 정확성, 유효성, 가치 

계속 언급하듯이 수집의 목적은 데이터의 빈도, 볼륨, 생성 및 소스 시스템에 따라 데이터 스토어에 선택하는 솔루션이 어떻게 결정되는지 이해하는 것입니다. 기본 사항으로 돌아가서 데이터의 5V는 무엇입니까? 다양성, 볼륨, 속도, 정확성 또는 유효성, 가치입니다.

 
**다양성**
다양성은 데이터의 유형입니다. 웹 서버 로그 파일, 동영상, 사진, 지리적 위치 데이터, 센서 또는 IoT 데이터 등이 될 수 있습니다. AWS는 Amazon RDS, Amazon S3, AWS Glue, Amazon Comprehend와 같은 AWS 서비스를 통해 정형, 반정형 및 비정형 데이터 처리를 지원합니다.

 
**볼륨**
볼륨은 전송되는 데이터의 양 또는 전체 크기일 뿐만 아니라 데이터의 일별, 월간, 연간 증가량이기도 합니다. 이는 사용할 AWS 서비스에 대한 지표가 될 수 있습니다. 일부 서비스에는 항목 또는 레코드 크기에 제한이 있으므로 해당 정보를 숙지해야 합니다. 다른 일부 서비스에는 스토리지 볼륨이 증가할 수 있는 전체 크기에 제한이 있습니다. 처리량에도 주의해야 합니다. 일반적인 데이터 분석 서비스 외에 네트워킹 서비스도 포함될 수 있기 때문입니다. 대량 데이터를 위한 AWS 스토리지 서비스에는 무엇이 있습니까? Amazon S3, Amazon Elastic Block Store(Amazon EBS), Amazon Redshift 그리고 DynamoDB가 있습니다.

 
**속도**
속도는 새 데이터를 소스 시스템에서 대상 스토리지 시스템으로 수집하고 처리하는 속도입니다. 소스 데이터가 얼마나 빨리 생성되는지, 그리고 수신 데이터를 얼마나 빨리 처리해야 하는지 확실히 이해해야 합니다. 데이터의 빈도는 데이터를 소스 시스템으로 보내는 생산자의 속성입니다. 실시간 스트림 솔루션과 준실시간 스트림 솔루션의 차이에 주의해야 합니다. 또한 시험에 출제될 수 있는 문제에 존재할 수 있는 모든 분석 요구 사항에 주의해야 합니다. 때때로 이 수집 단계에서 데이터가 변환되는 경우가 있기 때문입니다. 가변적인 트래픽과 대량의 데이터 수집을 처리할 수 있는 AWS 서비스에는 무엇이 있습니까? Amazon Kinesis, Amazon MSK, Amazon OpenSearch Service, Lambda입니다.

 
**정확성**
정확성은 수집되는 데이터의 품질, 완전성 및 정확도를 의미합니다. 데이터는 다양한 소스에서 제공될 수 있으며 데이터 생성 방법에 따라 데이터가 불완전하거나 일관적이지 않을 수 있습니다. IoT 센서가 오프라인 상태가 되어 일정 기간의 데이터가 누락되었다고 가정해 보겠습니다. 수집된 데이터의 정확성을 알고 있다면 데이터를 처리할 때 이를 고려하도록 할 수 있습니다.

 
**가치**
가치는 데이터의 V 중에서 아마도 가장 중요할 것입니다. 데이터가 올바르게 수집되고 수집된 데이터가 현재 및 미래의 비즈니스에 부가가치를 창출하도록 하려면 수집되는 데이터를 고려해야 합니다. 테라바이트 규모의 데이터를 수집한 다음 해당 데이터를 모두 정리 및 처리하지만 최종 데이터 제품이 아무런 부가가치도 창출하지 않는다면 이는 비용과 시간 낭비입니다. 영역 2에서 다시 다룰 스토리지는 수집 및 저장되는 데이터 유형에 따라 다릅니다. AWS에는 다양한 사용 사례 및 요구 사항에 맞는 여러 스토리지 서비스가 있습니다. 정형, 반정형 및 비정형 데이터를 사용하고 저장할 수 있는 서비스를 파악하십시오.

 

그러면 수집으로 돌아가겠습니다. 환경에서 수집하는 데이터의 유형 및 온도를 바탕으로 어떤 종류의 수집 솔루션이 요구 사항에 가장 적합한지 결정할 수 있습니다. 데이터 엔지니어 수명 주기의 수집 단계를 지원하는 AWS 서비스에는 무엇이 있습니까?

 
#### 트랜잭션 데이터 시스템  

트랜잭션 데이터 시스템은 소량의 데이터를 신속하게 저장하고 검색할 수 있어야 합니다. 또한 최종 사용자는 데이터에 신속하고 간단하게 액세스할 수 있어야 합니다. DynamoDB 및 Amazon RDS는 트랜잭션 데이터용 AWS 스토리지 서비스입니다. AWS Database Migration Service(AWS DMS)를 사용하여 트랜잭션 데이터를 AWS로 수집할 수도 있습니다.

 

트랜잭션 데이터베이스를 사용하면 테이블의 행이 주기적으로 업데이트됩니다. 예를 들어 고객이 이사를 하거나 새 전화번호를 사용하게 될 수 있습니다. AWS DMS는 데이터베이스 트랜잭션 로그 파일을 업데이트하여 업데이트 사항을 캡처하고 전송할 수 있습니다. 이 서비스는 Oracle 데이터베이스를 Amazon Aurora 데이터베이스로 마이그레이션하는 것과 같이 기존 데이터베이스 시스템을 새로운 데이터베이스 엔진으로 마이그레이션하는 데 사용할 수 있습니다. 또한 분석 관점에서는 AWS DMS를 일반 데이터베이스 엔진에서 S3 데이터 레이크로 연속 복제를 실행하는 데 사용할 수도 있습니다.

 

클릭 스트림 로그와 같은 스트리밍 데이터는 지속적으로 실시간 또는 거의 실시간으로 수집되어야 합니다. 스트리밍 데이터는 소용량 이벤트를 대규모로 읽는 것이 특징입니다. 예를 들면 1KB 페이로드를 지속적으로 스트림에 쓰는 것입니다. 스트리밍 데이터의 경우 시간, 이벤트 양 또는 특정 세그먼트별로 처리를 바인딩하여 결과를 실시간으로 생성하기 위해 처리하는 데이터의 양을 제한해야 할 수 있습니다. 고빈도로 들어오는 핫 데이터를 실시간으로 처리하려는 경우 스트리밍 솔루션이 최선의 선택이 될 수 있습니다. 스트리밍 데이터를 수집하기 위해 사용할 수 있는 AWS 서비스는 무엇입니까? Amazon MSK, Amazon Kinesis는 이미 언급했습니다. Kinesis 제품군에는 특정 사용 사례 및 요구 사항에 따라 사용할 수 있는 여러 가지 서비스가 있습니다. Amazon Kinesis Data Firehose는 스트리밍 데이터를 수집하고 구성 가능한 기간 동안 데이터를 버퍼링한 다음 대상 스토리지 서비스에 기록합니다. 어떤 스토리지 서비스에서 Kinesis Data Firehose를 사용하여 데이터를 스트리밍할 수 있는지 알고 있습니까? 이러한 서비스에는 Amazon S3, Amazon Redshift, Amazon OpenSearch Service 등이 포함됩니다.

 
#### Kinesis Data Streams 
Amazon Kinesis Data Streams는 실시간으로 데이터를 수집합니다. 사용자 지정 애플리케이션을 사용하여 수신 데이터를 처리할 수 있고 지연 시간이 낮습니다. Amazon Managed Service for Apache Flink(이전의 Amazon Kinesis Data Analytics)는 스트리밍 소스에서 데이터를 읽고 SQL 문 또는 Apache Flink 코드를 사용하여 스트림에 대한 분석을 수행합니다. Amazon Kinesis Video Streams는 스트리밍 비디오, 오디오 스트림 및 기타 시계열 데이터(예: 열 화상 및 레이더 데이터)를 처리합니다. 파일의 데이터를 사용하고 Kinesis Data Streams 또는 Kinesis Data Firehose를 사용하여 해당 데이터를 스트림에 기록하는 데 도움이 되는 Amazon Kinesis 에이전트도 있습니다.

 

여기서 질문이 있습니다. 여러분의 회사는 Kinesis Data Streams를 사용하여 매일 대량의 실시간 데이터를 수집하고 있습니다. 한 데이터 분석가가 이 수집의 쓰기 성능이 크게 감소하고 쓰기 요청이 제한되고 있음을 발견했습니다. 필요한 성능을 충족하도록 하려면 이 파이프라인을 어떻게 업데이트할 수 있습니까?

 

자, 한 걸음 물러서서 Kinesis Data Streams의 기본 사항을 점검해 보겠습니다. 각 스트림은 특정 용량을 제공하는 하나 이상의 샤드로 구성됩니다. 워크로드가 증가함에 따라 애플리케이션이 해당 용량을 초과하는 속도로 샤드를 읽거나 쓸 수 있습니다. 이 상황에서는 과부하가 걸린 샤드인 핫 샤드가 발생합니다. 이러한 샤드에는 신속하게 용량을 추가해야 합니다. 그러므로 현재 파이프라인을 이해하고 적절한 컴퓨팅 용량을 확보할 수 있도록 예상 데이터 흐름 속도를 고려해야 합니다. 파티션 키 전략은 Kinesis의 프로비저닝된 용량을 활용하고 핫 샤드를 방지하는 데 도움이 될 수 있습니다. 크기 조정 결정을 위한 가시성을 확보하려면 스트림 지표를 모니터링하고 경보 임계값을 설정하는 것이 중요합니다.

 

저라면 솔루션으로 UpdateShardCount 작업을 사용하여 스트림 크기를 확장하고 샤드 수를 늘리겠습니다. 또한 무작위 파티션 키를 사용하여 필요에 따라 조정하고 해시 키 공간을 샤드 전체에 균등하게 배포하겠습니다.

또 다른 AWS 스트리밍 서비스는 Amazon MSK입니다. Amazon MSK를 사용하여 기존 Apache Kafka 클러스터를 대체할 수 있습니다. Kinesis가 새로운 솔루션에 더 나은 옵션일 수 있습니다. Kinesis는 서버리스이고 데이터 처리량에 대해서만 비용을 지불하기 때문입니다. MSK에서는 클러스터를 통해 데이터를 전송하는지 여부에 관계없이 클러스터 비용을 지불합니다.

 
#### AWS AppFlow 
수집에 사용할 수 있는 또 다른 AWS 서비스는 Amazon AppFlow입니다. Amazon AppFlow의 사용 사례는 무엇입니까? 서비스형 소프트웨어 서비스에서 데이터를 수집하고 데이터를 변환하여 Amazon S3, Amazon Redshift 또는 다른 SaaS 서비스에 기록하는 데 사용할 수 있습니다.

 

ile Transfer Protocol(FTP) 및 Secure File Transfer Protocol(SFTP) 프로토콜을 사용하여 데이터를 수집해야 한다면 어떤 AWS 서비스를 선택하시겠습니까? 일반적인 파일 전송 프로토콜을 사용하여 Amazon S3에 직접 파일을 전송하기 위해 AWS Transfer Family를 사용할 수 있습니다.

 

온프레미스 스토리지에 저장된 데이터를 수집해야 한다면 어떻게 합니까? 어떤 AWS 서비스를 사용하시겠습니까? AWS DataSync는 기존 온프레미스 스토리지 시스템에서 데이터를 수집할 수 있습니다. 또한 DataSync는 NFS, Server Message Block(SMB)과 같은 일반적인 프로토콜을 사용합니다.

 

대량의 데이터를 수집해야 하는 경우에는 어떻게 합니까? AWS Snow Family를 이해하고 다양한 사용 사례 및 요구 사항에 가장 적합한 Snow 서비스를 파악해야 합니다.

 
#### 배치 데이터 수집 
이어서 배치 데이터 수집에 대해 살펴보겠습니다. 배치 데이터를 사용하면 일반적으로 더 큰 이벤트 페이로드를 처리하고 CRON 작업과 같은 예약된 작업을 사용하여 매시간, 매일 또는 매주 단위로 이러한 페이로드를 수집할 수 있습니다. 예를 들어 S3 버킷에 저장된 데이터를 수집했다면 데이터 소스를 기준으로 데이터 경계를 설정하는 것입니다. 배치 데이터 수집은 즉시 처리할 필요가 없는 콜드 데이터에 적합합니다.

 

배치 데이터 수집을 위한 AWS 서비스에는 무엇이 있습니까? Amazon EMR은 일반적인 Hadoop 프레임워크를 배포하는 방법을 제공하며 일부 도구는 데이터베이스에서 데이터를 수집하는 데 사용할 수 있습니다. 예를 들어 Amazon EMR에서 Spark를 실행하고 Java Database Connectivity(JDBC) 드라이버를 사용해 관계형 데이터베이스에 연결하여 데이터 레이크에 데이터를 로드할 수 있습니다.

 
#### AWS Glue 
AWS Glue는 JDBC 소스에 연결할 수 있는 완전관리형 ETL 서비스로, 다양한 데이터베이스 엔진에 연결할 수 있으며 이러한 연결을 통해 추가 처리를 위해 데이터를 전송할 수 있습니다. AWS Glue는 다양한 데이터 스토어와 데이터 스트림 간에 데이터를 처리, 개선, 마이그레이션하는 데 도움이 되는 서비스입니다. 데이터 엔지니어는 AWS Glue 대화형 세션을 사용하여 데이터를 분석 및 처리할 수 있으며, AWS Glue Studio에서 시각적으로 ETL 워크플로를 개발, 실행, 모니터링할 수 있습니다.

 

AWS에는 시스템 요구 사항을 지원하는 데 도움이 되는 다양한 서비스가 있습니다. 이번 강의를 마무리하면서 AWS에서 스테이트풀 및 스테이트리스 데이터 트랜잭션을 관리하는 방법에 대해 이야기해 보겠습니다. 스테이트풀 데이터 트랜잭션은 현재 상태에 대한 정보를 저장합니다. 방금 트랜잭션 데이터베이스에 대해 이야기했었는데요, 스테이트풀 데이터 트랜잭션을 지원하는 AWS 서비스에는 무엇이 있습니까? Amazon ElastiCache for Redis, Amazon RDS가 있습니다.

 

스테이트리스 데이터 트랜잭션은 데이터 또는 세션을 저장하지 않으며 과거 상태에 의존하지 않습니다. 스테이트리스 데이터 트랜잭션을 지원하는 AWS 서비스에는 무엇이 있습니까? Lambda, Amazon API Gateway, Amazon S3가 있습니다.

 

다음에는 영역 1의 두 번째 작업 설명인 데이터 변환 및 처리를 시작하겠습니다.

## 2. 데이터 변환 및 처리

수집된 데이터를 변환 및 처리하는 영역 1의 두 번째 작업 설명을 시작하겠습니다. 지난 강의에 이어서 데이터 엔지니어 수명 주기의 네 번째 단계인 변환에 대해 이야기해 보겠습니다.

이제까지 우리는 소스 시스템에서 데이터를 생성했습니다. 데이터를 저장하기 위해 저장된 데이터를 선택했습니다. 선택한 데이터 스토어로 데이터를 수집했습니다. 데이터 엔지니어링 수명 주기의 다음 단계는 변환입니다. 이 단계까지 여러분의 책임은 주로 데이터를 한 위치에서 다른 위치로 이동하는 것이었습니다. 이제 해당 데이터를 유용하게 만들어야 합니다.

 

변환은 데이터를 원래 형식에서 다운스트림 사용 사례에 유용한 다른 형식으로 변경하는 것을 의미합니다. 데이터는 적절히 변환되지 않으면 보고서, 분석, 기계 학습 등에 사용할 수 없습니다. 원시 데이터를 다운스트림 이해 관계자가 사용할 수 있는 데이터로 전환하는 데 사용할 수 있는 도구는 무엇입니까?

 

쿼리, 모델링, 변환입니다. 기본 사항으로 돌아가서 쿼리란 무엇인가요? 쿼리는 데이터 엔지니어링, 데이터 과학 및 분석의 기본 요소입니다. 쿼리를 통해 데이터를 검색하고 이를 기반으로 작업을 수행할 수 있습니다. 이 시험에서는 쿼리가 어떻게 작동하는지 알아야 합니다.

 

데이터 모델링은 또 다른 기본 요소이며 저는 많은 사람들이 이 단계를 건너뛰는 것을 보았습니다. 유용한 방식으로 데이터를 구성하기 위한 설계 및 계획 없이는 데이터 시스템 구축을 시작할 수 없습니다. 데이터 모델은 데이터가 실제 세계와 관련되는 방식을 표현하고 데이터가 구조화 및 표준화되는 방식을 반영해야 합니다. 개념, 논리, 물리, 정규화와 같은 다양한 데이터 모델을 이해해야 합니다. 또한 배치 분석 데이터 및 데이터 볼트를 모델링하는 다양한 기술도 이해하는 것이 좋습니다.

 

좀더 자세히 살펴보겠습니다. 데이터 모델링 및 쿼리에서 온라인 트랜잭션 처리(OLTP)와 온라인 분석 처리(OLAP)는 근본적으로 다릅니다. OLTP는 트랜잭션 처리 및 실시간 업데이트에 최적화되어 있으며 데이터의 최신 상태에 중점을 둡니다. OLTP는 정규화된 또는 비정규화된 모델을 사용합니다. OLTP는 OLAP보다 스토리지 요구 사항이 낮고 응답 시간도 더 짧습니다. OLTP는 주문 처리, 결제 처리, 고객 데이터 관리에 적합합니다.

 

OLAP는 복잡한 데이터 분석 및 보고에 최적화되어 있으며 최신 상태 및 과거 데이터에 중점을 둡니다. OLAP는 스타 스키마, 눈송이 스키마 또는 기타 분석 모델을 사용합니다. 스토리지 요구 사항은 테라바이트에서 페타바이트까지 대용량입니다. OLAP는 고객 행동 예측, 추세 분석에 적합합니다.

 

지금까지 데이터 모델링과 해당 데이터의 쿼리에 대해 이야기했는데, 그렇다면 어째서 여전히 데이터를 변환해야 합니까? 여러분이 생각하는 변환은 무엇입니까? 저는 가장 먼저 떠오르는 생각이 데이터를 올바른 유형으로 매핑하는 것입니다. 예를 들면 수집된 문자열 데이터를 숫자 또는 날짜 데이터 형식으로 변경하거나 레코드를 표준 형식으로 변환하거나 잘못된 데이터를 제거하는 것입니다.

 

변환의 후반 단계에서는 데이터 스키마를 변경하고 정규화를 적용하거나 심지어 대규모 집계를 적용하여 기계 학습 프로세스용 데이터를 보고하거나 특성화할 수도 있습니다. 데이터 변환은 데이터를 통일 및 통합하는 기능을 추가합니다. 다운스트림에서 사용할 데이터를 조작, 보강 및 저장하여 가치를 높이고 확장성 및 신뢰성을 추가하며 일반적으로 비용 효율이 더 높습니다.

 

변환 단계에서 고려해야 할 몇 가지 주요 사항은 무엇입니까? 비용 및 투자 수익률 또는 관련 비즈니스 가치는 어떻습니까?

 

이 시험에서는 배치 데이터 변환을 이해해야 합니다. 여기에 여러분의 기초 지식을 테스트하기 위한 질문과 힌트가 있습니다. 이 질문은 지난 강의에서 정답을 알려드렸습니다. 변환은 이 변환 단계에서만 발생합니까? 아니요. 데이터가 수집되는 동안 소스 또는 생성 단계에서 데이터가 변환되는 경우가 있기 때문입니다.

 

데이터를 처리하고 변환하는 이유에 대해 이야기해 보겠습니다. 원시 데이터는 최종 데이터 분석에서 거의 쓸모가 없습니다. 바로 이것이 데이터 분석가가 필요한 비즈니스 인사이트를 생성할 수 있도록 데이터 파이프라인을 설계하고 구축해야 하는 이유입니다. 이를 데이터 준비 또는 데이터 랭글링이라고 합니다. 여러 가지 방법으로 데이터를 준비하고 여러 분석에서 사용할 수 있습니다. 데이터 준비에서 계산 필드를 추가하고, 필터를 적용하고, 필드 이름 또는 데이터 유형을 변경할 수 있습니다. 데이터 소스가 SQL 데이터베이스에 있는 경우 데이터 준비를 사용해 테이블을 조인할 수 있습니다. 또는 여러 테이블의 데이터로 작업하기 원하는 경우 SQL 쿼리를 입력할 수 있습니다.

 

지난 강의에서 언급했듯이 수집을 위한 AWS 서비스 및 도구 일부에서는 수집 프로세스의 일환으로 간단한 변환을 수행할 수도 있습니다. 예를 들어 Kinesis Data Firehose는 데이터를 Parquet 형식으로 기록할 수 있지만 더 복잡한 변환을 수행하려면 일련의 분석 태스크 및 다양한 데이터 소비자에 맞게 데이터를 완전히 최적화해야 할 수도 있습니다.

 

AWS에서 클라우드 컴퓨팅 및 분산 컴퓨팅을 사용하여 데이터를 변환하고 처리할 수 있습니다. 데이터 변환 및 처리를 위한 클라우드 컴퓨팅 AWS 서비스에는 무엇이 있습니까? Lambda, Amazon EMR, AWS Glue, Amazon Redshift가 있습니다.

 

여러 노드에 걸쳐 데이터를 병렬로 처리하는 컴퓨팅 태스크와 관련된 분산 컴퓨팅에 사용할 수 있는 AWS 서비스 및 프레임워크에는 무엇이 있습니까? Amazon EMR, AWS Batch, AWS Step Functions가 있습니다. Lambda를 사용하여 데이터의 검증 및 간단한 변환을 수행할 수 있습니다. 예를 들어, CSV 파일이 수신될 때마다 실행되도록 Lambda 함수를 시작할 수 있습니다. 코드가 수신되는 파일이 CSV 파일인지 확인하고, 일부 계산을 수행하고, 해당 결과로 데이터베이스를 업데이트하고 파일을 다른 S3 버킷으로 이동하여 나중에 배치 처리하도록 할 수 있습니다.

 

서버리스 처리가 필요한 경우에는 어떻게 해야 할까요? 어떤 AWS 서비스가 도움이 될 수 있습니까? 지난 강의에서 언급했듯이 AWS Glue에는 각각 개별 서비스로 분할할 수 있는 여러 구성 요소가 있습니다. AWS Glue는 데이터 변환 및 처리를 수행하기 위해 Python 엔진 또는 Spark 엔진을 사용하는 서버리스 환경입니다. 두 엔진 모두 Amazon S3에 저장된 데이터로 작동합니다. AWS Glue에는 AWS Glue 데이터 카탈로그도 포함되어 있습니다.

 

AWS DMS를 사용하여 데이터베이스를 S3 버킷에 복제한다고 가정해 보겠습니다. S3 버킷에는 소스 데이터베이스의 데이터가 포함된 파일이 여러 개 있습니다. 소스 데이터베이스의 각 테이블에는 Amazon S3의 접두사가 지정된 디렉터리가 생성됩니다. AWS Glue 데이터 카탈로그는 이 데이터세트의 논리적 뷰를 제공하고 추가 메타데이터를 캡처할 수 있습니다. 그런 다음 Athena에서 데이터 카탈로그를 사용하여 SQL 쿼리를 실행하거나 Amazon EMR과 AWS Glue ETL 엔진을 사용하여 사용자가 ETL 코드에서 직접 카탈로그 객체를 참조하도록 도울 수 있습니다. AWS Glue 크롤러는 데이터 소스를 검사하고 자동으로 스키마 및 기타 정보도 추론하여 자동으로 해당 정보를 채울 수 있습니다.

Spark, Hive, Apache Hudi, Apache HBase, Presto, Pig 등의 빅 데이터 처리 도구를 사용해야 한다면 어떻게 합니까? 여기서 도움이 되는 AWS 서비스는 무엇입니까? Amazon EMR은 이러한 오픈 소스 빅 데이터 처리 도구를 실행하기 위한 관리형 빅 데이터 처리 서비스입니다.

 

AWS에서 데이터 처리에 Spark를 사용하는 방법의 예를 살펴보겠습니다. Amazon EMR 클러스터를 시작하면 Amazon EMR은 Spark와 같은 빅 데이터 프레임워크를 실행하기 위한 관리형 서비스를 제공합니다. AWS Management Console을 통해 Amazon EMR 클러스터를 시작하거나 AWS Command Line Interface를 사용하여 인스턴스 유형, 인스턴스 수, Spark 버전 등 원하는 사양으로 클러스터를 생성할 수 있습니다.

 

Amazon EMR 클러스터가 실행되면 클러스터의 프라이머리 노드에 연결하여 명령줄 인터페이스에 액세스할 수 있습니다. 그런 다음 Spark 구성 파일을 수정하여 워크로드 요구 사항을 기반으로 메모리 할당, 실행기 코어 수, 병렬 처리 설정과 같은 속성을 설정하여 Spark를 구성할 수 있습니다.

그런 다음 AWS에서 처리할 수 있도록 데이터를 준비하고, Amazon S3에 데이터를 저장하고, 데이터 소스에 대한 적절한 권한 및 액세스 제어를 구성할 수 있습니다.

 

그러면 Scala, Python 또는 Java를 사용하여 Spark 애플리케이션을 개발할 수 있습니다. Spark API 및 기능을 사용하여 원하는 데이터 처리 작업을 수행할 코드를 작성할 수 있습니다. Spark는 데이터 정제, 집계, 필터링, 조인, 기계 학습 등 데이터 처리를 위한 일련의 변환, 작업 및 라이브러리를 제공합니다.

그런 다음 Spark-submit 명령을 사용하여 실행을 위해 Spark 애플리케이션을 Amazon EMR 클러스터에 제출합니다. 이때 메인 클래스 또는 스크립트 파일 및 필요한 명령줄 인수를 지정해야 합니다.

 

Amazon EMR은 클러스터 전체에서 Spark 애플리케이션의 시작 및 관리를 처리합니다. 늘 그렇듯이 Spark 애플리케이션의 진행 상황 및 성능을 모니터링해야 합니다. Amazon EMR은 애플리케이션의 실행 상태, 리소스 사용률을 추적하고 문제를 진단할 수 있는 로그, 지표 및 모니터링 기능을 제공합니다.

 

Spark 애플리케이션의 성능을 모니터링할 때 해야 할 질문은 더 나은 성능을 위해 사용할 수 있는 최적화 기술(예: 데이터 파티셔닝, 캐싱, Spark 구성 튜닝)이 무엇이냐는 것입니다. 데이터세트의 크기와 처리 요구 사항에 따라 Amazon EMR 클러스터를 확장하거나 축소하여 더 많은 리소스를 할당하거나 비용을 절감할 수 있습니다. 처리된 데이터를 요구 사항에 따라 Amazon S3, Amazon Redshift, Amazon RDS 또는 기타 호환 가능한 데이터 스토리지에 저장할 수 있습니다.

 

이러한 단계는 대규모 데이터 처리 워크로드에 Spark의 분산 컴퓨팅 기능과 AWS의 확장성을 사용하는 데 도움이 됩니다.

 

앞서 AWS는 서버리스를 위해 Spark 실행을 위한 AWS Glue를 제공한다고 이야기했습니다. 그러나 Amazon EMR의 경우 Spark를 실행하려는 클러스터에 특정 구성이 필요합니다. 따라서 데이터 처리에 Hadoop 에코시스템을 사용하는 경우 환경에 대한 더 많은 제어가 필요하다면 Amazon EMR이 더 나은 선택입니다.

 

데이터 준비는 데이터 처리 단계의 핵심 구성 요소입니다. 데이터 준비를 수행할 수 있는 서비스로는 Amazon Kinesis, Amazon EMR, AWS Glue가 있습니다. Amazon Managed Service for Apache Flink는 SQL 명령을 사용하여 기본 데이터 변환을 수행하는 훌륭한 도구입니다. 그러나 더 복잡한 변환이 필요할 수 있습니다. 이러한 상황에서는 Amazon EMR 또는 AWS Glue와 같은 서비스가 필요할 수 있습니다. 이러한 서비스는 보다 복잡한 변환을 위한 강력한 지원을 제공합니다.

 

여기서 질문이 있습니다. IoT 데이터를 안전하게 거의 실시간으로 수집하기 위한 파이프라인을 구축하는 경우 어떤 AWS 서비스를 사용할 수 있습니까? 모호하기는 하지만 안전하고 거의 실시간으로 수집한다고 표현했습니다. 여러분의 설계가 궁금한데요, 저는 가장 먼저 떠오르는 생각이 스트리밍 IoT 데이터를 Amazon S3에 수집하는 Kinesis Data Firehose입니다. 그리고 민감한 데이터를 모두 제거하고 이 데이터를 변환한 후 S3 버킷에 저장하는 Lambda 함수도 생성합니다.

 

이 시험에서는 이러한 서비스의 차이점을 이해해야 합니다. Amazon EMR은 모든 기능을 갖춘 분산형 Hadoop 환경입니다. 변환 능력은 AWS Glue가 가능한 것보다 훨씬 뛰어납니다. AWS Glue는 완전관리형 ETL 서비스이지만 Amazon EMR은 그렇지 않습니다. Amazon EMR을 사용하려면 Spark, Hive, Pig, HBase, Presto와 같은 추가 프레임워크와 소프트웨어를 설치해야 합니다. AWS Glue는 데이터 정리, 보강, 이동만 필요한 경우 훌륭한 솔루션입니다

 

한 가지 예를 살펴보겠습니다. 이 아키텍처에서는 Amazon EC2 인스턴스가 Apache 웹 서버를 실행하고 있습니다. 이 솔루션의 목표는 Kibana 대시보드를 사용하여 데이터를 시각화하고 사용자에게 제공하는 것입니다. Kinesis 에이전트가 Amazon EC2 인스턴스에 설치되고 데이터를 Kinesis Data Firehose로 지속적으로 전송하도록 구성됩니다. Kinesis Data Firehose는 스토리지용 S3 버킷으로 스트림의 데이터를 전송하도록 구성되어 있습니다. 이 데이터는 AWS Glue를 사용하여 카탈로그화할 수 있으며 데이터 레이크 솔루션의 일부로 사용할 수 있습니다. Kinesis Data Firehose도 처리를 위해 스트림의 데이터를 Amazon Managed Service for Apache Flink로 전송하도록 구성되어 있습니다. Amazon Managed Service for Apache Flink는 SQL 명령 또는 기본 제공 기계 학습 기능을 사용하여 스트림의 데이터를 집계할 수 있습니다. 그런 다음 Amazon Managed Service for Apache Flink는 변환된 데이터를 다른 Kinesis Data Firehose로 보냅니다. Kinesis Data Firehose는 사용자 보고서에 사용하기 위해 인덱싱될 데이터를 Amazon OpenSearch Service로 보냅니다. 그러면 Kibana가 Amazon OpenSearch Service에 연결하여 사용자를 위한 로그 레코드의 시각화를 구축할 수 있습니다.

 

기본 사항 강의에서 언급했듯이 데이터 엔지니어는 보안, 데이터 관리, 오케스트레이션, 데이터 아키텍처, 소프트웨어 엔지니어링, 운영을 통합하여 수명 주기를 관리해야 합니다. 변환은 오케스트레이션에 의존하며 오케스트레이션은 여러 테이블, 데이터세트 및 시스템에 걸쳐 실행될 수 있는 중간 변환 및 전송 파이프라인과 같은 다양한 작업을 결합합니다. 변환은 스키마 업데이트, 데이터 랭글링, 패턴 업데이트 등을 수행할 수 있습니다.

이러한 작업뿐만 아니라 스트리밍 변환 및 처리도 이해해야 합니다. 또한 처리 및 ETL 워크로드에 어떤 서비스를 언제 선택할지도 이해해야 합니다. 이 과정의 유료 버전에서는 ETL 워크로드 처리에 도움이 될 수 있는 AWS 서비스에 대해 더 자세히 알아볼 수 있도록 플래시 카드를 추가할 테니 꼭 확인하시기 바랍니다.

 

여러분이 근무하는 의류 회사에서 데이터 카탈로그와 통합된 S3 버킷에 모든 과거 트랜잭션을 저장하여 과거 트랜잭션을 판매 실적 보고서 데이터와 조인한다고 가정해 보겠습니다. 데이터 처리는 Amazon Redshift 클러스터에서 완료됩니다. 데이터 분석가가 여러분에게 Amazon Redshift 클러스터의 워크로드를 줄이기 위한 솔루션을 제공해 달라고 요청했습니다. 여러분의 솔루션은 무엇입니까? 저는 가장 먼저 떠오르는 생각이 Amazon Redshift SQL을 사용하여 테이블을 조인하고 Redshift Spectrum을 사용하여 Amazon S3에서 과거 트랜잭션 데이터를 위한 외부 테이블을 생성하는 것입니다.

 

저에게 변환 실패 및 성능 문제 해결 방법을 묻는다면 동일한 문제 해결 접근 방식을 따릅니다. 첫째, 데이터 변환에 사용되는 AWS 서비스에서 생성된 로그를 확인합니다. 둘째, 다양한 단계에서 데이터 품질 및 무결성을 확인합니다. 또 하나의 단계는 소스 데이터에서 변환 로직을 검증하여 매핑, 필터, 집계 및 계산이 모두 올바른지 확인하는 것입니다.

 

변환의 일부로 AWS Glue를 사용하고 있다고 가정해 보겠습니다. 변환을 개발 모드에서 실행하거나 디버깅 모드에서 실행하여 상세한 로그를 수집하고 중간 결과를 조사할 수 있습니다. AWS Glue에는 데이터, 중간 결과 및 실행 계획을 검사하는 데 도움이 되도록 다양한 프로그래밍 언어 또는 프레임워크에 대한 디버깅 기능이 내장되어 있습니다. 때로는 문제를 더 잘 격리하고 식별하기 위해 이 디버깅 단계에서 더 작은 데이터세트를 사용하는 것이 도움이 됩니다.

 

여기서 잠시 멈추고 질문 하나 하겠습니다. 이 문제 해결 프로세스가 성능 최적화를 보장할 수 있는 예는 무엇입니까? 데이터 변환 코드 및 쿼리의 실행을 프로파일링하고 분석하면 처리 시간, 메모리 사용량 또는 입력/출력 작업이 더 높은 영역 및 병목 현상을 식별할 수 있습니다. 데이터 변환 작업을 최적화하기 위해 병렬 처리에 추가해야 하는 보다 효율적인 알고리즘, 파티셔닝 전략 등을 식별할 수도 있습니다. 또는 메모리 할당, 실행기 코어 수, 병렬 구성 등 성능을 최적화기 위해 업데이트할 리소스 할당 설정을 식별합니다. 또한 캐싱을 통합하여 데이터 액세스 및 처리 시간을 줄이는 방법도 식별할 수 있습니다.

 

그러나 AWS에서 일반적인 변환 실패 및 성능에 대한 문제 해결 및 디버깅으로 돌아가자면 증분 처리 기술을 구현하여 새 데이터 또는 수정된 데이터에 대한 변환만 처리하도록 할 수도 있습니다. 그리고 네트워크 문제, 리소스 가용성과 같은 일시적인 실패 또는 오류를 처리하기 위한 재시도를 통합합니다.

또한 데이터 변환 프로세스를 테스트해야 하며, 테스트 또는 스테이징 환경을 사용하고 단위 테스트, 통합 테스트 및 엔드투엔드 테스트를 구현하고 다양한 데이터 시나리오, 실패 시나리오 및 희귀 사례를 사용하여 테스트해야 합니다.

 

이 작업 설명에서는 데이터 준비, 큐레이션, 보강의 기본 사항을 포함하여 데이터 분석 파이프라인 중 처리 단계의 핵심 부분도 다룹니다. 시간이 걸리더라도 처리 솔루션의 주요 차이점과 여러 솔루션 중 특정 솔루션을 사용해야 하는 경우를 확실히 이해해야 합니다. 애플리케이션 로그를 Amazon CloudWatch Logs로 보내고 거의 실시간으로 DynamoDB 데이터로 로그를 보강한 다음 추가 연구를 위해 출력을 사용해야 한다고 가정해 보겠습니다. 로그를 어떻게 수집하고 보강합니까?

 

한 가지 솔루션은 DynamoDB 데이터로 로그를 보강하는 Lambda 함수를 생성하고, Kinesis Data Firehose 스트림을 생성하고, S3 버킷을 로그 대상으로 사용하여 CloudWatch Logs를 구독하도록 스트림을 구성하는 것입니다. 그런 다음 로그 이벤트를 전송 스트림으로 보내는 CloudWatch Logs 구독을 생성합니다. Kinesis Data Firehose는 Kinesis Data Streams 또는 CloudWatch Logs와 같은 소스에서 Amazon Managed Service for Apache Flink, Amazon S3와 같은 다운스트림 서비스로 스트리밍 데이터를 수집, 변환, 로드합니다.



방금 보안 통합에 대해 이야기했는데요. 이 시험에서는 JDBC 및 ODBC(Open Database Connectivity)를 사용하여 다양한 데이터 소스에 연결하는 방법을 알고 있어야 합니다.

 

JDBC는 관계형 데이터베이스와 연결하고 상호 작용하기 위한 Java API입니다. AWS에서 JDBC를 사용하여 데이터 소스에 연결하려면 먼저 연결하려는 특정 데이터베이스를 위한 JDBC 드라이버를 준비합니다. 다음으로, 데이터베이스에 필요한 포트에 액세스하기 위한 인바운드 규칙을 사용하여 필요한 보안 그룹을 구성합니다. 그런 다음 Java 애플리케이션에서 JDBC 드라이버를 사용하여 적절한 연결 URL, 사용자 이름 및 암호를 제공해 데이터베이스에 대한 연결을 설정합니다. 연결 URL에는 일반적으로 데이터베이스 호스트 이름 또는 IP 주소, 포트 번호 및 데이터베이스 이름이 포함됩니다. 연결이 설정되면 JDBC API를 사용하여 SQL 문을 실행하고 쿼리, 업데이트 및 기타 데이터베이스 작업을 수행할 수 있습니다.

 

ODBC는 관계형 데이터베이스를 포함하여 다양한 데이터 소스에 연결하기 위한 표준화된 API입니다. 먼저 연결하려는 데이터 소스에 해당하는 ODBC 드라이버를 선택합니다. 그런 다음 데이터 소스에 연결할 시스템(Amazon EC2 인스턴스 또는 온프레미스 서버일 수 있음)에 ODBC 드라이버를 설치합니다. 그런 다음 데이터 소스에 연결하는 데 필요한 구성 정보가 포함된 ODBC 데이터 소스 이름을 설정합니다. 여기에는 드라이버, 연결 세부 정보 (예: 호스트 이름, 포트, 인증) 및 기타 설정이 포함됩니다. 그런 다음 ODBC 연결을 지원하는 애플리케이션 또는 도구에서 DSN을 지정하여 데이터 소스에 대한 연결을 설정합니다. 그러면 쿼리를 실행하고 데이터와 상호 작용할 수 있습니다.

 

Amazon RDS, Aurora 및 Amazon Redshift는 AWS 내의 데이터베이스 및 AWS 서비스에 대한 연결 프로세스를 단순화하고 직접 연결 옵션과 자동화된 관리를 제공하여 수동으로 JDBC 또는 ODBC 구성할 필요성을 줄여줍니다.

 

사용하는 데이터 소스와 AWS 환경의 특정 요구 사항을 기반으로 적절한 연결 방법을 선택하는 방법을 이해하십시오.

 

이번 강의를 마무리하면서 다른 시스템에서 데이터를 사용할 수 있도록 AWS 서비스를 사용하여 데이터 API를 생성하는 방법에 대해 이야기해 보겠습니다. 이 시험에서는 AWS 서비스와 기술을 조합하여 사용하는 방법을 이해해야 합니다. 이에 대해서는 이미 조금 다룬 적이 있습니다.

 

먼저, 데이터가 저장되는 위치를 알아야 합니다. 데이터 처리 또는 변환을 수행해야 하는 경우 AWS는 데이터를 정리하고, 정규화하고 사용 가능한 형식으로 변환하는 데 도움이 되는 AWS Glue 또는 Amazon EMR을 제공합니다. API Gateway를 사용하여 데이터 API를 생성하고 관리할 수 있습니다. API Gateway는 프런트엔드 서비스에 액세스하여 클라이언트가 제어된 액세스를 통해 데이터에 안전하게 액세스할 수 있도록 합니다. 인증, 권한 부여, 속도 제한, 캐싱, 요청, 응답 변환을 제공합니다. Lambda를 사용하여 데이터를 처리하고 API에 제공할 수 있습니다. Lambda에서 코드를 작성하여 데이터 소스에서 데이터를 가져오고, 추가 변환을 적용하고, 형식이 지정된 데이터로 API 요청에 응답할 수 있습니다. 또한 요구 사항에 따라 API Gateway 또는 기타 AWS 서비스를 통해 Lambda를 트리거할 수 있습니다.

 

보안과 인증은 어떻습니까? AWS Identity and Access Management(AWS IAM)를 사용하여 API Gateway와 Lambda 함수에 대한 액세스 및 권한을 제어함으로써 데이터 API를 보호하는 보안 조치를 구현할 수 있습니다. AWS Certificate Manager, API 키, OAuth 또는 기타 인증 메커니즘을 사용하여 API 엔드포인트를 보호할 수도 있습니다.

 

캐싱 및 성능 최적화에는 어떤 AWS 서비스를 사용할 수 있습니까? 데이터 API의 성능을 향상하려면 ElastiCache 또는 API Gateway에 내장된 캐싱 기능을 사용하여 응답을 캐시하고 지연 시간을 줄일 수 있습니다. 또한 Amazon CloudWatch를 사용하여 데이터 API의 성능, 지연 시간, 사용량을 모니터링하고 추적할 수 있습니다. 그리고 문제 해결 및 분석을 위해 API 요청. 로그 및 오류를 캡처하도록 로깅을 구성할 수 있습니다. API Gateway를 사용하여 API 버전을 관리할 수 있으며 변경 사항을 관리하고 이전 버전 호환성을 유지할 수도 있습니다.

 

확장 및 가용성을 위해 Lambda 함수에 자동 크기 조정을 사용하고 고 동시성 및 가용성 요구 사항을 처리하도록 API Gateway를 구성할 수 있습니다.

 

테스트 및 검증의 경우 지속적 통합 및 배포를 위해 AWS CodePipeline 및 AWS CodeBuild를 사용하여 테스트 및 배포 프로세스를 자동화할 수 있습니다.

 

이 시험에서는 안전하고 확장 가능한 데이터 API를 생성하고 데이터를 다른 시스템에서 사용할 수 있도록 하는 데 사용할 다양한 AWS 서비스를 알아야 합니다.
 

다음에는 영역 1의 세 번째 작업 설명인 데이터 파이프라인 오케스트레이션을 시작하겠습니다.

## 3. 데이터 파이프라인

영역 1의 세 번째 작업 설명인 데이터 파이프라인 오케스트레이션을 시작하겠습니다. 기본 사항 강의에서 언급했듯이 데이터 수명 주기를 관리하려면 데이터 아키텍처를 통합해야 합니다.

 

데이터 아키텍처는 기본적으로 조직의 진화하는 데이터 요구 사항을 지원하는 시스템 설계입니다. 고려해야 할 운영 및 기술적 측면이 있습니다. 운영 아키텍처는 기능 요구 사항입니다. 기술 아키텍처는 데이터가 수집, 저장, 변환 및 제공되는 방식입니다.

 

최적의 데이터 아키텍처 구축에 대한 자세한 내용은 AWS Well-Architected Framework 및 6가지 핵심 요소를 참조하십시오. 다음은 몇 가지 질문입니다. 모놀리식 아키텍처 또는 마이크로서비스를 사용합니까? 단일 테넌트 또는 다중 테넌트에 대한 고려 사항은 무엇입니까? 저는 성능과 보안을 최우선으로 생각합니다. 이벤트 기반 아키텍처나 서버리스, 컨테이너는 어떻습니까? 제어 기능, 유연성 및 비용은 몇 가지 요소입니다.

 

이 시험에서는 데이터 웨어하우스, 데이터 레이크 데이터 레이크 하우스, 데이터 스택, Lambda, Kappa, 데이터 메시 등과 같은 데이터 아키텍처의 유형 및 예를 알아야 합니다. 이러한 다양한 데이터 아키텍처에서 데이터 파이프라인을 구축하여 소스 시스템에서 데이터를 가져오고 변환할 수 있습니다. 여기에는 종종 여러 단계가 포함되며 데이터 파이프라인의 각 단계를 통과하는 데이터를 추가로 변환하거나 보강할 수 있습니다.

 

서로 다른 데이터세트에서 독립적으로 또는 서로 결합하여 작동하는 수십 또는 수백 개의 파이프라인이 있을 수 있고 다양한 유형의 변환을 수행할 수도 있습니다. 각 파이프라인은 요구 사항 및 목표를 충족하기 위해 여러 서비스를 사용할 수 있으며 이러한 서비스 및 파이프라인을 모두 오케스트레이션하는 것은 복잡할 수 있습니다. 하지만 AWS는 데이터 파이프라인을 조정하는 데 도움이 되는 서비스를 제공합니다.

 

데이터 오케스트레이션은 데이터가 수집 서비스에서 처리 서비스를 거쳐 스토리지 위치로 올바르게 흐르도록 보장하는 프로세스입니다. 이 시험에서는 파이프라인 오케스트레이션의 핵심 개념을 이해해야 합니다. 데이터 파이프라인을 정의할 수 있습니까? 데이터 파이프라인은 특정 순서로 실행되어야 하는 데이터 처리 태스크의 모음입니다.

 

일부 태스크는 순차적으로 실행되어야 하지만 다른 태스크는 병렬로 실행되어야 할 수도 있습니다. 태스크 순서를 지정하는 것은 워크플로입니다.

 

오케스트레이션을 위한 서버리스 AWS 서비스에는 무엇이 있습니까? Step Functions, Lambda가 있습니다. Step Functions의 경우 상태 머신 언어를 사용하여 상태, 전환, 오류 처리, 재시도, 입력, 출력, 이벤트 소스, 트리거, 모니터링, 로깅, 테스트, 배포를 설계합니다.

 

데이터 파이프라인 오케스트레이션에는 데이터 파이프라인 워크플로에 대한 태스크 실행 자동화도 포함됩니다. 여기에는 다양한 태스크 간의 종속성을 관리하는 태스크와 파이프라인이 예상대로 실행되도록 보장하는 태스크가 포함됩니다.

데이터 파이프라인의 예에는 무엇이 있습니까? 저는 시험 준비 과정 중 하나에서 정기적으로 피드백을 수신하는 파이프라인을 작업 중입니다. 이 데이터 파이프라인은 수신되는 데이터가 유효한지 확인합니다. 그런 다음 해당 데이터 파일을 Parquet 형식으로 변환합니다.

 

제가 희망하는 두 번째 파이프라인은 매일 특정 시간에 실행되어 제가 모든 피드백을 수신했는지 확인한 다음 Spark 작업을 실행하여 데이터세트를 조인하고 추가 데이터로 해당 데이터를 보강합니다.

 

그런 다음 세 번째 파이프라인이 새로 보강된 데이터를 데이터 웨어하우스로 로드합니다. 그러면 긍정적인 피드백, 부정적인 피드백, 중립적인 피드백, 평점 등에 대한 쿼리를 실행할 수 있습니다. 비용 최적화를 위해 Amazon Redshift를 Amazon S3and 기반 데이터 레이크로 전환하고, 대신 Athena를 사용하여 쿼리를 실행할 수 있습니다.

 

다음은 지식의 심도를 테스트하기 위한 질문입니다. 데이터 파이프라인을 Directed Acrylic Graph로 정의해야 합니까? 아니요. 하지만 특정 오케스트레이션 도구에는 필요합니다. 예를 들어 Apache Airflow에서는 파이프라인을 DAG로 정의해야 합니다. Step Functions는 파이프라인이 반드시 DAG일 것을 요구하지 않습니다.

 

다른 질문도 살펴보겠습니다. 데이터 파이프라인을 트리거하여 일정 기반 파이프라인과 이벤트 기반 파이프라인을 실행하려면 어떻게 해야 할까요? 방금 언급한 제가 구축 중인 파이프라인으로 돌아가 보겠습니다. 첫 번째 파이프라인은 업로드되는 새 피드백 파일에 대한 응답으로 실행되는 이벤트 기반 파이프라인입니다.

 

저의 데이터 파이프라인에 대해 이야기하고 있으니 다른 질문을 또 해보겠습니다. 제가 모든 소용량 파일을 배치로 처리하여 단일 Parquet 파일로 변환하기로 결정하면 어떻게 됩니까? 무엇이 필요할까요? 파일 배치의 마지막에 파일 배치를 구성하는 다른 파일에 대한 정보가 포함된 매니페스트 파일이 필요합니다. 더 자세히 들어가서, 이 파일을 어떻게 구성해야 합니까? manifest라는 이름으로 시작하는 파일이 S3 버킷에 기록될 때만 시작하는 S3 이벤트 알림을 사용할 수 있습니다. 이러한 이벤트가 발생하면 저의 파이프라인이 트리거되고 첫 번째 단계는 매니페스트 파일을 읽은 다음 매니페스트 파일에 나열된 각 파일이 존재하는지 확인하는 것입니다. 그리고 모든 업로드 이벤트에 응답하지 않더라도 이 파이프라인은 이벤트 중심 파이프라인이기도 합니다.

 

파이프라인 구축 및 구성에서 정말 중요한 부분 중 하나는 파이프라인에서 단계 실패를 처리하는 방식입니다. 데이터 파이프라인에서 실패가 발생하는 일반적인 이유에는 무엇이 있습니까? 파이프라인이 처리를 위해 CSV 파일을 수신해야 하지만 대신 JSON 형식의 파일을 수신하는 경우와 같은 데이터 품질 문제가 있습니다. 파이프라인은 JSON 형식 파일을 처리하는 방법을 모르므로 이로 인해 실패가 발생합니다. 데이터 품질 문제가 해결될 때까지 작업을 복구할 수 없으므로 아마도 하드 실패가 될 것입니다.

 

코드 오류도 있을 수 있습니다. 다음 강의에서 프로그래밍 개념을 적용하는 방법에 대해 더 자세히 살펴보겠습니다. 하지만 작업을 업데이트하다가 실수로 코드에 로직 또는 구문 오류가 발생했다고 가정해 보겠습니다. 때로는 코드를 배포 전에 테스트하더라도 특정 오류를 포착하지 못할 수 있습니다. 그러나 코드를 배포 전에 테스트하는 것이 항상 모범 사례입니다. 코드를 수정한 다음 다시 배포해야 하므로 이로 인해서도 하드 실패가 발생할 수 있습니다.

 

엔드포인트 오류도 있을 수 있습니다. 엔드포인트 오류로 인한 소프트 실패의 예는 무엇입니까? 엔드포인트 오류가 일시적인 네트워크 오류로 인해 발생한 경우 연결을 다시 시도하면 오류를 해결할 수 있으므로 이는 소프트 실패일 것입니다. 그러나 작업이 정확하게 필요한 권한으로 구성되지 않았다면 이는 하드 실패가 됩니다.

데이터 파이프라인은 여러 단계와 복잡한 종속성으로 구성되기 때문에 종속성 오류도 있습니다. 이는 파이프라인 내부의 종속성일 수도 있고 파이프라인 간의 종속성일 수도 있습니다.

 

그렇다면 실패 재시도 전략을 사용하여 어떻게 파이프라인을 오케스트레이션합니까? Apache Airflow, Step Functions와 같은 여러 오케스트레이션 도구는 재시도 횟수, 재시도 간격, 백오프 비율을 지정할 수 있는 기능도 제공합니다. 이것은 어떻게 작동할까요?

 

예를 살펴보겠습니다. Step Functions에서는 백오프 비유 값을 지정할 수 있습니다. 재시도 지연 시간은 이 값만큼 증가합니다. 재시도 간격을 10초로 지정하고 백오프 비율을 1.5로 지정한다고 가정해 보겠습니다. Step Functions는 10초 동안 기다립니다. 두 번째 재시도에서는 Step Functions가 15초 동안 기다립니다. 세 번째 재시도에서는 Step Functions가 22.5초 동안 기다리는 식입니다.

 

문제 또는 실패에 대한 알림을 보내기 위해 AWS 알림 서비스를 어떻게 사용합니까? 모니터링과 알림은 데이터 파이프라인 자동화 프로세스의 필수적인 부분입니다. 방금 재시도를 설명하는 데 사용했던 Step Functions 예제로 돌아가 보겠습니다. 상태 머신에 오류가 있는 경우 담당자 또는 팀에 해당 오류를 경고하는 이메일 알림을 보내도록 구성할 수 있습니다.

 

Amazon Simple Notification Service(Amazon SNS)를 사용하여 이메일을 발송할 수 있습니다. Amazon SNS 주제를 생성하고 하나 이상의 이메일 주소에서 해당 주제를 구독하는 방법을 알아야 합니다. 이 과정을 진행하면서, 특히 영역 3 데이터 운영 및 지원에서 경고 및 모니터링에 대해 더 자세히 다룰 것입니다.

 

데이터 파이프라인 오케스트레이션을 위한 AWS 서비스에 대해 자세히 살펴보겠습니다. AWS Data Pipeline 또는 Step Functions를 사용하는 서버리스 오케스트레이션 엔진이 있습니다. Amazon Managed Workflows for Apache Airflow(Amazon MWAA)를 사용하는 오픈 소스 관리형 프로젝트가 있고, AWS Glue 워크플로를 사용하는 서비스별 오케스트레이션이 있습니다.

 

이 시험에서는 다양한 사용 사례, 요구 사항, 관리 작업, 통합 작업, 로깅, 오류 처리, 성능 및 비용을 고려하여 각 솔루션을 언제 사용해야 하는지 알아야 합니다. AWS Data Pipeline은 지정된 간격으로 AWS 데이터 소스와 온프레미스 데이터 소스 간에 데이터를 추출, 변환, 로드하는 기능을 제공합니다. Data Pipeline을 사용하면 저장된 데이터에 정기적으로 액세스하고, 대규모로 데이터를 변환 및 처리하며, Amazon S3, Amazon RDS, Amazon Redshift, DynamoDB, Amazon EMR과 같은 AWS 서비스에 그 결과를 전송할 수 있습니다.

 

여기서 질문이 있습니다. AWS Data Pipeline을 사용하여 온프레미스 데이터베이스와 같은 다른 JDBC 데이터 스토어에서 읽고 쓸 수 있습니까? 예. Amazon EC2, Amazon ECR 및 온프레미스 컴퓨팅 리소스를 사용하여 데이터 변환 작업을 실행할 수 있습니다. Step Functions는 시각적 설계 도구를 제공하는 서버리스 오케스트레이션 서비스입니다. 디자인 문서 같아서 제가 좋아하는 도구인데 로우코드 접근 방식을 사용하여 데이터 파이프라인 및 서버리스 애플리케이션을 개발하는 데 도움을 줍니다. 이 시각적 설계 도구에서는 기본 제공 통합을 사용하여 드래그 앤 드롭할 수 있습니다. 또는 Amazon States Language를 JSON과 함께 직접 사용하여 파이프라인을 정의할 수 있습니다.

 

몇 가지 Spark 작업을 특정 순서로 실행하려는 간단한 사용 사례가 있는데, 이 작업을 오케스트레이션하거나 별도의 애플리케이션을 유지 관리하는 데 시간을 쓰고 싶지 않다고 가정해 보십시오. Step Functions를 사용하여 이를 수행할 수 있는데, Step Functions에서 전체 워크플로를 생성하고 Amazon EMR에서 Spark와 상호 작용할 수 있습니다.

 

그럼 단계를 살펴보겠습니다.

 

첫째, 입력 파일 경로를 전달하여 Step Function 상태 머신을 트리거합니다.

둘째, 상태 머신의 첫 번째 단계가 Lambda 함수를 시작합니다.

셋째, Lambda 함수가 Amazon EMR에서 실행되는 Spark와 상호 작용하여 Spark 작업을 제출합니다.

넷째, 상태 머신이 Spark 작업 상태를 확인하기 전에 몇 초 동안 기다립니다.

다섯째, 작업 상태에 따라 상태 머신이 성공 또는 실패 상태로 이동합니다.

여섯째, 상태 머신이 작업이 완료될 때까지 몇 초 동안 기다립니다. 작업이 완료되면 상태 머신이 최종 상태로 업데이트됩니다.

 

덜 복잡한 환경인 경우 AWS Data Pipeline 또는 Step Functions 무엇을 선택하시겠습니까? AWS Data Pipeline을 선택하고 더 복잡한 환경에서는 Step Functions를 선택하겠습니다.

 

앞으로 환경 복잡성에 따라 사용할 수 있는 더 많은 AWS 서비스를 살펴보겠습니다. 다음에는 AWS Glue 워크플로를 시작하겠습니다. 데이터 파이프라인이 AWS Glue 구성 요소만 사용하는 경우 AWS Glue 워크플로가 훌륭한 선택입니다. AWS Glue 워크플로는 데이터 파이프라인을 구축하기 위한 AWS Glue 서비스의 일부로, 워크플로를 생성, 시각화, 실행하는 기능을 제공합니다.

 

AWS Glue 워크플로를 사용하여 워크플로 그래프에 트리거를 추가하고 각 트리거에 대해 감시되는 이벤트와 작업을 정의하여 워크플로를 구축합니다. 몇 가지 예를 살펴보겠습니다. AWS Glue 워크플로를 사용하여 어떤 유형의 파이프라인을 생성할 수 있습니까? 제가 생각하는 파이프라인 하나는 AWS Glue 크롤러를 실행하여 수집된 CSV 파일을 데이터 카탈로그의 새 파티션에 추가하는 것입니다. 그런 다음 AWS Glue Spark 작업을 실행하고 카탈로그를 사용하여 새 데이터를 읽습니다. 그리고 CSV 파일에 있는 데이터를 Parquet 파일로 변환합니다.

 

그런 다음 새 AWS Glue 크롤러를 실행하여 변환된 Parquet 파일을 데이터 카탈로그에 추가합니다. 여기에서 더 자세히 살펴보고 두 개의 작업을 병렬로 실행해 보겠습니다. 한 AWS Glue 작업은 데이터를 집계하고 결과를 Amazon DynamoDB 테이블에 쓸 수 있습니다. 다른 AWS Glue 작업은 새 데이터를 해당 데이터의 기존 참조 데이터세트에 조인하는 새로운 보강된 데이터세트를 생성할 수 있습니다. 마지막으로 AWS Glue Python 셸 작업을 실행하여 작업 성공 또는 실패에 대한 알림을 보냅니다.

 

AWS Glue 워크플로에서 Hive 작업을 실행하기 위해 Amazon EMR이 필요한 사용 사례가 있다면 어떻게 합니까? AWS Glue가 아닌 서비스는 어떻게 통합합니까? Boto3 라이브러리를 사용하여 Amazon EMR 또는 Amazon Simple Queue Service(Amazon SQS)와 같은 다른 AWS 서비스와 상호 작용하는 AWS Glue Python 셸 작업을 실행하면 Amazon SQS 대기열에 파일을 기록할 수 있습니다.

더 복잡한 데이터 파이프라인의 통합에 대해 좀 더 자세히 살펴보겠습니다. Step Functions를 사용하면 복잡한 ETL 워크플로와 관련된 여러 ETL 작업을 오케스트레이션할 수 있습니다. 예를 들어 온라인 사용자 참여와 예상 판매 수익 및 기회 간의 상관 관계를 살펴보고 싶다고 합시다. Step Functions를 사용하여 여러 AWS Glue 작업을 조정하여 분석할 데이터를 혼합하고 준비할 수 있습니다.

 

전체 데이터 파이프라인의 예를 살펴보겠습니다. Amazon S3를 기본 데이터 스토어로 사용하여 서버리스 데이터 레이크를 구축합니다. 다양한 방법을 사용하여 Amazon S3에 데이터를 수집하고 저장할 수 있습니다. 예를 들어 Amazon Kinesis Data Firehose를 사용하여 스트리밍 데이터를 수집할 수 있습니다. AWS DMS를 사용하여 기존 데이터베이스에서 관계형 데이터를 수집할 수 있고, DataSync를 사용하여 온프레미스 NFS에서 파일을 수집할 수 있습니다. 데이터는 원시 영역이라는 S3 버킷으로 수집됩니다. 해당 데이터를 사용할 수 있도록 하려면 데이터 카탈로그에서 해당 스키마를 카탈로그화합니다. Amazon S3에 의해 호출된 Lambda 함수를 사용하여 데이터를 카탈로그화하는 AWS Glue 크롤러를 시작하면 이를 수행할 수 있습니다. 크롤러가 테이블 정의 생성을 마치면 Amazon CloudWatch Events 규칙을 사용하여 두 번째 Lambda 함수를 호출합니다.

 

이 단계에서는 AWS Glue ETL 작업을 시작하여 데이터를 처리하고 처리된 영역이라는 또 다른 S3 버킷으로 출력합니다. AWS Glue ETL 작업은 데이터를 Parquet 형식으로 변환하여 처리된 S3 버킷에 저장합니다. ETL 작업을 수정하여 보다 세분화된 파티셔닝, 압축 또는 데이터 보강과 같은 다른 목표를 달성할 수 있습니다.

 

모니터링과 알림은 자동화 프로세스의 필수적인 부분입니다. ETL 작업이 완료되는 즉시 또 다른 CloudWatch 규칙이 Amazon SNS 주제를 사용하여 이메일 알림을 보냅니다. 이 알림은 데이터가 성공적으로 처리되었음을 나타냅니다. 이 부분은 이번 강의의 이전 섹션에서 언급한 AWS 알림 서비스 사용과 연결됩니다. 이 파이프라인은 데이터를 분류 및 변환하고 완료 시 이메일 알림을 보냅니다.

 

이제까지는 더 복잡한 데이터 파이프라인을 다뤘습니다. 계속해서 덜 복잡한 환경을 위한 AWS 서비스에 대해서도 이야기해 보겠습니다. Apache Airflow는 워크플로를 작성, 모니터링, 예약하는 데 도움이 되는 오픈 소스 오케스트레이션 소프트웨어입니다. AWS에는 Apache Airflow의 관리형 버전인 Amazon MWAA가 있습니다.

 

이번 강의의 시작 부분에서 언급했던 핵심 개념의 기본 사항으로 돌아가 보겠습니다. Apache Airflow의 경우 데이터 파이프라인이 DAG로 생성됩니까? 예. Python을 사용하여 DAG를 정의합니다. 그러면 DAG가 파이프라인의 태스크와 태스크 간 종속성을 제공합니다. Amazon MWAA를 사용하면 DAG를 정의 및 예약하여 AWS Glue 태스크, Lambda 함수 또는 데이터 ETL 파이프라인 내의 다른 사용자 지정 태스크와 같은 태스크를 실행할 수 있습니다. AWS는 AWS Glue 구성 요소를 오케스트레이션하기 위한 AWS Glue 워크플로를 제공합니다. 이 워크플로는 AWS Glue 크롤러 및 AWS ETL 작업을 실행할 수 있는 순서가 지정된 단계의 시퀀스로 구성됩니다.

 

그렇다면 이러한 유형의 워크플로는 어떤 모습일까요? 매우 복잡해질 수 있지만 간단히 흐름만 살펴보겠습니다. AWS Glue 크롤러는 매일 실행되어 데이터 레이크의 원시 영역에서 새로 수집된 데이터를 데이터 카탈로그에 추가합니다. 크롤러가 완료되면 AWS Glue ETL 작업이 시작되어 원시 CSV 데이터를 Parquet 형식으로 변환한 후 데이터 레이크의 큐레이팅된 영역에 기록합니다. AWS Glue 작업이 완료되면 AWS Glue 크롤러가 시작되어 새로 변환된 데이터를 큐레이팅된 영역, 그리고 데이터 카탈로그에 추가합니다. 데이터 과학자가 두 개의 AWS 리전에 있는 두 개의 S3 버킷에 저장된 데이터에 대해 임시 쿼리를 실행하도록 파이프라인을 오케스트레이션한다고 가정해 보겠습니다.

 

데이터 과학자가 Athena를 사용하여 두 리전 모두의 데이터를 쿼리할 수 있도록 파이프라인을 오케스트레이션하려면 어떻게 합니까? 한 리전에서 AWS Glue 크롤러를 실행하여 모든 리전의 데이터세트를 카탈로그화할 수 있으며, 데이터가 크롤링되면 데이터 과학자가 Athena 쿼리를 실행할 수 있습니다.

 

보다 복잡한 워크플로를 위해 Step Functions를 사용하여 데이터 변환 파이프라인을 오케스트레이션하는 또 다른 옵션을 살펴보겠습니다. 먼저 개략적으로 살펴보겠습니다. JSON 데이터를 워크플로의 첫 번째 상태에 전달되는 입력 텍스트로 포함하여 상태 머신을 시작합니다.

 

워크플로의 첫 번째 상태는 이 입력 데이터를 사용하고 구성된 기능을 수행합니다. 상태 머신에 전달된 입력을 사용하여 Lambda 함수를 실행하고 함수가 JSON 데이터를 수정한 다음 수정된 JSON 데이터를 워크플로의 다음 상태로 전달하는 경우를 예로 들 수 있습니다. EventBridge를 사용하여 일정 기반 또는 이벤트 기반으로 Step Function을 시작하거나 Step Functions API를 호출하여 필요 시 Step Function을 시작할 수 있습니다. 이제 각 단계를 살펴보겠습니다.

 

먼저, 파일이 S3 버킷에 업로드되면 CloudWatch 이벤트가 트리거됩니다. 이 CloudWatch 이벤트는 버킷에 새로 업로드된 파일의 위치가 포함된 JSON 객체를 전달하는 상태 머신을 시작합니다. 다음 단계는 작업이 성공했는지 여부입니다. 이 단계는 일종의 선택입니다. 마지막 단계에서 전달된 JSON 데이터를 검사하여 작업 상태 필드가 성공으로 설정되었으면 AWS Glue 크롤러 실행 단계로 분기됩니다. 작업 상태 필드가 실패로 설정되었으면 작업 실패로 분기됩니다.

 

AWS Glue 크롤러 실행 단계에서는 Lambda 함수가 시작되고, 이 함수는 이전 Lambda 함수가 Parquet 파일을 기록한 위치에 대해 실행되도록 AWS Glue 크롤러를 시작합니다. 작업 실패 단계에서는 상태 머신의 실행을 중지하고 실행을 실패로 표시합니다. 상태 머신은 catch 문을 사용하여 상태 머신 실행이 오류 상태임을 감지하면 오류 단계를 실행합니다.

 

오류 단계에서는 또 다른 Lambda 함수가 트리거되어 데이터 엔지니어링 팀에 파일 처리가 실패했음을 알리는 알림을 보냅니다. 데이터 파이프라인 및 워크플로에는 여러 분야에 걸친 복잡하고 반복적인 단계가 많이 포함됩니다.

 

일반적인 기계 학습 모델 개발 워크플로의 또 다른 예는 데이터 준비에서 시작하여 모델 훈련 및 튜닝, 프로덕션, 배포로 이어집니다. 그러나 이러한 단계에는 각각 여러 하위 태스크가 있습니다.

데이터 파이프라인은 소스 시스템에서 시작하므로 생성 단계이지만, 수집은 데이터 엔지니어가 데이터 파이프라인 활동을 설계하기 시작하는 단계(스테이지)이며 오케스트레이션은 복잡한 파이프라인을 관리하고 여러 시스템으로 확장하는 데 도움이 될 수 있습니다. 일반적으로 데이터 분석 파이프라인의 단계는 다음 순서를 따릅니다.



첫째, 적절한 도구가 데이터를 수집하고 조정합니다.

둘째, 데이터가 영구적 방식으로 저장됩니다.

셋째, 데이터가 변환, 처리 또는 분석됩니다. 이 데이터 처리 및 분석 솔루션이 스토리지로부터 데이터를 가져와서 작업을 수행한 후 흔히 새로운 위치에 데이터를 다시 저장합니다.

그런 다음 필요에 따라 다른 도구가 데이터를 처리 및 분석할 수도 있고, 동일한 도구가 데이터를 다시 사용하여 데이터에서 추가 답변을 얻을 수도 있습니다.

넷째, 비즈니스 사용자가 데이터를 활용할 수 있도록 데이터가 시각화됩니다. 이 시점에서 데이터가 사용자에게 제공되는 것입니다. 그러면 사용자가 데이터에서 인사이트를 확보하여 향후 비즈니스 결정을 내리는 데 사용할 수 있습니다. 사용하려는 데이터 유형에 적합한 수집 솔루션을 사용하는 것이 중요합니다.

이 시험에서는 성능, 가용성, 확장성, 복원력, 내결함성을 위한 데이터 파이프라인을 구축하는 방법을 알아야 합니다. 이에 대해서는 이미 몇 번 이야기했지만 몇 가지 모범 사례를 빠르게 살펴보겠습니다.

 

Spark, Amazon EMR 또는 AWS Glue와 같은 분산 처리 프레임워크를 사용하여 대량의 데이터를 처리하고 리소스 클러스터 전체에 걸쳐 병렬 처리를 실행합니다.

 

최적의 성능 및 비용 효율성을 보장하기 위해 데이터 파이프라인에서 자동 크기 조정을 구성합니다. 여러 처리 리소스에 데이터를 분산하기 위해 적절한 데이터 파티셔닝 기술을 구현합니다.

 

데이터를 저장하는 데 Amazon S3 또는 Amazon Elastic File System(Amazon EFS)과 같은 내결함성 스토리지 서비스를 사용합니다. 데이터 가용성을 보장하고 재해 복구에 도움이 되도록 백업, 전략 및 계획을 구현하여 Amazon S3or 또는 AWS Backup으로 데이터를 보호합니다.

 

CloudWatch를 사용하여 데이터 파이프라인의 상태 및 성능을 모니터링합니다. 문제 또는 성능 문제를 적극적으로 탐지하고 대응할 수 있도록 경보 및 알림을 설정합니다.

 

Step Functions 또는 AWS Glue 워크플로를 사용하여 데이터 파이프라인 내에서 오류 처리 및 재시도 메커니즘을 구현합니다. 데이터 검증 및 품질 검사를 구현하고 자동화된 테스트, 버전 제어 및 배포 프로세스를 포함합니다.



문제 해결, 유지 관리, 확장성을 위해 데이터 파이프라인의 여러 단계 또는 구성 요소를 단위로 격리합니다. Lambda, AWS Glue 또는 컨테이너화 기술을 사용하여 개별 데이터 처리 구성 요소를 캡슐화하고 격리할 수 있습니다.



또한 변경 사항을 효율적으로 배포할 수 있도록 데이터 파이프라인에 대해 CI/CD 방식을 구현합니다. 



다음에는 네 번째 작업 설명인 프로그래밍 개념 적용을 시작하겠습니다.

## 4. 프로그래밍 개념 적용

영역 1의 네 번째 작업 설명인 프로그래밍 개념 적용을 시작하겠습니다. 이번 강의에서는 데이터 파이프라인 그리고 프로그래밍 개념을 사용하여 데이터를 유용하게 만드는 방법에 중점을 둘 것입니다.

 

이전 강의에서는 데이터 소스, 수집, 변환, 데이터 처리, 데이터 파이프라인 오케스트레이션에 대해 살펴보았습니다. 데이터를 수집, 변환하고 변환된 데이터를 데이터 마트 또는 데이터 레이크에 로드하는 단계, 즉 ETL 프로세스에 대해서도 논의했습니다. 이러한 ETL 프로세스의 개별 단계를 결합하여 데이터 처리 방식을 운영화하고 자동화할 수 있습니다.

 

AWS에서는 데이터 변환 엔진을 제공하며 데이터를 변환하는 다양한 방법이 존재합니다. 데이터 엔지니어는 원시 데이터세트를 사용하고, 원시 데이터세트를 다른 데이터세트와 결합하고, 다양한 분석 엔진을 통해 데이터를 처리하고, 새로운 또는 다른 데이터세트를 생성합니다. 그러나 이 모든 것은 비즈니스 목적, 요구 사항, 생성하려는 정보 등에 따라 달라집니다.

 

이제부터 정형 쿼리 언어(SQL)를 시작하겠습니다. 먼저 SQL의 장점입니다. SQL을 사용하는 최대 장점은 SQL이 다년간 사용되었고 잘 알려진 표준 언어라는 것입니다. 그러나 Spark와 같이 데이터 변환에 대한 코드 기반 접근 방식은 데이터 변환을 수행하는 데 더 강력하고 활용도가 높을 수 있습니다.

 

데이터 엔지니어는 데이터의 도구와 대상을 이해해야 합니다. SQL에 크게 의존하는 환경이라면 변환에 SQL을 사용하는 것이 더 나은 선택일 것입니다. 하지만 환경에 지연 시간 및 처리량 요구 사항이 포함된 복잡한 데이터 처리 요구 사항이 있다면 Spark가 더 나은 선택일 수 있습니다.

 

AWS에서 Spark 작업을 실행하는 방법은 여러 가지가 있습니다. Spark Streaming을 사용하면 데이터를 배치 방식으로 또는 거의 실시간으로 처리할 수 있습니다. Spark SQL을 사용하면 표준 SQL과 데이터에 기계 학습 기술을 적용하기 위한 Spark ML을 사용하여 데이터를 처리할 수 있습니다. 또한 서버리스 방식으로 Spark를 실행하는 방법을 추가하는 AWS Glue를 사용하여 Spark 작업을 실행할 수도 있고, Amazon EMR을 통해 Spark 실행용 클러스터를 배포하는 데 관리형 서비스를 사용할 수도 있습니다. Amazon Elastic Container Service (Amazon ECS) 및 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 컨테이너화된 환경에서 Spark 엔진을 실행하거나 DataBricks와 같은 AWS 파트너의 관리형 서비스를 사용할 수도 있습니다.

 

여기서 질문이 있습니다. 데이터 변환 대상이 Amazon Redshift에 있는 경우 SQL 또는 Spark 중 무엇을 사용하여 데이터를 처리합니까? 데이터 변환 대상이 Amazon Redshift에 있는 경우 ELT 접근 방식을 사용하고 데이터 변환이 데이터 웨어하우스, 즉 Amazon Redshift 내부에서 수행되는 것입니다. 그리고 Amazon Redshift는 SQL 기반 쿼리를 사용하여 시스템 내 데이터 및 객체와 상호 작용합니다. 데이터 웨어하우스 대신 데이터 처리 엔진을 사용하여 변환을 수행하는 동안 Spark SQL을 사용하여 변환에 SQL을 사용할 수도 있습니다. 이 구성에서는 데이터 변환이 Spark 클러스터로 오프로드되는 동안 데이터 웨어하우스가 최종 사용자 쿼리에 집중할 수 있습니다. 따라서 데이터가 중간 스토리지로 추출되는 ETL 접근 방식처럼 흐릅니다.

 

Spark를 사용하여 데이터가 변환된 다음 데이터 웨어하우스에 로드됩니다. 여기서 질문이 있습니다. 쿼리가 최적화되고 최대한 신속하게 처리되도록 해야 하는 이유는 무엇입니까? 대량의 데이터에 대해 복잡한 쿼리를 실행하면 처리하는 데 시간이 걸릴 수 있습니다. 예를 들어 Amazon Redshift가 쿼리를 최적화하는 방식을 살펴보겠습니다. Amazon Redshift는 제출된 SQL 쿼리를 파서 및 옵티마이저를 통해 라우팅하여 쿼리 계획을 개발합니다. 그런 다음 실행 엔진이 쿼리 계획을 코드로 변환하고 실행을 위해 해당 코드를 컴퓨팅 노드로 보냅니다.

계속해서 다른 엔진을 살펴보겠습니다. 대규모 데이터세트 작업을 위한 여러 오픈 소스 소프트웨어 패키지로 구성된 프레임워크가 있고 실행을 단일 서버에서 수천 개의 노드까지 확장할 수 있어야 하는 경우 AWS에서 어떤 엔진을 사용할 수 있습니까? Amazon EMR을 사용하면 완전히 사용자 지정 가능한 클러스터에서 Hadoop, Spark, HBase, Presto와 같은 프레임워크를 사용하여 대규모 데이터 세트를 처리 및 분석할 수 있습니다. Yarn, Tez, Pig 등 다양한 Hadoop 도구를 실행할 수도 있습니다.

 

ETL 프로세스에 대해 이야기했던 이 강의의 시작 부분으로 돌아가 보겠습니다. AWS Data Pipeline은 다양한 AWS 서비스 및 온프레미스 리소스 간에 데이터 이동 및 변환을 정의하는 관리형 ETL 서비스입니다. 데이터 파이프라인을 사용하여 데이터가 저장되어 있는 데이터 노드로 구성된 파이프라인을 생성하는 종속 프로세스를 정의합니다. 여기에는 순차적으로 실행되는 Amazon EMR 작업, SQL 쿼리와 같은 활동 또는 비즈니스 로직이 포함됩니다.

 

예를 들어 Amazon S3에 저장된 클릭스트림 데이터를 Amazon Redshift로 이동하려는 경우 로그 파일을 저장하는 S3 데이터 노드, Amazon EMR 클러스터를 사용하여 로그 파일을 .CSV 파일로 변환하고 Amazon S3에 다시 저장하는 Hive 작업, Amazon S3에서 Amazon Redshift로 데이터를 복사하는 Redshift 복사 작업, Amazon Redshift 클러스터에 연결할 Redshift 데이터 노드를 사용하여 파이프라인을 정의합니다. 그런 다음 일과 종료 후 실행할 일정을 선택할 수 있습니다.

 

Spark 코딩 기술이 없는 사용자가 Spark 엔진을 사용하여 SQL 기반 변환을 실행할 수 있도록 SQL 문을 사용하여 복잡한 변환을 수행하는 ETL 작업을 설계하는 데 도움이 될 수 있는 AWS Glue Studio와 같은 시각적 인터페이스를 제공하는 AWS 서비스도 있습니다.

 

일련의 파일에 변환을 적용하는 데 유용한 AWS Glue DataBrew도 있습니다. 이제부터는 데이터 변환 및 처리에 프로그래밍 개념을 적용하는 방법에 대해 이야기해 보겠습니다. Athena는 Amazon S3에 저장된 정형 및 반정형 데이터를 쿼리하는 데 도움이 됩니다.

 

Athena를 사용하여 데이터 카탈로그의 테이블을 쿼리한다고 가정해 보겠습니다. Athena에서 지원하는 열 이름을 알고 계십니까? 예를 들어 Athena는 슬래시, 공백, 마침표와 같은 특수 문자를 지원합니까? 아니요. Athena는 영숫자 문자 및 밑줄이 포함된 열 이름을 지원합니다. 예를 들어 열 이름 /var/log는 var_log 또는 varlog로 이름이 변경될 수 있습니다.

 

파이프라인에서 코드형 인프라는 오케스트레이션 시스템의 핵심 개념이며 데이터 엔지니어링 수명 주기의 모든 단계에 적용됩니다. 첫 번째 강의에서 이야기했던 데이터 수집 및 변환 런타임을 축소하기 위한 코드 최적화로 돌아가 보겠습니다.

 

코드를 최적화하고 데이터 수집 및 변환 런타임을 줄이기 위해 AWS에서 사용할 수 있는 전략은 무엇입니까?

병렬 처리를 사용하여 워크로드를 여러 컴퓨팅 리소스에 분산할 수 있습니다. 예를 들어, AWS Glue는 Spark를 사용하여 데이터 변환 작업 병렬 실행을 지원합니다. 데이터를 파티셔닝하고 병렬로 처리하면 전체 런타임을 크게 줄일 수 있습니다.

 

배치 처리를 사용하면 오버헤드를 줄이고 효율성을 높일 수 있습니다. AWS Glue와 Amazon EMR은 대량의 데이터를 효율적으로 처리할 수 있는 배치 처리 기능을 제공합니다.

 

저장 및 처리에 최적화된 데이터 형식을 선택합니다. Parquet 또는 ORC와 같은 열 기반 형식은 압축 성능이 더 우수합니다. 열 단위 데이터 액세스는 더 빠른 데이터 수집 및 변환에 도움이 될 수 있습니다.

 

특히, 대규모 데이터세트의 경우 데이터를 처리하기 전에 최적화된 형식으로 변환할 수도 있습니다. 전체 데이터세트 대신 필요한 데이터만 로드 및 처리하고, 필터링 기술을 사용하여 프로세스 초기에 불필요한 데이터를 제거해야 합니다. 이렇게 하면 처리되는 데이터의 양이 줄어들고 전체 런타임이 향상됩니다.

 

날짜, 리전 등 특정 기준에 따라 데이터를 분할하여 스캔되는 데이터의 양을 줄일 수 있습니다. 파티셔닝은 수집 및 변환 중에 스캔되는 데이터의 양을 줄이는 데 도움이 될 수 있습니다.

데이터를 압축하여 스토리지 요구 사항을 낮추고 데이터 전송 속도를 향상시킬 수 있습니다. AWS Glue는 데이터를 압축 형식으로 변환하고 기록하기 위한 압축 옵션을 기본 제공합니다. 데이터 특성과 사용 중인 처리 도구에 따라 적절한 압축 알고리즘을 선택할 수 있습니다.

 

데이터 처리 태스크를 위한 메모리 및 리소스 할당을 최적화할 수 있습니다. 요구 사항에 맞게 메모리 구성 및 병렬 처리 설정을 조정하는 방법을 알고 있어야 합니다. 그러면 최적화된 사용으로 충분한 리소스를 확보하는 데 도움이 되므로 성능 병목 현상을 방지하고 전체 런타임을 개선할 수 있습니다.

 

인덱싱, 쿼리 재작성, 적절한 SQL 쿼리 설계 등 쿼리 최적화에 대해서는 이미 다루었습니다. 마지막 처리 실행 이후의 변경 사항 또는 델타 데이터만 처리하기 위해 증분 처리를 사용하는 방법도 다루었습니다.

 

데이터 수집 및 변환 프로세스의 성능을 지속적으로 모니터링하고 분석하려면 CloudWatch를 사용하여 병목 현상을 식별하고, 리소스 할당을 최적화하고, 성능 향상을 위해 코드를 개선하십시오.

인프라 배포의 모든 측면에 자동화를 사용하는 코드 최적화로 돌아가 보겠습니다. IaC의 경우 CloudFormation 템플릿을 사용하여 배포할 인프라의 정의 및 구성을 지정할 수 있습니다. AWS CDK를 반복 가능한 배포에도 사용할 수 있습니다. 데이터 파이프라인의 경우 CI/CD를 사용하여 데이터 파이프라인을 구현, 테스트, 배포할 수 있습니다. Lambda 함수, Step Function, DynamoDB 테이블과 같은 서버리스 데이터 파이프라인을 패키징하고 배포하기 위한 AWS Serverless Application Model(AWS SAM)도 있습니다.

버전 제어의 경우 템플릿 정의 파일이 있으면 이 파일을 AWS CodeCommit 또는 GitHub와 같은 소스 코드 리포지토리에 커밋할 수 있습니다. 그러면 새 버전의 파일이 소스 제어 리포지토리에 커밋될 때 실행되도록 CI/CD 파이프라인을 자동화할 수 있습니다.

 

목표는 코드를 최적화하여 데이터 수집 및 변환 런타임을 축소하는 것입니다. 서버리스 애플리케이션은 병렬화 및 동시성을 통해 성능을 개선할 수 있습니다. 예를 들어 Lambda가 자동으로 크기 조정을 관리해주지만, 사용자는 애플리케이션에 사용되는 개별 Lambda 함수를 최적화하여 지연 시간을 줄이고 처리량을 늘릴 수 있습니다.

 

동시성 및 성능 요구 사항을 충족하도록 Lambda 함수를 구성하려면 어떻게 해야 합니까? 먼저, 코드가 최적화되었는지 확인합니다. 모범 사례와 AWS SDK 및 라이브러리를 사용하고 있습니까? 다음으로, 함수에 적절한 양의 메모리 및 CPU가 할당되었는지 확인합니다. 또한 예상되는 워크로드 및 성능 요구 사항에 따라 동시성 제한을 구성합니다. 즉각적인 응답이 필요하지 않은 비동기식 호출을 사용할 수 있습니다. 프로비저닝된 동시성을 사용하여 함수를 사전 워밍하고, 함수에 적절한 시간 제한 값을 설정하여 대기 및 조기 종료를 방지할 수 있습니다. 예상 실행 시간을 기준으로 시간 제한을 조정하고, 일시적인 오류를 처리하도록 재시도 동작을 구성할 수 있습니다. 또 다른 옵션은 더 높은 입력/출력 성능을 위해 Amazon EBS 볼륨에 프로비저닝된 IOPS를 사용하는 것입니다.

 

Lambda 함수가 Amazon VPC 내부의 리소스에 액세스해야 하는 경우 이로 인해 추가될 수 있는 지연 시간을 고려해야 합니다. 지연 시간을 최소화하기위해 함수를 동일한 Amazon VPC에 배치하고 데이터베이스 및 캐시를 사용할 수 있습니다. 그리고 함수의 성능을 모니터링하여 구성 및 코드를 최적화합니다.

 

다음은 코드 최적화에 대한 질문입니다. 파이프라인이 Kinesis Data Streams를 사용하여 거의 실시간으로 사용자 활동을 모니터링하고 수집하도록 설계되었다고 가정해 보겠습니다. Amazon EC2 인스턴스에서 실행되는 또 하나의 애플리케이션은 Kinesis Data Streams API를 AWS SDK for Java와 함께 사용하여 데이터를 사용 및 처리한 후 분석을 위해 Amazon OpenSearch 클러스터로 보냅니다. 데이터 분석가가 Amazon OpenSearch 클러스터에서 데이터 손실을 유발하는 오류를 발견했습니다. Java 코드를 수정하여 누락 데이터를 복구할 수 있습니까?

 

예. 멱등성 처리를 사용하고 샤드 반복기가 애플리케이션 오류 발생 전의 샤드 위치를 가리키도록 Java 애플리케이션 코드를 수정할 수 있습니다. 더 자세한 내용은 추가한 링크를 참조하십시오.

 

이 시험에서는 워크플로의 반복 속도를 높이고, 품질을 향상하고, 효율을 증대하기 위해 프로그래밍 개념을 적용하고 CI/CD용 AWS 서비스 및 도구를 사용하여 데이터 파이프라인의 배포, 테스트 및 모니터링을 자동화하는 방법을 이해할 수 있어야 합니다. 앞서 CloudFormation과 AWS SAM을 사용하여 Lambda 애플리케이션 생성을 자동화하고 Lambda 함수 내에 스토리지 볼륨을 탑재하는 방법을 설명했습니다. 자동화된 테스트는 데이터 변환의 정확성 및 무결성을 보장하는 데 도움이 됩니다.

 

자동화할 수 있는 다른 테스트에는 무엇이 있을까요? 데이터 품질 검사가 있고 스키마 검증도 있습니다. 여기서 잠시 이 시험에 필요한 기본 사항으로 돌아가겠습니다. 데이터 엔지니어는 효율적이고 확장 가능한 데이터 처리 시스템을 설계하고 구현하기 위해 다양한 데이터 구조 및 알고리즘을 확실히 이해해야 합니다. 올바른 데이터 구조 및 알고리즘을 사용하면 성능을 최적화하고 확장성을 개선하며 처리 시간을 단축할 수 있습니다.

데이터 구조는 데이터 관계를 표현하며 데이터가 컴퓨터 메모리에서 구성 및 저장되는 방식을 나타냅니다. 또한 데이터를 효과적으로 정렬하고 액세스하는 방법을 제공합니다. 데이터 구조에는 관계형 데이터베이스, NoSQL 데이터베이스, 데이터 웨어하우스, 분산 파일 시스템, 메시지 대기열, 트리 데이터, 해시 테이블이 있습니다.

 

다양한 데이터 구조를 이해하고 특정 사용 사례에 적합한 데이터 구조를 선택할 수 있어야 합니다. 알고리즘은 컴퓨터 소프트웨어, 웹 페이지, 프로그램 및 하드웨어가 태스크를 완료하기 위해 사용하는 일련의 명령 또는 규칙입니다. 알고리즘의 예로는 정렬, 검색, 최단 경로, 해싱, 그래프 등이 있습니다. 각 알고리즘은 데이터에 액세스하는 방법뿐만 아니라 태스크를 완료하기 위해 명령에 응답하는 방법도 제공합니다. 하지만 알고리즘이 특정 데이터 구조에 맞게 설계된 경우 다른 데이터 구조에 사용하면 결과가 비효율적일 수 있습니다.

 

예를 들어 데이터 엔지니어가 대규모 데이터세트를 정렬하기 위해 정렬 알고리즘을 구현해야 할 경우 데이터 크기 및 유형에 따라 퀵 정렬 또는 병합 정렬과 같은 알고리즘을 선택할 수 있습니다. 

분석가가 추가 결정을 수립할 수 있도록 대량의 고객 데이터를 처리하고, 데이터를 분석하고, 인사이트를 얻어야 한다고 가정해 보겠습니다. 이 작업을 수행하려면 대량의 데이터를 처리하고 최대한 신속하게 효율적으로 검색할 수 있는 데이터 구조에 데이터를 저장해야 합니다. 여러분은 Amazon EMR을 Hadoop 분산 파일 시스템과 함께 사용하여 데이터를 저장하기로 결정합니다. 그런 다음 데이터를 처리하여 인사이트를 추출해야 합니다. 이를 위해서는 데이터를 효율적으로 처리할 수 있는 적절한 알고리즘을 선택해야 합니다. 여러분은 클러스터에서 병렬 분산 알고리즘을 사용하여 대규모 데이터세트를 처리하기 위한 프로그래밍 모델인 MapReduce 알고리즘을 사용하기로 결정합니다. 그런 다음 데이터 처리 파이프라인에서 MapReduce 알고리즘을 구현합니다. 여기에는 데이터를 더 작은 청크로 분할하고, 각 청크를 일련의 키-값 페어에 매핑하고, 결과를 축소하여 최종 결과를 얻는 작업이 포함됩니다. 마지막으로 데이터 파티셔닝, 캐싱, 병렬 처리와 같은 기술을 사용하여 데이터 처리 파이프라인의 성능을 최적화합니다.

 

데이터 구조 및 알고리즘을 이해함으로써 고객 데이터에서 귀중한 인사이트를 추출할 수 있는 효율적이고 확장 가능한 데이터 처리 시스템을 설계하고 구현할 수 있었습니다. 데이터 구조 및 알고리즘은 데이터를 효율적으로 처리하고 데이터 처리 파이프라인을 최적화함으로써 성능을 향상시키고 처리 시간을 단축하는 데 도움이 됩니다. 데이터를 더 빠르게 처리할수록 데이터에서 더 많은 가치를 창출할 수 있습니다.

 

올바른 데이터 구조 및 알고리즘을 선택하면 필요에 따라 확장할 수 있는 파이프라인 및 시스템을 설계할 수 있습니다. 또한 데이터 구조 및 알고리즘을 이해함으로써 성능 문제를 식별 및 수정하고 코드 효율성을 최적화할 수 있습니다.

 

이번 강의를 마무리하면서 데이터 엔지니어가 AWS에서 프로그래밍 개념을 적용하는 방법에 대한 예를 몇 가지 살펴보겠습니다. 데이터 수집의 경우 데이터 엔지니어는 Lambda를 사용하여 데이터 수집 태스크를 처리하는 코드를 작성할 수 있습니다. 예를 들어 S3 버킷과 같은 이벤트 소스를 수신하는 Lambda 함수를 생성할 수 있습니다. 이 함수는 새 데이터가 버킷에 업로드될 때 시작되며 데이터를 구분 분석, 변환, 데이터 웨어하우스 또는 데이터베이스로 로드하는 등의 작업을 수행합니다.

 

데이터 처리의 경우 Amazon EMR 및 AWS Glue를 사용하여 Python 또는 Scala와 같은 프로그래밍 언어로 데이터 처리 작업을 정의하는 코드를 작성할 수 있습니다. Spark와 같은 프레임워크를 사용하여 EC2 인스턴스 클러스터 전체에 데이터 처리 태스크를 분산하고 병렬화할 수 있습니다.

 

실시간 데이터 처리의 경우 Kinesis에서 코드를 작성하여 스트리밍 데이터를 사용하고 처리할 수 있습니다. 또한 AWS SDK 및 Amazon Kinesis 클라이언트 라이브러리를 사용하여 데이터 스트림과 상호 작용할 수 있습니다.

 

데이터 변환의 경우 복잡한 변환을 수행하려면 AWS Glue를 사용하고 AWS Glue Python 기반 ETL 프레임워크에서 코드를 작성하여 데이터 정리, 집계, 정규화, 스키마 진화와 같은 변환 논리를 정의할 수 있습니다.

 

데이터 저장의 경우 정형 데이터에 Amazon RDS 또는 Amazon Redshift를 사용하고 SQL 쿼리를 작성하여 데이터와 상호 작용할 수 있습니다.

 

반정형 또는 비정형 데이터의 경우 Amazon S3 및 AWS SDK를 사용하여 Amazon S3에 저장된 데이터를 읽고, 쓰고, 조작할 수 있습니다. 워크플로 오케스트레이션의 경우 Step Functions를 사용하여 상태 머신을 정의함으로써 서버리스 워크플로를 구축할 수 있습니다. 그런 다음 프로그래밍 개념을 사용하여 워크플로의 구조, 조건 및 작업을 정의합니다.

 

다음에는 첫 번째 연습 문제를 시작하겠습니다.

# 영역 2. 데이터 스토어 관리

## 1. 데이터 스토어 선택

영역 2의 첫 번째 작업 설명인 데이터 스토어 선택을 시작하겠습니다. 이 첫 번째 작업 설명에서는 AWS에서 제공하는 다양한 스토리지 플랫폼 및 옵션을 이해해야 합니다. 매일 스토리지를 사용하는데이터 엔지니어로서 다양한 스토리지 시스템의 기본 구성 요소, 특성, 성능 고려 사항, 내구성, 비용을 이해하는 것이 중요합니다. 데이터의 사용 사례, 데이터가 저장되는 방식, 데이터가 검색되는 방식을 아는 것이 데이터 아키텍처에 적합한 스토리지 솔루션을 선택하기 위한 첫 번째 단계입니다.

 

스토리지 시스템은 원시 인프라보다 추상화 수준이 높습니다. 대부분의 데이터 아키텍처에서 데이터 스토리지 및 쿼리 시스템에는 분산 시스템, 서비스 및 하드웨어 스토리지 계층이 포함되어 있습니다. 이러한 시스템이 올바르게 작동하려면 원시 인프라가 필요합니다.

 

다양한 AWS 스토리지 서비스에 대해 이미 살펴보았습니다. Amazon S3는 객체 스토리지, Amazon EFS는 파일 스토리지, Amazon EBS는 블록 스토리지입니다. 이들 스토리지 플랫폼은 각각 다양한 사용 사례의 요구 사항을 충족하는 고유한 특성이 있습니다.

 

각 플랫폼의 주요 특성을 빠르게 알아보겠습니다.

 

Amazon S3는 용량과 관계없이 데이터를 저장 및 검색할 수 있도록 설계된 가용성, 확장성 및 내구성이 뛰어난 객체 스토리지 서비스입니다. Amazon S3의 몇 가지 사용 사례에는 정적 콘텐츠, 백업, 미디어 파일 저장, 데이터 레이크 저장 등이 있습니다. Amazon S3는 데이터 레이크에 사용되는 기본 스토리지이기도 합니다.

 

Amazon EFS는 여러 EC2 인스턴스에서 공유할 수 있는 확장 가능한 완전관리형 파일 스토리지 서비스를 제공합니다. Amazon EFS의 몇 가지 사용 사례에는 여러 인스턴스에 걸친 공유 파일 스토리지, 웹 호스팅, 콘텐츠 관리 시스템, 빅 데이터 처리 등이 있습니다. 또 다른 파일 스토리지로는 Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for NetApp ONTAP 등이 있습니다.

 

Amazon EBS는 Amazon EC2 인스턴스에 연결된 영구 블록 수준 스토리지 볼륨을 제공합니다. Amazon EBS의 몇 가지 사용 사례에는 짧은 지연 시간, 고성능 스토리지가 필요한 데이터베이스, 트랜잭션 애플리케이션 및 기타 애플리케이션 실행 등이 있습니다.

 

Amazon RDS는 MySQL, PostgreSQL, Oracle, SQL Server 등을 위한 관리형 데이터베이스 서비스입니다. Amazon RDS의 사용 사례는 관계형 데이터베이스 호스팅 및 관리입니다.

 

DynamoDB는 규모와 관계없이 10밀리초 미만의 지연 시간을 제공하는 관리형 NoSQL 데이터베이스 서비스입니다. DynamoDB의 몇 가지 사용 사례에는 지연 시간이 짧은 읽기 및 쓰기 작업, 실시간 데이터, 게임, IoT 및 동적 크기 조정이 필요한 애플리케이션이 있습니다. 또한 DynamoDB는 문서, 그래프, 키-값, 메모리, 검색과 같은 여러 데이터 유형을 사용할 수 있습니다.

 

Amazon Redshift는 분석 및 비즈니스 인텔리전스 워크로드를 위한 페타바이트 규모의 관리형 데이터 웨어하우스 서비스입니다. Amazon Redshift는 정형 데이터를 저장 및 관리하는 데 적합하지만, 대용량의 고속 반정형 및 비정형 데이터를 처리하는 데에도 훌륭한 솔루션입니다.

 

데이터 레이크를 구축, 보호, 관리하기 위한 관리형 서비스인 AWS Lake Formation도 있습니다. Lake Formation은 정형 데이터와 비정형 데이터를 중앙 리포지토리에 결합하므로 데이터를 크롤링하고, 분류하고, 분석을 위해 준비할 수 있습니다.

 

적합한 AWS 스토리지 서비스 및 구성을 선택하는 것은 요구 사항, 데이터 유형, 애플리케이션 및 워크로드의 성능 요구 사항에 따라 달라집니다. 데이터 스토어를 성능 요구 사항에 따라 선택할 때 고려해야 할 요소는 무엇입니까? 적절한 스토리지 솔루션을 선택할 때 초당 입출력 작업 요구 사항, 확장성, 내구성 그리고 데이터 액세스 패턴도 고려해야 합니다.

 

두 가지 질문이 있습니다. 병렬성이 높은 파일 시스템을 위해 어떤 AWS 서비스를 선택하시겠습니까? Amazon FSx for Lustre입니다. 병렬 파일 시스템의 사용 사례는 무엇입니까? 대규모 병렬 데이터 처리 요구 사항이 있는 고성능 컴퓨팅, 데이터 분석 및 기계 학습 워크로드입니다.

 

인 메모리 캐싱과 저 지연 시간 액세스를 위해 어떤 AWS 서비스를 선택하시겠습니까? ElastiCache입니다. ElastiCache의 사용 사례는 자주 액세스하는 데이터, 세션 스토리지 및 실시간 분석을 캐싱하는 것입니다.

 

다른 질문도 살펴보겠습니다. 다음과 같은 요구 사항에서 Amazon Redshift를 선택하는 이유를 설명할 수 있습니까? 새 애플리케이션에서 30테라바이트의 비압축 데이터를 저장해야 합니다. 매일 수백에서 수천 개의 집계된 쿼리가 발생합니다. 복잡한 조인도 여러 개 있지만 테이블 열의 작은 하위 집합만 데이터 쿼리에 사용됩니다.

 

Amazon Redshift는 대상 데이터 압축 및 인코딩 체계를 통한 병렬 처리 및 열 기반 데이터 스토리지를 사용하여 최적의 쿼리 성능으로 데이터를 저장할 수 있습니다. Amazon Redshift를 선택하는 이유는 열 기반 아키텍처가 테이블 열의 하위 집합을 쿼리할 때 입력 및 출력이 줄어드는 이점이 있기 때문입니다. 그리고 데이터가 열 단위로 저장되기 때문에 압축률이 매우 높아 입력 및 출력이 더욱 줄어들며 더 많은 데이터를 저장하고 빠르게 쿼리할 수 있습니다. Amazon Redshift는 SQL 조인도 지원합니다.

 

다음은 데이터 마이그레이션 또는 원격 액세스 방법 구현에 대한 질문입니다. Amazon Redshift 클러스터를 보유하고 있고 새 애플리케이션의 데이터를 클러스터 테이블로 마이그레이션할 계획입니다. 이 신규 데이터를 마이그레이션하려면 어떻게 해야 합니까? Amazon Redshift에서 COPY 명령은 데이터 파일에서 테이블로 데이터를 로드합니다. 파일은 S3 버킷 또는 Amazon EMR 클러스터 등에 위치할 수 있습니다. COPY 명령은 테이블을 로드할 때 소스 데이터의 문자열을 대상 열의 데이터 형식으로 변환하려고 시도합니다.

 

기본 동작과 다른 변환을 지정해야 하거나 기본 변환에서 오류가 발생한다면 어떻게 합니까? 지원되는 파라미터를 설정하여 데이터 변환을 관리할 수 있습니다.

 

다음은 특정 성능 요구에 맞는 스토리지 서비스 및 구성에 대한 질문입니다. Amazon RDS에서 호스팅되는 애플리케이션이 있습니다. 크기가 수 테라바이트에 달하는 최근 3개월 데이터에 대해 분석 워크로드 및 쿼리가 실행됩니다. 오래된 데이터는 데이터베이스 외부에 저장해야 하지만 이 과거 데이터가 보고를 위해 최신 데이터와 결합되는 경우가 있습니다. 성능을 최적화하기 위한 여러분의 솔루션은 무엇입니까?

 

저라면 오래된 데이터를 Amazon S3로 내보내겠습니다. 그런 다음 현재 데이터를 Amazon RDS에서 Amazon Redshift로 내보내는 일일 작업을 생성합니다. 그런 다음 일반 쿼리에는 Amazon Redshift를 사용하고 보고를 위해 과거 데이터와 현재 데이터를 결합하는 데는 Redshift Spectrum을 사용할 수 있습니다. 또한 데이터 스토리지 및 액세스 패턴을 이해해야 합니다.

페타바이트 규모의 데이터를 키/값 페어로 저장해야 한다고 가정해 보겠습니다. 수집된 데이터는 처리 및 분석되며 해당 데이터에 즉시 액세스해야 합니다. 또한 SQL 또는 SQLite 인터페이스를 사용하여 데이터를 읽고, 쓰고, 관리하는 기능도 필요합니다. 여러분의 솔루션은 무엇입니까?

 

저의 솔루션은 Amazon EMR에서 Hive를 사용하여 데이터를 분석하고 해당 데이터를 DynamoDB 테이블에 저장하는 것입니다. Hive를 사용하면 SQL 유사 인터페이스를 사용하여 페타바이트 규모의 데이터를 읽고 쓰고 관리할 수 있습니다. 이전 강의에서 Amazon EMR은 Spark, Hive, Apache HBase, Apache Flink, Apache Hudi, Presto와 같은 오픈 소스 도구를 사용하여 대량의 데이터를 처리할 수 있다고 했습니다.

 

단기 실행 작업의 경우 클러스터를 가동/중단하면서 사용한 인스턴스에 대해 초당 비용을 지불할 수 있습니다. 장기 실행 워크로드의 경우 수요에 따라 자동으로 확장되는 고가용성 클러스터를 생성할 수 있습니다.

 

다른 질문도 살펴보겠습니다. 데이터에 대한 액세스를 방지하기 위해 권한을 관리하려면 어떻게 해야 합니까? 데이터 보안, 무결성 및 규정 준수를 보장하려면 권한을 관리하여 데이터에 대한 무단 액세스를 방지하는 것이 중요합니다. Amazon Redshift와 Amazon RDS는 모두 사용자 권한 및 보안 그룹을 통해 액세스를 제어하는 메커니즘을 제공합니다.

 

두 서비스 모두에서 데이터에 대한 액세스를 방지하는 데 사용할 수 있는 몇 가지 옵션을 살펴보겠습니다. Amazon Redshift와 Amazon RDS는 모두 IAM, 보안 그룹 및 AWS Key Management Service(AWS KMS)를 통해 데이터 보안을 관리하고 액세스를 제어하는 기능을 제공합니다.

 

Amazon Redshift 및 Amazon RDS에서 IAM은 클러스터 및 리소스에 대한 액세스를 제어하는 데 도움이 됩니다. 특정 권한 및 역할을 보유한 IAM 사용자를 생성하고 역할 및 책임에 따라 적절한 IAM 정책을 할당할 수 있습니다.

 

또한 데이터 보호 계층을 추가하기 위해 보안 그룹을 사용하고, 암호화를 추가하고, AWS KMS를 사용하여 암호화 키를 관리할 수도 있습니다. 또한 파라미터 그룹을 사용하면 잠금 메커니즘 등의 데이터베이스 파라미터를 구성하여 동시성을 제어하고 잠재적인 데이터 충돌을 방지할 수 있습니다. 마지막으로 데이터베이스 감사를 사용하여 데이터베이스에 대한 액세스 및 변경 사항을 추적 및 로깅하고 무단 액세스 시도를 모니터링합니다.

 

또한 의심스럽거나 승인되지 않은 액세스 시도를 탐지하고 해결하기 위해 액세스 로그 및 감사 추적을 확인하는 반복 모니터링 일정을 구성해야 합니다.

 

영역 2의 두 번째 작업 설명인 데이터 카탈로그 작성 시스템 이해를 시작하겠습니다.

## 2. 데이터 카탈로그 작성 시스템 이해

영역 2의 두 번째 작업 설명인 데이터 카탈로그 작성 시스템 이해를 시작하겠습니다. 이 시험에서는 데이터 카탈로그를 다루는 이 작업 설명에서 데이터를 계획, 설계, 구조화, 구성하는 방법을 이해해야 합니다. 많은 조직에서 데이터는 여러 개의 격리된 위치에 물리적으로 저장됩니다. 이러한 격리로 인해 사용자가 가장 의미 있는 정보를 제공하기 위해 다양한 소스의 데이터를 결합하는 것이 어려울 수 있습니다.

 

데이터 카탈로그는 데이터의 위치, 설명, 스키마 및 런타임 지표에 대한 정보를 저장하는 메타데이터 리포지토리입니다. 사용자는 이 중앙 집중식 정보를 사용하여 연합 쿼리 작성, ETL 작업 등을 용이하게 수행할 수 있습니다.

 

데이터 엔지니어는 데이터 카탈로그를 설정하는 방법과 데이터 카탈로그와 통합되는 데이터 파이프라인 및 스토리지 시스템의 다양한 데이터 통합을 유지 관리하는 방법을 이해해야 합니다.

 

영역 4에서 데이터 보안 및 거버넌스에 대해 논의하겠지만 여기서 데이터 카탈로그와 함께 설명하려고 합니다. 파이프라인이 빠른 데이터 변환 덕분에 효율적이고 최고의 데이터 사용 도구를 사용하더라도 데이터는 보호되지 않으면 가치가 없습니다. 그리고 데이터를 보호하고 거버넌스 규정을 준수하더라도 분석 데이터세트를 쉽게 검색하고 사용할 수 있어야 합니다.

 

각 데이터세트에 대해 필요한 세부 정보를 캡처하려면 요구 사항에 따라 데이터를 분류하는 조직 정책을 적용할 수 있습니다. 데이터 웨어하우스나 데이터 레이크를 생성하려면 이 데이터의 카탈로그를 작성해야 합니다. 데이터 카탈로그에는 AWS Glue에서의 ETL 작업의 소스 및 대상으로 사용하는 데이터를 설명하고 참조하는 메타데이터가 포함되어 있습니다. 이는 데이터의 위치, 스키마 및 런타임 지표에 대한 인덱스입니다. 데이터 카탈로그의 정보를 사용하여 ETL 작업을 생성하고 모니터링할 수 있습니다. 데이터 카탈로그의 정보는 메타데이터 테이블로 저장되며, 각 테이블은 단일 데이터 스토어를 지정합니다. 일반적으로 데이터 스토어의 데이터 인벤토리를 가져올 때는 크롤러를 실행하지만, 메타데이터 테이블을 데이터 카탈로그에 추가할 때는 다른 방법이 있습니다.

 

Athena를 사용하여 데이터 레이크의 데이터를 쿼리한다고 가정해 보겠습니다. 이것은 어떻게 작동할까요? Athena는 카탈로그 내의 데이터베이스 및 테이블 정의를 사용하여 쿼리를 실행할 수 있습니다. 데이터를 쿼리하기 위해 Athena는 카탈로그를 사용하여 데이터 파일의 S3 위치, Parquet 또는 CSV와 같은 파일 형식 유형을 제공하는 메타데이터, 사용되는 파티션에 대한 정보 등을 가져옵니다. Lake Formation도 데이터 카탈로그에 대한 인터페이스를 제공합니다. 데이터 카탈로그의 경우 Lake Formation은 열 수준에서 키/값 속성을 추가하는 기능을 추가로 제공합니다. 방금 AWS Glue를 사용하면 테이블 수준에서 키/값 태그를 사용하여 일부 비즈니스 속성을 캡처할 수 있다고 했습니다. Lake Formation은 데이터베이스, 테이블 및 열 수준에서 액세스 권한을 구성할 수 있는 추가 기능을 제공합니다. 몇 분 후에 권한에 대해 더 자세히 살펴보겠습니다.

 

여기서 질문이 있습니다. Amazon S3에 4개의 AWS 계정에 대한 데이터를 보관하는 중앙 데이터 레이크가 있습니다. 각 사업부가 관련 데이터에만 계속 액세스할 수 있도록 데이터 레이크에 대한 역할 기반 액세스 제어를 유지해야 합니다. 여러분의 솔루션은 무엇입니까?

 

저라면 4개 AWS 계정 각각에 데이터 레이크 스토리지를 구축하겠습니다. Lake Formation을 사용하여 여러 계정의 데이터를 중앙 데이터 레이크 계정으로 카탈로그화합니다. 그런 다음 Lake Formation 서비스 연결 역할을 사용하여 각 계정의 S3 버킷 정책을 업데이트하고 Lake Formation 권한을 사용하여 보다 구체적인 액세스 권한을 부여합니다. Lake Formation에서는 데이터 카탈로그 메타데이터 및 기본 데이터에 대한 교차 계정 액세스를 구성할 수 있습니다.

 

예를 들어 여러 AWS 계정을 사용하는 대기업에서는 이러한 계정 대부분은 단일 AWS 계정으로 관리되는 데이터 레이크에 액세스해야 할 수 있습니다. 데이터 카탈로그 및 기본 데이터를 다른 AWS 계정과 공유하면 사용자는 여러 계정에 걸쳐 테이블을 조인하고 쿼리할 수 있는 쿼리 및 작업을 실행할 수 있습니다. 사용자와 AWS Glue ETL 작업은 Lake Formation 테이블 수준 및 열 수준의 데이터 보호를 활용하면서 여러 계정의 테이블을 쿼리하고 조인할 수 있습니다.

데이터 카탈로그에 대해 더 알아보겠습니다. 그리고 어떻게 하면 모든 데이터를 수집하는 워크플로를 구축하고, 데이터가 카탈로그화되고, 적절한 메타데이터가 추가되도록 할 수 있는지 논의해 보겠습니다. 무엇을 사용하시겠습니까?

 

가장 먼저 떠오르는 생각은 새 데이터가 수집된 후 AWS Glue 크롤러를 실행하여 새 데이터가 데이터 카탈로그에 자동으로 추가되도록 하는 것입니다.

 

여기서 질문이 있습니다. 새로운 데이터 엔지니어링 작업이 프로덕션에 배포될 때 데이터 카탈로그가 새 데이터의 모든 필요한 세부 정보로 업데이트되도록 하려면 무엇을 추가해야 합니까? AWS Glue API가 데이터 카탈로그를 업데이트하는 데 사용되고 있는지 확인하는 검사를 추가할 수 있습니다.


이 시험에서는 파티션과 데이터 카탈로그 간 동기화가 Amazon S3와 같이 분할된 데이터 소스에 저장된 데이터를 관리 및 구성하는 데 얼마나 중요한지도 이해해야 합니다. 날짜, 리전 또는 범주와 같은 특정 기준에 따라 파티션으로 분할된 대규모 데이터세트의 경우 이러한 파티션을 정확하게 반영하도록 데이터 카탈로그를 업데이트하면 데이터를 효율적으로 쿼리 및 분석할 수 있습니다. AWS Glue는 파티션과 데이터 카탈로그 간 동기화 프로세스를 자동화하는 기능도 제공합니다.

 

Amazon S3를 사용한 예 하나를 살펴보겠습니다.

 

첫째, AWS Glue에서 데이터 카탈로그 역할을 수행할 데이터베이스를 생성합니다. 이를 위해 AWS Glue 콘솔 또는 AWS Glue API 및 SDK를 사용할 수 있습니다.

둘째, 데이터 소스의 크롤러를 생성합니다. AWS Glue 콘솔에서 데이터 소스에 대한 크롤러를 생성합니다. 이 예에서는 분할된 데이터가 포함된 S3 버킷입니다. 크롤러는 자동으로 데이터와 해당 파티션을 검색하여 메타데이터 및 스키마 정보를 수집합니다.

셋째, 파티셔닝 스키마를 정의하려면 크롤러를 구성할 때 파티셔닝 스키마를 지정합니다. 예를 들어 데이터가 날짜 기준으로 분할된 경우 파티셔닝 열과 해당 데이터 유형을 제공합니다. 파티션이 자동으로 감지되지 않을 경우 AWS Glue API를 사용하여 데이터 카탈로그에서 파티션을 수동으로 추가하거나 수정할 수 있습니다.

넷째, 크롤러를 실행하여 데이터 소스를 스캔하고 파티션에 대한 메타데이터를 수집합니다. 크롤러는 검색된 정보로 데이터 카탈로그를 업데이트합니다.

다섯째, 크롤러를 예약합니다. 데이터 수집 빈도에 따라 주기적으로 크롤러를 실행하도록 예약하여 새 파티션이 추가되거나 기존 파티션이 변경될 때 데이터 카탈로그를 최신 상태로 유지할 수 있습니다.

여섯째, Athena를 사용하여 데이터를 쿼리합니다. 데이터 카탈로그가 파티션을 정확하게 반영하도록 업데이트되면 Athena 또는 기타 쿼리 엔진을 사용하여 특정 파티션 기준에 따라 데이터를 효율적으로 쿼리할 수 있습니다. AWS Glue 데이터 카탈로그를 파티션과 동기화하면 분석 및 데이터 처리 태스크를 위해 데이터를 정리하고, 검색하고, 액세스할 수 있습니다.

 

AWS Glue에서 카탈로그화를 위해 새로운 소스 또는 대상 연결을 생성하는 방법에 대해서도 이야기해 보겠습니다. AWS Glue에서 새로운 소스 또는 대상 연결을 생성하는 이유는 무엇입니까? AWS 환경 내 다양한 소스 및 대상에서 데이터를 액세스하고, 카탈로그화하고, 처리하기 위해 데이터 자산을 관리하는 데 도움이 되기 때문입니다. 데이터 소스 또는 데이터 대상에 대한 연결을 생성할 수 있습니다. 선택하는 연결 유형은 데이터를 수집하기 위해 소스에 연결하는지, 데이터를 저장하거나 기록하기 위해 대상에 연결하는지에 따라 달라집니다.

 

예를 들어 데이터 소스로 S3 버킷에 연결하는 경우 S3 버킷 경로를 제공하고 선택적으로 포함/제외 패턴을 제공해야 합니다. 또는 Amazon RDS에 연결하는 경우 데이터베이스 엔드포인트, 포트, 자격 증명 및 기타 관련 정보를 제공해야 합니다. 연결을 테스트하여 AWS Glue가 제공된 구성을 사용하여 데이터 소스 또는 대상에 성공적으로 액세스할 수 있는지 확인할 수 있습니다. 연결을 저장하면 AWS Glue 콘솔의 연결 페이지에 있는 연결 목록에 표시됩니다. 이제 AWS Glue 크롤러 또는 기타 ETL 작업에서 이 연결을 사용하여 데이터에 액세스하고 카탈로그화할 수 있습니다. 새 연결이 데이터 소스인 경우 크롤러를 생성하여 자동으로 데이터를 검색하고 카탈로그화할 수 있습니다.

 

크롤러는 생성된 연결을 사용하여 데이터에 액세스하고 데이터 카탈로그에 메타데이터를 채웁니다. AWS에서 메타데이터 및 데이터 카탈로그는 데이터 자산을 구성, 설명, 관리하는 데 도움이 되며 사용자 및 애플리케이션이 데이터의 특성, 구조, 스키마, 사용법을 이해하는 데 도움이 됩니다.

 

이 시험에서는 메타데이터의 구성 요소와 데이터 카탈로그의 구성 요소를 알고 있어야 합니다.

 

메타데이터 구성 요소는 다음과 같습니다. 데이터 유형, 필드 이름, 필드 간 관계 등 데이터의 구조를 설명하는 스키마 정보. 데이터세트에서 다양한 속성의 데이터 유형을 지정하는 데이터 유형 정보. 데이터베이스, 데이터 레이크, 데이터 웨어하우스 등 데이터의 오리진을 나타내는 데이터 소스 정보. 생성 시간, 수정 시간 및 기타 시간 관련 정보를 기록하는 타임스탬프. 데이터 정확성, 완전성 및 일관성에 대한 세부 정보를 제공하는 데이터 품질 지표.

 

데이터 소유자와 데이터에 액세스할 수 있는 사용자를 정의하는 데이터 소유권 및 액세스 제어.

 

데이터 카탈로그 구성 요소는 다음과 같습니다. 테이블 및 데이터베이스, 열 이름, 데이터 유형 및 기타 속성을 포함하여 데이터의 구조를 정의하는 테이블 스키마. 쿼리 효율을 높이기 위해 데이터 카탈로그에 저장되는 파티셔닝 정보. AWS Glue에서는 날짜, 리전 등 특정 기준에 따라 데이터를 분할할 수 있습니다. 위치 및 스토리지 정보도 데이터 카탈로그에 기록됩니다. 예를 들면 S3 버킷 경로 또는 데이터베이스 엔드포인트입니다. 통계 및 메타데이터 크롤링은 데이터 카탈로그에 귀중한 정보를 채웁니다. AWS Glue 크롤러는 자동으로 데이터를 검색하고 다양한 소스에서 메타데이터를 수집합니다.

 

다양한 데이터세트와 처리 단계 간에 데이터가 어떻게 흐르는지 보이기 위해 추적할 수 있는 데이터 계보 및 종속성 정보.

 

특정 기준에 따라 데이터를 분류하고 검색하는 기능을 제공하는 태깅 및 레이블.

액세스 제어 정책은 승인된 사용자 및 애플리케이션만 특정 데이터 세트를 보거나 상호 작용할 수 있도록 하는 데 도움이 됩니다.

 

사용자가 해당 데이터로 작업을 수행하기 위한 데이터 분석 및 비즈니스 도구와의 통합.



보안은 영역 4에서 다룰 것입니다. 하지만 방금 액세스 제어 정책에 대해 언급했으므로 다양한 보안 및 규정 준수 조치를 구현하여 요구 사항에 따라 데이터를 분류하는 방법에 대해서도 알아보겠습니다. 데이터 분류는 데이터를 다양한 수준의 민감도 또는 중요도로 분류하는 것으로, 적절한 보안 제어 및 액세스 정책을 적용하여 그에 따라 데이터를 보호하기 위한 것입니다.

 

AWS는 요구 사항에 따라 데이터를 분류하고 관리하는 데 도움이 되는 여러 도구 및 서비스를 제공합니다. IAM을 사용하면 IAM 역할, 그룹 및 정책을 정의하여 AWS 서비스, 리소스, 데이터에 대한 액세스를 제어할 수 있습니다. 또한 AWS는 저장 시 데이터와 전송 중 데이터를 보호하기 위한 다양한 암호화 옵션을 제공합니다.

 

예를 들어 AWS KMS는 AWS 서비스에 저장된 데이터의 암호화 키를 관리하는 데 도움이 됩니다. Amazon Virtual Private Cloud(VPC) 및 보안 그룹은 리소스에 대한 액세스를 격리하고 제어하는 데 도움이 됩니다. AWS 서비스 제어 정책(SCP)은 AWS Organizations에서 권한을 관리하는 데 사용됩니다. 멤버 계정이 특정 작업 또는 특정 서비스에 액세스하는 것을 방지하는 제어 기능을 설정하는 기능을 제공합니다. 이는 데이터 보안 및 규정 준수를 보장하는 데 도움이 됩니다.

 

Amazon Macie는 기계 학습을 사용하여 AWS에서 민감한 데이터를 자동으로 검색, 분류, 보호하는 보안 서비스입니다. 특정 요구 사항에 따라 이러한 AWS 서비스와 모범 사례를 조합하여 AWS에서 데이터를 안전하게 분류, 보호, 관리할 수 있습니다.

 

다음에는 영역 2의 세 번째 작업 설명인 데이터 수명 주기 관리를 시작하겠습니다.


## 3. 데이터 수명주기 관리

영역 2의 세 번째 작업 설명 데이터 수명 주기 관리를 시작하겠습니다. 이 영역에 대한 소개 강의에서 데이터 수명 주기 전반에 걸쳐 소스 시스템의 데이터 관리와 그 중요성에 대해 자주 언급했습니다.

 

변환 단계에서는 데이터 관리가 중요합니다. 변환을 통해 관리가 필요한 새로운 데이터 세트가 생성되기 때문입니다. 데이터 엔지니어는 저장하는 데이터, 데이터 스토리지 수명 주기 및 데이터 보존을 고려해야 합니다.

 

다음은 몇 가지 질문입니다. 이 데이터는 얼마나 중요합니까? 이 데이터를 얼마나 오래 보관해야 하며 얼마나 자주 데이터에 액세스합니까?

 

쿼리 액세스 패턴은 데이터 액세스 빈도에 따라 데이터세트마다 다릅니다. 따라서 스토리지는 필요한 지속성의 세 가지 범주인 핫, 웜, 콜드로 분류될 수 있습니다. 일반적으로 새로운 데이터일수록 더 자주 액세스합니다.

 

데이터에 대한 스토리지 및 스토리지 계층을 선택할 때 각 계층 스토리지의 비용도 포함해야 합니다. 빠른 액세스를 보장하기 위해 모든 데이터를 핫 데이터로 저장하면 스토리지 비용이 증가합니다. 그러나 모든 데이터를 콜드 스토리지에 저장하면 검색 시간이 길어지고 해당 데이터에 액세스하는 데 드는 검색 비용이 증가할 수 있습니다.

 

그렇다면 또 다른 질문은 어떻게 하면 데이터 수명 주기를 기준으로 스토리지 비용을 최적화할 수 있는가 하는 것입니다. 데이터세트가 Amazon S3에 저장되는 경우 S3 수명 주기 정책을 사용하여 Amazon S3에 저장된 데이터의 스토리지 계층을 변경할 수 있습니다. 비즈니스 및 법적 요구 사항을 충족하기 위해 S3 수명 주기 정책을 사용하여 데이터를 삭제할 수도 있습니다.

 

또한 Amazon Redshift에서 데이터 수명 주기를 기반으로 스토리지 비용을 최적화할 수도 있습니다. 데이터 웨어하우징 용량 및 성능 요구 사항이 변화 또는 확장함에 따라 클러스터의 크기를 조정하여 Amazon Redshift의 다양한 컴퓨팅 및 스토리지 옵션을 활용할 수 있습니다.

 

예를 들어 탄력적 크기 조정을 사용하여 노드를 추가 또는 제거하고 클러스터의 노드 유형도 변경할 수도 있습니다.

그러면 앞서 고려한 데이터 보존 질문으로 다시 돌아가게 됩니다. 어떤 데이터를 유지해야 하며, 이 데이터를 얼마나 오래 저장해야 합니까? 이는 데이터를 올바르게 분류해야 하는 이유를 보여주는 훌륭한 예이므로 잠시 멈추고 기본 사항을 되짚어 보겠습니다. 데이터에는 가치가 있으므로 저장하는 데이터의 가치를 알아야 합니다. 보호하고 있는 데이터에 대해 생각해 보십시오. 데이터는 어떻게 저장되며 누가 액세스할 수 있나요? 모든 데이터가 동일하게 생성되는 것은 아니므로 데이터를 올바르게 분류하는 것은 보안에 매우 중요합니다.

 

고려해야 할 또 다른 분류는 데이터를 다시 생성할 수 있는가입니다. 일부 규제 및 규정 준수 요구 사항에 따라 지정된 기간 동안 데이터를 저장해야 할 수도 있습니다. 그리고 보존 요구 사항을 충족하면서 데이터를 저장하고 검색하려면 데이터 보존 정책 및 보관 전략이 필요합니다.

 

여기서 또 다른 질문으로 이어집니다. 데이터가 보호되고 적절한 복원력과 가용성을 갖추고 있는지 어떻게 보장합니까? 적절한 복원력 및 가용성으로 데이터를 보호하려면 내구성, 가용성 및 장애 발생 시 복구를 보장하도록 설계된 AWS 서비스와 모범 사례를 조합하여 구현할 수 있습니다.

 

방금 데이터 복제에 대해 이야기했는데요. 여러 가용 영역 또는 리전에 걸쳐 데이터를 복제하는 데 사용할 수 있는 다양한 AWS 서비스를 알아야 합니다. 예를 들어 Amazon S3는 자동으로 여러 가용 영역에 걸쳐 데이터를 복제하므로 고가용성 및 내구성을 보장합니다. 또한 Amazon RDS는 가용 영역 장애가 발생할 경우 자동 장애 조치를 보장하는 다중 AZ 구성이 있습니다.

 

데이터 보호에는 백업 및 복원도 포함됩니다. 예를 들어 AWS Backup을 사용하거나 데이터베이스에 대한 자동 스냅샷을 구성합니다. 또 다른 백업 및 복원 옵션은 여러 리전에 걸쳐 데이터를 복제하는 것입니다. 데이터 보호에는 실수로 인한 삭제 또는 덮어쓰기에서 데이터를 보호하기 위한 버전 관리도 포함될 수 있습니다. Amazon S3에는 객체의 여러 버전을 저장할 수 있는 버전 관리 기능이 있습니다.

 

여기서 질문이 있습니다. 테이블에서 특정 타임스탬프 이후의 항목을 자동으로 삭제하기 위해 어떤 AWS 서비스 기능을 사용할 수 있습니까? DynamoDB에는 만료 날짜가 있는 데이터를 관리하거나 오래된 데이터를 정리하는 데 도움이 되는 유지 시간(TTL) 기능이 있습니다.

다른 질문도 살펴보겠습니다. 처리를 위해 Amazon Managed Streaming for Apache Kafka 클러스터로 전송된 후 Amazon Redshift에 저장되는 데이터가 있습니다. 시간이 지남에 따라 데이터가 저장되면서 Amazon Redshift 클러스터의 쿼리 성능에서 문제가 발견되기 시작합니다. 또한 일부 유지 관리 태스크를 완료하는 데에도 더 오래 걸립니다. 데이터에 고정된 보존 기간이 있는 경우 각 테이블이 동일하지만 서로 다른 시간 범위의 데이터를 포함하도록 해당 데이터를 일련의 시계열 테이블로 구성할 수 있습니다. 그런 다음 이전 테이블에서 DROP TABLE 명령을 실행하여 오래된 데이터를 제거할 수 있습니다.

 

하지만 왜 대규모 DELETE 프로세스를 실행하는 대신 DROP TABLE 명령을 실행해야 하는지 의문이 들을 수 있습니다.

DROP TABLE을 사용하면 해당 공간을 회수하기 위해 후속 VACUUM 프로세스를 실행할 필요가 없습니다. 시계열 테이블 세트 수가 줄어들면 데이터가 특정 쿼리에 대해 인덱싱되고 최적화되어 쿼리 시간을 개선하고 성능을 향상시키는 데 도움이 될 수 있습니다. 또한 각 데이터세트에 대해 새 테이블을 생성하고 기존 테이블을 삭제하면 VACUUM을 실행할 필요가 없어지고 테이블이 타임스탬프 기준으로 정렬되면 비용과 시간을 절약할 수 있습니다.

 

다른 질문도 살펴보겠습니다. 현재 Amazon S3 버킷에서 S3 Standard 스토리지 계층을 사용하여 여러 소스 및 데이터 스토어의 CSV 형식 데이터를 대량으로 저장하고 있습니다. Athena 집계 함수를 사용하면 이 데이터에 대한 인사이트와 요약된 보기를 얻을 수 있습니다. 6개월 후에는 데이터에 대한 액세스가 거의 발생하지 않지만, 2년 후에는 원시 데이터를 보관 및 저장해야 한다는 규정 요구 사항이 있습니다. 이 설계에서 스토리지 비용을 어떻게 최적화하고 어떤 S3 스토리지 계층을 선택하시겠습니까?

 

이 데이터를 쿼리하려면 AWS Glue ETL 작업을 사용하여 원시 데이터를 열 기반 형식으로 압축, 분할, 변환할 수 있습니다. 그런 다음 Athena를 사용하여 이 처리된 데이터를 쿼리합니다. 저라면 처리된 데이터를 6개월 후에 S3 Standard 스토리지 계층에서 S3 Standard-Infrequent Access 스토리지 계층으로 전환하는 S3 수명 주기 정책을 생성하겠습니다. 그런 다음 2년 후에 Amazon Simple Storage Service Glacier(Amazon S3 Glacier)로 데이터를 전환하는 또 다른 S3 수명 주기 정책을 생성하겠습니다.

 

다음에는 영역 2의 네 번째 작업 설명을 시작하겠습니다.

## 4. 데이터 모델 설계 및 스키마 진화 

영역 2의 네 번째 작업 설명인 데이터 모델 설계 및 스키마 진화를 시작하겠습니다. 이미 몇 번 언급했듯이 데이터에서 비즈니스 인사이트를 얻으려면 데이터가 사용 가능한 형식이어야 합니다. 데이터를 사용 가능한 형식으로 변환하는 과정이 데이터 모델링 및 설계입니다.

 

데이터 엔지니어는 모델링 및 모범 사례와 다양한 사용 사례의 데이터 소스에 적절한 수준 및 유형의 모델링을 적용하기 위한 유연성을 이해해야 합니다. 여기서 질문이 있습니다. 이벤트 데이터에서 엄격한 정규화가 잘 작동합니까? 그렇지는 않습니다. 요즘에는 데이터가 데이터 소스 및 사용 사례에 따라 다르기 때문입니다.

 

AWS는 논리적 분리, 차원, 속성 등을 유지하면서 데이터 모델의 유연성을 제고하는 서비스 및 도구를 제공합니다. 예를 들어 Amazon Redshift는 스타 스키마 또는 눈송이 스키마 데이터 모델링을 지원하고 Kimball 방법과 같은 데이터 모델링 패턴도 지원합니다. Inmon 및 데이터 볼트 데이터 모델링 패턴을 사용하고 구현할 수도 있지만 추가 데이터 변환 및 사용자 지정 솔루션이 필요할 수 있습니다.

 

데이터 엔지니어는 Amazon Redshift Serverless에서 데이터 웨어하우스의 데이터를 가져오고 쿼리하여 데이터에서 인사이트를 얻을 수 있습니다. 또한 엔지니어는 쿼리 편집기 v2를 사용하여 스키마 및 테이블을 구축하고, 시각적으로 데이터를 가져오고, 데이터베이스 객체를 탐색할 수 있습니다.

 

또한 Spark와 같은 데이터 처리 프레임워크를 이해해야 합니다. Spark를 Amazon EMR, Amazon S3, AWS Glue 등의 AWS 서비스와 결합하여 정형 및 비정형 데이터를 대규모로 처리하는 데이터 처리 파이프라인을 구축할 수 있습니다. 이것은 어떻게 작동할까요?

 

Amazon EMR을 사용하면 필요 시 Spark 클러스터를 시작하고, 데이터를 병렬로 처리하고, 필요에 따라 클러스터의 크기를 조정할 수 있습니다. Amazon S3를 사용하면 Amazon S3의 데이터를 Spark DataFrames 또는 복원력 있는 분산 데이터세트로 직접 읽어들일 수 있습니다. 예를 들어 Spark의 spark.read API를 사용하여 정형 데이터를 읽거나 spark.sparkContext.textFile API를 사용하여 Amazon S3에서 직접 원시 텍스트 데이터를 읽을 수 있습니다. AWS Glue의 경우 AWS Glue 데이터 크롤러를 사용하여 자동으로 Amazon S3에 있는 정형 데이터의 스키마를 추론하고 데이터 카탈로그에 대응하는 테이블을 생성할 수 있습니다. 그러면 Spark가 이 메타데이터를 사용하여 정형 데이터를 처리할 수 있습니다.

 

AWS는 Spark를 보완하고 다양한 데이터 유형을 처리할 수 있는 다른 데이터 처리 및 분석 서비스를 제공합니다. 예를 들어 Athena는 Amazon S3에 저장된 정형 데이터에 대한 대화형 SQL 쿼리에 사용할 수 있으며, Amazon Comprehend는 비정형 텍스트 데이터에 대한 자연어 처리 태스크에 사용할 수 있습니다.

 

데이터 엔지니어로서또 다른 핵심 기본 사항은 데이터 계보를 사용하여 데이터의 정확성 및 신뢰성을 보장하는 것입니다. 데이터 계보는 데이터를 처리하는 시스템 또는 파이프라인에서 오류 추적, 책임 소재, 데이터 디버깅에 도움이 됩니다. 데이터가 데이터 수명 주기를 통과함에 따라 어떤 시스템이 데이터에 영향을 미치는지, 데이터가 파이프라인을 따라 전달될 때 데이터가 어떻게 구성 또는 변환되는지 이해해야 합니다.

 

이 시험에서는 수명 주기 전반에서 데이터의 오리진, 변환 및 흐름을 추적하기 위해 데이터 계보를 설정하는 방법을 이해해야 합니다. AWS는 데이터 계보를 설정하고 더 나은 데이터 거버넌스, 규정 준수, 데이터 흐름 이해를 보장하는 데 도움이 되는 도구 및 서비스를 제공합니다. 예를 들어 Amazon SageMaker ML 계보 추적은 기계 학습 관련 데이터 계보를 추적합니다.

 

SageMaker ML 계보 추적을 사용하여 데이터 계보를 설정하는 방법을 살펴보겠습니다. SageMaker는 대규모로 기계 학습 모델을 빌드, 훈련, 배포하는 기능을 제공하는 완전관리형 서비스입니다. SageMaker에서는 훈련 작업 및 모델 배포에 적합한 구성을 설정하여 ML 계보 추적을 선택할 수 있습니다. ML 계보 추적이 활성화되면 SageMaker가 자동으로 훈련 데이터 및 모델 아티팩트에 대한 메타데이터를 추적하고 캡처합니다. 이 메타데이터에는 입력 데이터, 출력 모델, 하이퍼파라미터에 대한 정보 ML 워크플로와 관련된 기타 세부 정보가 포함됩니다.

 

SageMaker ML 계보 추적은 훈련 데이터에 적용된 데이터 처리 단계 및 변환도 기록합니다. 이 모든 정보는 훈련 데이터의 계보를 설정하고 재현성을 보장하는 데 도움이 됩니다. SageMaker ML 계보 추적은 예측에 사용된 모델 아티팩트 및 추론 데이터도 추적할 수 있습니다. 이 기능은 전체 ML 수명 주기에 걸쳐 계보를 설정하는 데 도움이 됩니다. Amazon SageMaker ML 계보 추적은 AWS Glue 및 Amazon S3와 통합되어 데이터 계보 정보를 추가로 보강하고 다양한 데이터 소스 및 프로세스 전반에 걸쳐 데이터 흐름을 캡처할 수 있습니다.

AWS의 인덱싱, 파티셔닝 전략, 압축 및 기타 데이터 최적화 기술에 대한 모범 사례에 대해서도 살펴보겠습니다. Amazon RDS의 인덱싱 기술은 무엇입니까? WHERE 절 또는 조인 작업에서 자주 사용되는 열을 식별하고 해당 열에 인덱스를 생성하여 데이터 검색 속도를 높일 수 있습니다.

 

DynamoDB의 경우 효율적인 쿼리 패턴을 지원하려면 올바른 기본 키와 보조 인덱스를 선택해야 합니다. 파티셔닝의 경우 Amazon S3를 사용하여 대규모 데이터세트를 저장할 수 있습니다. Athena 또는 AWS Glue를 사용하여 데이터의 하위 집합을 관리하고 쿼리하기 위해 데이터를 속성 또는 범주를 기반으로 논리적 폴더로 분할할 수 있습니다.

 

Amazon Redshift의 경우 배포 키와 정렬 키를 사용하여 노드 전체에 데이터를 균일하게 분산하고 쿼리 중 데이터 이동을 최소화하도록 테이블을 생성할 수 있습니다.

 

S3 객체의 경우 gzip 또는 bzip2와 같이 지원되는 압축 형식을 사용할 수 있습니다. Amazon Redshift에서는 반복적이거나 압축률이 높은 데이터가 포함된 열에 압축 인코딩을 적용하여 스토리지 및 쿼리 성능을 최적화할 수 있습니다. Amazon S3에 저장된 데이터에 대해 Parquet 또는 ORC와 같은 데이터 직렬화 형식을 사용할 수 있습니다.

캐시 및 자주 액세스하는 데이터의 경우 ElastiCache 또는 Amazon CloudFront를 사용하여 백엔드 데이터베이스에서 로드를 줄이고 읽기 집약적인 워크로드의 데이터 액세스 속도를 높일 수 있습니다.

 

Amazon S3 Glacier 또는 Amazon S3 Glacier Deep Archive와 같은 비용 효율적인 스토리지 계층에 과거 데이터 또는 자주 액세스하지 않는 데이터를 저장하도록 데이터 보관 및 보존 정책을 정의할 수 있습니다.

 

프로비저닝된 처리량 및 자동 크기 조정의 경우 DynamoDB를 사용하여 애플리케이션 요구 사항에 따라 읽기 및 쓰기 처리량을 프로비저닝할 수 있습니다.

자동 크기 조정을 사용하여 자동으로 용량을 조정할 수도 있습니다.

쿼리 성능을 최적화하려면 Amazon RDS Performance Insights 또는 Amazon Redshift 쿼리 모니터링을 통해 쿼리를 튜닝하고 SQL 문을 최적화하여 성능 병목 현상을 식별하고 해결할 수 있습니다.

데이터 전송을 최적화하려면 DataSync 또는 Transfer Family를 사용할 수 있으며, 물론 Aurora, DynamoDB 또는 OpenSearch 서비스와 같은 AWS 관리형 서비스를 사용할 수 있습니다.

그리고 데이터 스토리지 및 액세스 패턴을 지속적으로 모니터링하여 개선할 영역을 식별하고 최적화를 구현해야 합니다.

 

이 시험에서는 다양한 유형의 데이터에서 귀중한 인사이트를 얻기 위해 적절한 스토리지 및 처리 옵션을 선택할 수 있도록 AWS에서 정형, 반정형 및 비정형 데이터를 모델링하는 방법을 알아야 합니다.

정형 데이터의 경우 Amazon RDS, Aurora, Amazon Redshift를 사용할 수 있습니다.

반정형 데이터의 경우 DynamoDB, Amazon DocumentDB(MongoDB 호환성), Athena, Amazon S3를 사용할 수 있습니다.

비정형 데이터의 경우 Amazon S3, Amazon Rekognition, Amazon Transcribe, Amazon Comprehend를 사용할 수 있습니다.

이어서 AWS에서의 스키마 진화 기술에 대해 살펴보겠습니다. 스키마 진화는 이벤트 데이터에서 일반적입니다. 필드가 추가 또는 제거될 수 있기 때문입니다. 또한 값 형식은 예를 들어 문자열에서 정수로 변경될 수 있으며 이러한 변경은 데이터 파이프라인에 영향을 미칩니다. 

 

스키마 진화는 이벤트 데이터에서 일반적입니다. 필드가 추가 또는 제거될 수 있기 때문입니다. 또한 값 형식은 예를 들어 문자열에서 정수로 변경될 수 있으며 이러한 변경은 데이터 파이프라인에 영향을 미칩니다. 스키마 진화는 기존 데이터를 보존하고 이전 버전과의 호환성을 보장하면서 데이터베이스 또는 데이터 스토어에 저장된 데이터의 스키마 또는 구조를 변경하는 프로세스입니다. AWS에는 데이터 스키마 변경 사항을 효과적으로 관리하기 위해 스키마 진화를 지원하는 여러 기술 및 서비스가 있습니다.

DynamoDB 스키마 진화의 경우 데이터의 새로운 속성 또는 변경 사항을 수용하도록 애플리케이션을 업데이트할 수 있습니다. DynamoDB는 새 스키마를 자동으로 처리하며 새 속성이 없는 데이터는 계속 액세스 및 사용할 수 있습니다.

 

Amazon RDS 및 Aurora의 경우 스키마 진화가 NoSQL 데이터베이스에 비해 더 복잡할 수 있습니다. Amazon RDS 또는 Aurora의 스키마 진화 기술에는 AWS DMS를 사용하여 가동 중지 시간을 최소화하면서 스키마 변경을 수행하거나 저장 프로시저(stored procedure)를 사용하여 제어된 방식으로 데이터 스키마를 업데이트하는 것이 포함될 수 있습니다.

데이터 레이크, 특히 Amazon S3를 사용하여 구현된 데이터 레이크는 읽기 스키마(Schema-on-read) 접근 방식을 따르는 경우가 많습니다. 즉, 데이터가 변환되지 않은 원시 형식으로 저장되고 스키마 또는 구조가 저장된다는 의미입니다.

 

데이터 처리 및 쿼리를 위해 Athena 또는 AWS Glue를 사용하면 분석 프로세스에서 데이터가 해석되는 방식을 조정하여 스키마를 향상시킬 수 있습니다. 스키마 버전 관리 기술을 구현하여 데이터 스키마에 대한 변경 사항을 추적하고 관리할 수 있습니다. 데이터 레코드 또는 메타데이터에 버전 관리 정보를 추가하여 각 데이터 레코드가 유지하는 스키마 버전을 이해할 수 있습니다. AWS Glue는 스키마 버전 관리를 지원하므로 데이터 구조의 버전을 정의하고 시간 경과에 따른 변경 사항을 추적할 수 있습니다.

 

반정형 또는 비정형 데이터로 작업할 때 Avro, Parquet, ORC와 같은 열 기반 데이터 형식을 사용하면 스키마 진화에 도움이 될 수 있습니다. 이러한 형식은 설계상 스키마 진화를 지원하고 기존 데이터를 손상시키지 않으면서 필드를 추가, 제거 또는 수정하는 기능을 제공합니다. 또한 자동화된 ETL 작업에 AWS Glue를 사용하여 데이터를 한 스키마에서 다른 스키마로 변환할 수 있으며 AWS Glue는 데이터 이동 및 변환 프로세스 중에 스키마 변경을 관리하는 데 도움이 될 수 있습니다.

 

여기서 질문이 있습니다. 스키마 진화와 관련하여 발생할 수 있는 문제를 사전에 해결하기 위해 사용할 수 있는 기술은 무엇입니까? 배달 못한 편지 대기열이 가장 먼저 생각납니다. 그러나 스키마 레지스트리 사용, 스키마 검증, 스키마 버전 관리 구현, 스키마 변경 사항 적용 전 백업 수행, 모니터링 및 경고 등을 설정할 수도 있습니다.

어떻게 하면 새 스키마 또는 버전을 검증하기 위해 자동화된 테스트를 구성할 수 있습니까? AWS Glue 및 AWS Data Pipeline을 사용하거나 사용자 지정 스크립트를 사용할 수 있습니다. AWS Glue DataBrew는 데이터의 잠재적인 문제를 프로파일링하고 시각화하는 데 도움이 됩니다.

스키마 변환은 데이터베이스를 한 데이터베이스 엔진에서 다른 데이터베이스 엔진으로 마이그레이션하거나 AWS로 마이그레이션할 때에도 중요합니다. AWS Schema Conversion Too은 데이터베이스 스키마를 한 데이터베이스 엔진에서 다른 데이터베이스 엔진으로 변환하는 데 도움이 됩니다. 이 도구는 Amazon RDS 또는 Aurora로 마이그레이션할 스키마 및 코드를 대상 데이터베이스 엔진과 호환되도록 변환하는 데 사용됩니다. AWS SCT는 평가를 통해 스키마 및 코드의 문제를 식별한 다음 변환 프로세스 중에 이러한 문제를 해결하는 방법에 대한 지침을 제공합니다. 또한 AWS SCT는 데이터 유형 매핑, 저장 프로시저(stored procedure) 변환, 인덱스 변환 등 스키마 변환의 여러 측면을 자동화하여 마이그레이션 프로세스를 간소화합니다.

 

AWS DMS는 AWS SCT와 함께 작동하여 마이그레이션 프로세스 중에 소스 데이터베이스 스키마가 대상 데이터베이스 엔진과 호환되도록 변환합니다. AWS DMS는 Amazon RDS, Aurora, Amazon Redshift 등 다양한 대상 데이터베이스 엔진에서 스키마 변환을 지원합니다.

 

다음에는 세 번째 연습 문제를 시작하겠습니다.


# 영역3. Data Operations and Support 

## 1. AWS 서비스를 사용하여 데이터 처리 자동화

영역 3의 첫 번째 작업 설명인 AWS 서비스를 사용하여 데이터 처리 자동화를 시작하겠습니다. 데이터 운영에는 데이터 및 시스템 그리고 변경 사항 및 이상 상태에 대한 모니터링 및 자동화된 경고가 포함됩니다.

AWS에서 워크로드를 실행하는 이점 중 하나는 인프라 배포의 모든 측면을 자동화할 수 있다는 것입니다. CloudFormation을 사용하면 데이터 파이프라인을 지원하는 인프라를 구축하기 위한 템플릿 파일을 생성할 수 있습니다. 예를 들어 CloudFormation 템플릿은 S3 버킷을 생성할 수 있습니다. 새 파일이 버킷에 기록될 때 Lambda 함수를 실행하는 EventBridge 규칙을 템플릿에 추가하면 S3 버킷에 기록되는 파일을 모니터링할 수 있습니다. 그리고 Amazon SNS 주제를 템플릿에 추가하여 알림을 보낼 수 있습니다.



또한 다른 Lambda 함수를 템플릿에 추가하여 새 파일이 수신되었는지 확인한 후 Step Function 상태 머신을 시작할 수 있습니다. 그러면 상태 머신이 AWS Glue 작업을 시작하여 파일을 처리한 다음 AWS Glue 크롤러를 실행하여 데이터 카탈로그를 업데이트합니다. 이 경우에도 Amazon SNS를 사용하여 모든 실패에 대한 알림을 보낼 수 있습니다. 그런 다음 이 템플릿을 Amazon S3 또는 CodeCommit에 저장하여 액세스, 변경 사항, 버전 등을 관리할 수 있습니다.

지속적 통합 및 전체 파이프라인 자동화를 위해 CodePipeline을 사용할 수 있습니다. 예를 들어, 새 Python 파일이 소스 리포지토리, 즉 CodeCommit에 커밋되면 CodeBuild를 사용하여 새로 커밋된 코드를 자동으로 빌드하고 존 코드에 통합할 수 있습니다. 또한 이 단계에서 단위 테스트를 실행하여 코드 품질을 테스트하고 특정 함수가 예상된 출력을 반환하는지 확인할 수 있습니다. 이를 통해 파일 수집, 변환 파이프라인 실행, 출력 파일 검증 등 자동화된 추가 엔드투엔드 테스트를 통해 코드 변경 사항을 자동으로 배포하는 지속적 전달을 구현할 수 있습니다.



Amazon MWAA 및 Step Functions를 사용하여 데이터 파이프라인을 오케스트레이션하는 방법에 대해 살펴보겠습니다. Amazon MWAA는 복잡한 워크플로를 오케스트레이션하기 위한 오픈 소스 플랫폼인 Apache Airflow를 제공하는 관리형 서비스입니다. Amazon MWAA에서는 Apache Airflow 워크플로 관리 기능을 사용하여 데이터 파이프라인을 생성, 예약, 모니터링할 수 있습니다.



이전 강의에서도 언급했지만 Apache Airflow는 DAG를 정의하여 데이터 워크플로를 표현하는 기능을 제공합니다. 각 DAG는 종속성이 포함 일련의 태스크를 나타내며 이러한 태스크는 Spark 작업, AWS Glue 작업, AWS Batch 작업 또는 파이프라인의 일부로 실행해야 하는 기타 태스크일 수 있습니다. MWAA를 사용하면 Apache Airflow 클러스터를 배포하고 크기를 조정할 수 있으며 Apache Airflow 설정 및 유지 관리 프로세스를 단순화할 수 있습니다.



Step Functions는 분산 애플리케이션 및 마이크로서비스를 워크플로로 조정하고 시각화하는 데 도움이 되는 완전관리형 서비스로, 다양한 AWS 서비스와 사용자 지정 애플리케이션 로직을 통합하는 서버리스 워크플로를 구축하는 기능을 제공합니다. 이미 여러 차례 언급했지만 Step Functions를 사용하면 데이터 파이프라인을 표현하는 상태 머신을 정의할 수 있습니다. 상태 머신은 일련의 상태로 구성되며, 각 상태는 태스크를 실행하거나, 결정을 하거나, 이벤트를 대기할 수 있습니다. 이러한 상태는 Lambda 함수, AWS Glue 작업, Amazon EMR 클러스터 또는 기타 AWS 서비스를 파이프라인의 일부로 시작할 수 있습니다. 또한 Step Functions에는 파이프라인을 모니터링하고 디버깅할 수 있도록 오류 처리 기능, 재시도 메커니즘, 워크플로 시각화 기능이 내장되어 있습니다.



Amazon MWAA는 복잡한 데이터 워크플로를 정의하기 위해 보다 유연한 코드 기반 접근 방식이 필요할 때 유용합니다. Step Functions는 다른 AWS 서비스와 통합되는 서버리스 이벤트 기반 워크플로를 생성하는 데 유용합니다.



여기서 잠시 멈추고 Amazon Managed Workflows 문제 해결에 대해 생각해 보겠습니다. 방금 논의하던 Amazon MWAA의 예로 돌아가겠습니다. 문제 해결은 데이터 워크플로의 설정, 배포, 실행 중에 발생할 수 있거나 발생한 문제를 식별하고 해결하는 것입니다.



기본 사항으로 돌아가서, 문제 해결에는 항상 권장 단계를 사용할 수 있습니다. 저라면 CloudWatch Logs를 확인하여 Apache Airflow DAG 실행 중에 생성된 오류 메시지 또는 예외를 식별하겠습니다.



또한 Apache Airflow 로그를 검토합니다. Apache Airflow는 태스크 실행 및 기타 내부 프로세스에 대한 자체 로그를 생성하기 때문입니다. 다음으로 IAM 역할 및 권한을 확인하여 MWAA, DAG 및 관련 AWS 서비스 (AWS Glue, Amazon S3, Amazon EMR 등)와 연결된 IAM 역할 및 권한이 올바르게 구성되었는지 확인합니다.



그런 다음 종속성 및 파일 참조를 확인하여 DAG와 Python 스크립트에 필요한 종속성이 설치되어 있는지, DAG 내의 파일 참조가 정확한지 확인합니다. DAG가 외부 서비스, API 또는 데이터베이스에 의존하는 경우 해당 종속성이 올바르게 작동하고 MWAA 환경에서 액세스할 수 있는지 확인합니다.



작업자 노드의 수 및 크기와 같이 MWAA 환경에 할당된 리소스를 모니터링하고 조정하는 것이 중요합니다. 워크플로가 리소스 집약적인 경우 메모리 부족 문제 또는 기타 리소스 관련 오류를 방지하기 위해 리소스를 조정해야 할 수도 있습니다.



또한 DAG를 MWAA에 배포하기 전에 개발 컴퓨터 또는 테스트 환경에서 로컬로 DAG를 테스트합니다. 최신 버전의 Apache Airflow 및 관련 플러그인을 사용해야 합니다. 물론 AWS Support를 사용하면 더 어려운 문제를 해결하고 진단하는 데 도움이 됩니다.



방금 외부 종속성에 대해 이야기했는데요. 이 시험에서는 데이터 API를 사용하고 관리하는 방법을 이해하여야 합니다. 데이터 API를 사용 및 관리하는 데 사용할 수 있는 AWS 서비스는 무엇입니까? Lambda를 사용하여 API를 요청하고 응답을 처리하는 서버리스 함수를 생성할 수 있습니다. 이러한 함수는 이벤트 또는 API Gateway에 의해 시작될 수 있습니다.

API Gateway는 외부 클라이언트에 노출할 수 있는 안전하고 확장 가능한 관리형 API 엔드포인트를 제공합니다. 또한 Lambda 함수 또는 기타 백엔드 서비스에 대한 프런트엔드 역할을 하도록 API Gateway를 구성할 수도 있습니다.



AWS SDK는 여러 프로그래밍 언어에서 사용이 가능하여 AWS 애플리케이션 코드 내에서 외부 API와 상호 작용할 수 있습니다.AWS SDK는 인증, 요청 서명 및 응답 구문 분석의 세부 정보를 처리합니다. 데이터 API 통합을 유지 관리하면 시간이 지나도 애플리케이션의 안정성, 성능, 보안이 보장됩니다.

몇 가지 모범 사례와 도움이 될 수 있는 AWS 서비스를 살펴보겠습니다.



CloudWatch를 사용하여 API 요청 및 Lambda 함수에 대한 모니터링 및 로깅을 설정합니다. 오류율, 지연 시간 및 기타 성능 지표를 모니터링하여 문제와 병목 현상을 탐지합니다.



가변적인 API 요청 로드를 자동으로 처리하도록 Lambda 함수 또는 백엔드 리소스에 대한 자동 크기 조정을 구성합니다.



API Gateway 캐싱 또는 ElastiCache를 통해 캐싱 전략을 구현하여 백엔드 시스템의 로드를 줄이고 응답 시간을 개선합니다.



API Gateway의 조절 및 속도 제한 기능을 사용하여 API 리소스의 남용을 방지하고 공정한 사용을 보장합니다.

Lambda 함수에서 API 실패를 처리하고 클라이언트에 의미 있는 오류 메시지를 제공하는 오류 처리를 구현합니다.



IAM 및 OAuth와 같은 인증 및 권한 부여 메커니즘을 사용하여 API 통합을 보호합니다.

API와 통합되는 백엔드 시스템에 대한 데이터 백업 및 재해 복구 전략을 구현합니다. 그리고 Lambda 함수 코드 및 구성도 백업합니다.



API 버전 관리는 이전 버전과의 호환성을 유지하면서 장기적으로 API 변경 사항을 관리하는 데 도움이 될 수 있습니다.

그리고 CloudFront(콘텐츠 전송용), DynamoDB(NoSQL 데이터 스토리지용), Lambda 계층과의 API 통합 성능을 지속적으로 모니터링하고 최적화하여 종속성을 관리하고 함수 패키지 크기를 줄입니다.

계속해서 AWS 서비스의 기능을 사용하여 데이터를 처리하는 방법에 대해 살펴보겠습니다. 도움이 될 수 있는 AWS 서비스에는 무엇이 있습니까? 이미 여러 번 언급했지만 Amazon EMR, Amazon Redshift, AWS Glue가 있습니다.

Lambda를 사용하여 데이터 처리를 자동화하는 방법에 대해서도 방금 이야기했습니다. 이전 강의에서도 언급했듯이 AWS Glue를 사용하여 데이터 변환을 준비할 수 있습니다. Athena를 사용하여 데이터를 쿼리하는 방법도 다루었습니다.



Athena를 사용하여 데이터를 쿼리하는 사용 사례는 무엇입니까? 인프라를 설정할 필요 없이 빠른 데이터 탐색 및 분석을 수행하거나 실행한 쿼리에 대해서만 비용을 지불하려는 시나리오가 있을 수 있습니다. Athena 콘솔에서 쿼리 편집기를 사용하여 SQL 쿼리를 실행하면 표준 SQL 문을 작성하여 데이터를 필터링, 집계, 조인, 분석할 수 있습니다. 쿼리를 실행한 후에는 Athena가 콘솔에 쿼리 결과를 표시합니다. 추가 분석 또는 시각화를 위해 결과를 다운로드하거나 Amazon S3에 저장할 수 있습니다.



이 작업 설명의 또 다른 부분은 이벤트 및 스케줄러 관리입니다. 이 강의 시작 부분에서 EventBridge에 대해 언급했는데요. EventBridge를 사용하면 AWS 환경에서 이벤트 기반 아키텍처를 구현하고 프로세스를 자동화할 수 있습니다. 이벤트 및 스케줄러 관리는 AWS에서 태스크, 워크플로, 프로세스를 자동화하는 데 중요합니다. 사전 정의된 패턴을 기반으로 수신 이벤트를 일치시키는 이벤트 규칙을 생성할 수 있습니다. 이러한 패턴은 이벤트 속성, 사용자 지정 이벤트 버스 또는 기타 조건을 기반으로 할 수 있습니다. 이벤트가 규칙과 일치하면 EventBridge는 추가 처리를 위해 이벤트를 하나 이상의 대상으로 라우팅합니다. EventBridge는 CloudWatch와 통합되어 CloudWatch 지표를 통해 규칙 및 대상 활동을 모니터링하고 이벤트 패턴을 기반으로 경보를 설정합니다.



AWS CloudTrail을 사용하여 EventBridge의 API 호출을 로깅할 수도 있습니다. EventBridge 스키마 레지스트리는 이벤트 스키마를 정의하고 관리하는 데 도움이 되는 선택적 기능입니다. 요약하자면 EventBridge는 이벤트를 모니터링하고, 경고하고, 문제를 해결하고, 대응하는 데 도움이 될 수 있습니다. 데이터 파이프라인을 오케스트레이션하는 예제를 살펴보면서 이 과정에서 지금까지 배운 모든 내용을 보여드리겠습니다.



Step Function을 사용하여 데이터 파이프라인을 오케스트레이션한다고 가정해 보겠습니다. 상태 머신의 경우 Lambda 함수에서 시작하겠습니다. 이 Lambda 함수는 수신 파일의 확장명을 확인하여 파일 유형을 결정합니다. 파일 유형이 결정되면 해당 정보를 다음 단계인 선택 상태로 전달합니다. 파일이 지원되는 유형인 경우 다른 Lambda 함수를 호출하여 해당 파일을 처리하고 상태 머신을 성공 상태로 종료합니다. 파일이 지원되지 않는 유형인 경우 알림을 전송하여 파일 처리 실패를 보고합니다.



여러분도 자신의 AWS 계정에서 이 예제를 시도해보고 성공 사례를 공유해주세요. 어려움이 있으면 연락하시기 바랍니다. 도움이 될 수 있는 단계별 가이드가 있습니다.



하지만 Step Function를 생성하기 전에 먼저 파일 확장명을 확인하는 Lambda 함수를 생성해야 합니다. 둘째, 식별된 유형의 파일을 처리하는두 번째 Lambda 함수를 생성합니다. 셋째, Amazon SNS 주제를 생성하고 이메일에서 해당 주제를 구독합니다. 넷째, 지금까지 생성한 구성 요소를 사용하여 새로운 Step Function 상태 머신을 생성합니다. 그것은 두 개의 Lambda 함수와 하나의 Amazon SNS 주제였습니다. 다섯 번째 단계는 AWS 서비스에 대한 이해도를 점검하기에 좋습니다. CloudTrail은 AWS 계정에서 수행된 활동을 거의 실시간으로 기록합니다. 그러나 객체 수준 데이터 이벤트는 CloudTrail에서 기본적으로 로깅되지 않습니다.



다섯 번째 단계는 CloudTrail 데이터 이벤트를 생성하도록 S3 버킷을 구성하는 것입니다. 관리 이벤트와 데이터 이벤트를 모두 기록하려면 CloudTrail 추적을 생성해야 합니다.



여섯 번째 단계는 제가 강조하고 싶은 부분인데, 바로 이벤트 및 스케줄러 관리입니다. 이 여섯 번째 단계는 Step Function 상태 머신을 시작하는 EventBridge 규칙을 생성하는 것입니다. EventBridge에서 규칙을 생성하고, 패턴을 정의하고, 서비스(이 예제에서는 Amazon S3)를 선택하고, 특정 작업을 선택하고, 특정 버킷을 추가하고, 대상(이 예제에서는 Step Function 상태 머신)을 선택할 수 있습니다.



다음에는 영역 3의 두 번째 작업 설명인 AWS 서비스를 사용하여 데이터 분석을 시작하겠습니다.

## 2. AWS 서비스를 이용하여 데이터 분석

영역 3의 두 번째 작업 설명인 AWS 서비스를 사용하여 데이터 분석을 시작하겠습니다.

이 과정에서 지금까지는 데이터를 수집, 저장, 변환하기 위해 데이터 수명 주기 및 AWS 서비스를 구축하는 데 중점을 두었습니다. 이번 강의에서는 데이터 소비자가 데이터를 분석하고 볼 수 있도록 돕는 AWS 서비스에 대해 이야기하겠습니다.

 

이 시험에서는 데이터 집계, 이동 평균, 그룹화, 피보팅 등 데이터 분석 및 보고에 사용되는 데이터 조작 작업을 이해해야 합니다.

 

여러 레코드의 데이터를 단일 값으로 요약 또는 결합하는 데이터 집계부터 시작하겠습니다. 데이터 집계에 Amazon Redshift, Athena, Amazon OpenSearch Service를 사용할 수 있습니다.

 

Amazon Redshift를 사용하면 SQL 쿼리에서 SUM, AVG, COUNT 등의 집계 함수를 사용하여 Amazon Redshift 데이터 웨어하우스에 저장된 대규모 데이터세트에 대한 데이터 집계를 수행할 수 있습니다.

 

Athena를 사용하면 Amazon S3에 저장된 데이터에 대해 SQL 쿼리를 실행하고 GROUP BY 및 집계 함수를 사용하여 데이터 집계를 수행할 수 있습니다. 또한 Amazon OpenSearch Service는 날짜 히스토그램, 용어 집계 등 다양한 기준에 따라 데이터를 요약하는 강력한 집계 기능을 제공합니다.

 

이동 평균은 지정된 윈도우 또는 기간에 속하는 값 집합의 평균을 계산합니다. 이 평균은 데이터의 변동을 스무딩하는 데 유용합니다. Amazon Redshift 및 Amazon QuickSight를 사용하여 이동 평균을 지원할 수 있습니다. Amazon Redshift를 사용하면 AVG와 같은 윈도우 함수를 적절한 윈도우 사양과 함께 사용하여 Amazon Redshift에서 이동 평균을 계산할 수 있습니다. QuickSight에서는 이동 계산 기능을 통해 시각화에서 기본적으로 이동 평균 계산을 지원합니다.

 

그룹화에는 특정 속성 또는 필드를 기반으로 한 데이터 그룹화가 포함됩니다. 이 작업은 일반적으로 데이터 집계, Amazon Redshift, Athena 및 QuickSight 지원 그룹화와 함께 사용됩니다. Amazon Redshift를 사용하면 SQL 쿼리에 GROUP BY 절을 사용하여 데이터를 그룹화할 수 있습니다. Athena는 데이터 그룹화에 GROUP BY 절을 지원합니다. QuickSight에서는 차원 및 집계를 사용하여 시각화에서 데이터를 그룹화할 수 있습니다.

 

피보팅은 데이터를 행에서 열로 변환하는 것입니다. 이 작업은 보고 목적으로 데이터를 재구성하는 데 유용할 수 있습니다. Amazon Redshift 및 QuickSight는 피보팅을 지원합니다. Amazon Redshift에서는 PIVOT 절을 SQL 쿼리의 집계 함수와 함께 사용하여 데이터를 피벗할 수 있습니다. QuickSight에서는 시각화에서 피벗 테이블을 생성하여 피벗된 형식으로 데이터를 표시하는 기능을 제공합니다.

 

이들은 AWS에서 데이터 분석 및 보고를 위해 수행할 수 있는 데이터 조작 작업의 몇 가지 예입니다. 데이터 소비자가 데이터를 분석하고 보는 데 도움이 되는 AWS 서비스에는 어떤 것이 있습니까?

방금 몇 가지 서비스를 소개하면서 일부는 이미 언급되었지만 이 작업 설명에서 다시 이야기하고 싶습니다.

 

QuickSight는 대화형 대시보드를 생성하고 임시 데이터 분석을 수행하는 기능을 제공합니다. AWS Glue DataBrew는 코드를 작성하지 않고도 분석을 위해 데이터를 정리 및 정규화할 수 있는 시각적 데이터 준비 도구입니다.

Athena는 Amazon S3에 저장된 데이터에 대해 대화형 SQL 쿼리를 직접 실행하는 기능을 제공합니다.

Amazon Redshift는 표준 SQL 쿼리를 사용하여 대규모 데이터세트를 분석하는 기능을 제공합니다. Amazon EMR은 Spark, Hadoop과 같은 인기 있는 프레임워크를 사용하여 대량의 데이터를 처리하는 기능을 제공합니다.

AWS Glue는 분석을 위해 데이터를 검색, 분류, 변환하는 기능을 제공합니다. ETL 코드를 자동으로 생성하여 데이터 스토어 간에 데이터를 이동하고 변환할 수 있습니다.

 

AWS Data Pipeline은 다양한 AWS 서비스와 온프레미스 데이터 소스 간의 데이터 이동 및 변환을 오케스트레이션하고 자동화하는 서비스입니다.

 

Lake Formation은 데이터 소비자에게 데이터 레이크에 저장된 여러 소스의 데이터를 안전하게 액세스하고 분석하는 기능을 제공합니다.

여기서 문제입니다. S3 버킷으로 전송되는 데이터가 있다고 가정해 보겠습니다. 현재는 8시간마다 EC2 인스턴스에서 스크립트를 실행하여 Amazon S3에서 데이터를 추출하고 정리한 후 CSV 파일로 변환합니다. 그런 다음 데이터는 스프레드시트 형식에서 분석됩니다. 대시보드에서 데이터를 시각화하고 검색 기능을 추가하려면 이 설계를 어떻게 업데이트할 수 있습니까? 한 가지 솔루션은 스크립트를 Lambda 함수로 이동하고 결과가 Amazon OpenSearch 클러스터로 전송되도록 구성하는 것입니다. EventBridge를 사용하여 8시간마다 함수를 실행할 수 있습니다. Amazon OpenSearch는 대시보드를 제공하고 로그 분석, 실시간 모니터링, 검색 등을 수행할 수 있는 기능을 제공합니다.

 

다른 질문도 살펴보겠습니다. IoT 센서에서 센서 데이터를 수집하고 매일 시각적 보고서를 제공하려면 비용 효율적인 솔루션을 생성해야 합니다. 저는 이 질문에 비용 효율성을 추가했는데, Amazon EMR에는 임시 및 영구라는 두 가지 클러스터 유형이 있기 때문입니다. 데이터 변환 워크로드를 신속하고 비용 효율적으로 수행하여 대규모 데이터세트를 정렬, 집계, 조인하려면 임시 클러스터를 생성할 수 있습니다. 임시 클러스터가 매일 야간에 데이터를 집계하면 완료 후 임시 클러스터를 자동으로 종료할 수 있습니다. 그런 다음 QuickSight를 사용하여 시각적 보고서를 제공할 수 있습니다.

 

Amazon RDS, Amazon Redshift, Athena 등과 같은 AWS 데이터베이스 서비스에서 SQL 쿼리를 실행하는 방법에 대해서도 이야기해 보겠습니다. 이러한 서비스는 각각 SQL 쿼리를 다르게 실행하지만 SQL 구문은 서비스 간에 일관됩니다.

Amazon RDS 및 Aurora부터 시작하겠습니다. SQL을 사용하여 관계형 데이터베이스의 구조 및 데이터를 관리할 수 있습니다. 예를 들어 SQL 쿼리를 사용하여 데이터를 생성하고, 읽고, 업데이트하고, 삭제할 수 있습니다. DynamoDB는 저장된 위치 또는 형식에 관계없이 데이터를 쿼리하는 오픈 소스 SQL 호환 쿼리 언어인 PartiQL을 지원합니다. PartiQL은 관계형 데이터베이스의 정형 데이터, 개방형 데이터 형식의 반정형 및 중첩 데이터, 심지어 행마다 다른 속성을 허용하는 문서 데이터베이스 또는 NoSQL의 스키마 없는 데이터를 처리하는 데 도움이 됩니다. Amazon DocumentDB는 MongoDB와 호환되는 완전관리형 NoSQL 데이터베이스 서비스입니다.

 

Amazon DocumentDB에서 SQL 유사 쿼리를 사용하여 데이터와 상호 작용할 수 있습니다. AWS Glue에서는 SQL 유사 변환을 사용하여 데이터를 대상 데이터베이스에 로드하기 전에 준비 및 정리할 수 있습니다. Athena는 SQL을 사용하여 Amazon S3, S3 데이터 레이크 및 기타 데이터베이스에 저장된 데이터를 CSV, Parquet 또는 JSON과 같은 형식으로 쿼리하는 기능을 제공합니다.

 

계속하기 전에 Athena에서 Spark를 사용한 데이터 분석 및 탐색에 대해 더 자세히 살펴보겠습니다. Athena에서 Spark 애플리케이션을 실행하기 위해 Spark 코드를 추가하여 결과를 처리하고 수신할 수 있습니다. Python 또는 Athena Notebook API를 사용하여 Spark 애플리케이션을 개발할 수도 있습니다. Spark를 사용하여 Amazon EMR 클러스터를 설정하고, Amazon S3 또는 기타 소스에서 데이터를 로드하고, Spark 기능을 사용하여 대화형으로 데이터를 탐색하고 분석할 수 있습니다.

 

Amazon EMR은 데이터 탐색 태스크를 수행하는 데 도움이 되는 Spark 코드를 작성하고 실행할 수 있는 Jupyter Notebook 인터페이스를 제공합니다.

 

방금 Jupyter Notebook을 언급했으니 AWS에서의 데이터 검증 및 정리에 대해서도 이야기해 보겠습니다. 어떤 AWS 서비스를 사용할 수 있습니까? Lambda, Athena, QuickSight, Jupyter Notebook, Amazon SageMaker Data Wrangler 등이 있습니다.

데이터에서 얻은 인사이트의 정확성과 신뢰성을 보장하려면 데이터 분석 프로세스에서 데이터를 검증하고 정리하는 방법을 알아야 합니다. AWS는 데이터 검증 및 정리에 사용할 수 있는 서비스를 제공하는데 각각 다른 목적으로 사용됩니다.

 

데이터 검증 및 정리 태스크의 경우 데이터 분석 워크플로의 요구 사항을 충족하기 위해 여러 가지 서비스를 사용할 수 있습니다.

 

예를 들어 사용자 지정 데이터 검증 및 보강에는 Lambda, SQL 기반 데이터 검증에는 Athena, 데이터 시각화 및 탐색에는 QuickSight, 더 복잡한 데이터 정제 태스크에는 Jupyter Notebook, 기계 학습 모델을 위한 데이터 준비에는 SageMaker Data Wrangler를 사용할 수 있습니다.

 

다음에는 영역 3의 세 번째 작업 설명을 시작하겠습니다.

## 3. 데이터 파이프라인 유지 관리 및 모니터링

영역 3의 세 번째 작업 설명인 데이터 파이프라인 유지 관리 및 모니터링을 시작하겠습니다. 모니터링은 데이터 파이프라인 수명 주기의 신뢰성, 가용성, 성능을 유지하는 데 중요합니다. 하지만 관찰 기능도 보장해야 합니다.

 

다음은 파이프라인의 데이터 측면에서 고려해야 할 몇 가지 질문입니다. 데이터를 쿼리할 때 입력과 출력이 올바른지 확인하려면 어떻게 해야 합니까? 쿼리가 테이블에 저장된 경우 스키마가 올바른지 어떻게 알 수 있습니까? 입력 데이터세트와 변환 데이터세트에 대해 데이터 품질 테스트를 실행해야 하는 이유는 무엇입니까? 변환 단계에서 데이터 품질 문제가 있는 경우 모든 문제에 플래그를 지정하고, 변경 사항을 롤백하고, 근본 원인을 조사할 수 있는 능력이 필요합니다.

 

데이터 파이프라인의 운영 부분에 대한 질문은 무엇입니까? 제가 가장 먼저 생각한 것은 시스템이 제대로 작동하도록 어떻게 보장할 것인가 하는 것입니다. 어떤 지표를 모니터링해야 합니까? 몇 가지 지표는 쿼리 대기열 길이, 쿼리 동시성, 메모리 사용량, 스토리지 사용률, 네트워크 지연 시간, 디스크 입력/출력입니다. 성능이 좋지 않은 쿼리가 있으면 해당 쿼리를 리팩터링하거나 튜닝할 수 있습니다. 쿼리의 성능이 양호하다면 데이터베이스를 튜닝해야 할 수 있습니다.

 

예를 들어 더 빠른 조회 성능을 위해 테이블을 클러스터링하거나 캐싱해야 합니다. 아니면 컴퓨팅 리소스를 확장해야 할 수도 있습니다. AWS는 모니터링하고, 보고하고, 문제 해결이 필요할 때 자동으로 조치를 취하는 데 사용할 수 있는 모니터링 및 관찰 기능 서비스와 도구를 제공합니다. 예를 들어 CloudWatch, AWS X-Ray, CloudWatch 로그 인사이트, Amazon GuardDuty, Amazon Inspector, AWS Security Hub 등이 있습니다.

 

애플리케이션을 관찰 가능하게 만들려면 계측을 통해 추적, 지표 및 로그를 공유하여 수집된 원시 데이터를 이해하고, 그로부터 인사이트를 얻고, 가시성을 추가해야 합니다.

 

먼저 모니터링하고 조치를 취하는 데 필요한 데이터를 확보하기 위해 데이터를 로깅하는 방법에 대해 이야기해 보겠습니다. AWS에서 애플리케이션을 모니터링하고 문제를 해결하기 위해 애플리케이션 데이터를 로깅하는 데 사용할 수 있는 AWS 서비스는 무엇입니까? CloudWatch가 AWS에서 실행하는 리소스 및 애플리케이션을 모니터링하는 데 어떻게 도움이 되는지 이미 살펴보았습니다. CloudWatch를 사용하여 지표를 수집하고 추적할 수 있습니다.

 

CloudWatch Logs는 로그 파일을 모니터링, 저장, 액세스하고 특정 임계값이 충족되면 알림을 받을 수 있는 기능을 제공합니다.

 

CloudWatch Events는 시스템 이벤트의 준실시간 스트림을 제공하며, 특정 이벤트를 감시하고 해당 이벤트가 발생할 때 다른 AWS 서비스에서 자동화된 작업을 시작하는 규칙을 작성하는 기능을 제공합니다.

 

EventBridge는 자체 애플리케이션, 통합된 서비스형 소프트웨어(SaaS) 애플리케이션 및 AWS 서비스의 데이터를 사용하여 애플리케이션을 서로 연결하는 데 도움이 됩니다. EventBridge는 이벤트 소스로부터 실시간 데이터 스트림을 제공하고 해당 데이터를 Lambda와 같은 대상으로 라우팅합니다. 데이터를 전송할 대상을 결정하는 라우팅 규칙을 설정하여 모든 데이터 소스에 실시간으로 대응하는 애플리케이션 아키텍처를 구축할 수 있습니다.

 

X-Ray는 엔드투엔드 추적 및 분석을 제공할 수 있습니다. 분산 아키텍처 및 마이크로서비스를 사용하는 대규모 애플리케이션이 있는 경우 X-Ray는 애플리케이션 구성 요소 간의 상호 작용을 시각화하고 이해하는 데 도움이 될 수 있으며 성능 최적화 및 문제 해결에 유용할 수 있습니다. Lambda는 Lambda 함수의 각 호출을 자동으로 기록합니다. 프로그래밍 언어를 사용하여 Lambda 함수에 사용자 지정 로그 문을 추가할 수도 있습니다.

애플리케이션이 Application Load Balancer 뒤에 배치되어 있는 경우 액세스 로그를 사용하여 요청 시간, 클라이언트 IP, 사용자 에이전트 등 각 요청에 대한 세부 정보를 캡처할 수 있습니다.

 

Amazon RDS 데이터베이스의 경우 오류 로그, 느린 쿼리 로그, 일반 로그 등 다양한 유형의 로그를 사용하여 데이터베이스 관련 활동 및 오류를 캡처하고 모니터링할 수 있습니다. 또한 AWS에서 감사 및 추적 기능을 보장하기 위해 인프라 및 서비스 전반에 걸쳐 활동과 이벤트를 캡처하고 분석하는 로깅 및 모니터링 솔루션을 배포할 수 있습니다.

 

추적 기능을 보장하기 위해 어떤 AWS 서비스를 사용할 수 있습니까? CloudTrail은 AWS 계정에서 수행된 모든 API 직접 호출을 기록합니다. 리소스 생성 또는 수정과 같은 이벤트에 대한 감사 추적을 제공하여 누가 무엇을 언제 했는지 이해하는 데 도움이 됩니다. CloudTrail 로그는 Amazon S3 또는 CloudWatch Logs로 전송될 수 있으며, 여기에서 알림을 설정하고 규정 준수 및 감사 목적으로 로그를 분석 및 보관할 수 있습니다. 그리고 추적 기능 및 감사를 위한 데이터의 정확성과 신뢰성을 보장하려면 데이터를 검증하고 정리하는 것이 중요합니다.

 

이에 대해서는 지난 강의에서 이야기했습니다. AWS는 Lambda, Athena, QuickSight, Jupyter Notebook, SageMaker Data Wrangler와 같은 데이터 검증 및 정리 서비스를 제공합니다. 여기서 잠시 멈추고 애플리케이션 데이터를 로깅하도록 CloudWatch Logs를 구성하는 방법과 자동화를 추가하는 방법에 대해 알아보겠습니다. 로그 그룹을 생성하면 로그 스트림의 컨테이너가 되며, 로그에 대한 액세스를 구성하고 제어하는 데 도움이 됩니다. CloudFormation을 사용하여 CloudWatch Logs의 구성을 자동화할 수도 있습니다.

 

CloudFormation은 로그 그룹, 로그 스트림, 로그 구성 등 코드형 인프라를 정의하여 로깅 설정을 관리하고 재현하는 데 도움이 됩니다. 또는 방금 설명한 것처럼 EventBridge를 사용합니다. 기본적으로 CloudWatch Logs의 로그 데이터는 무기한 보존됩니다. 로그 보존 정책을 구성하여 해당 데이터를 보존할 기간을 지정할 수 있습니다. 그러면 비용을 관리하고 데이터 보존 요구 사항을 준수하는 데 도움이 됩니다.

 

이제 추적 기능을 위한 AWS 서비스로 돌아가겠습니다. AWS Config는 AWS 리소스에 대한 구성 변경 사항을 기록하는 서비스로, 리소스의 과거 구성에 대한 상세 보기를 제공하며 규정 준수 감사 및 변경 관리에 사용할 수 있습니다. AWS 관리형 규칙은 AWS 리소스가 모범 사례를 준수하는지 평가하는 사전 정의된 AWS Config용 규칙으로 보안 및 규정 준수를 보장하는 데 도움이 됩니다.

AWS CloudFormation StackSets를 사용하면 여러 AWS 계정 및 리전에서 CloudTrail, CloudWatch 및 AWS Config 구성을 일관되게 배포하고 관리할 수 있습니다.

VPC 흐름 로그는 Amazon VPC의 네트워크 인터페이스에서 전송되고 수신되는 IP 트래픽에 관한 정보를 캡처합니다. 이는 보안 및 규정 준수 감사를 위해 네트워크 트래픽을 모니터링하고 문제를 해결하는 데 도움이 됩니다.

 

Security Hub는 AWS 환경 전반의 보안 경고 및 규정 준수 상태에 대한 보기를 제공하며 GuardDuty, Amazon Inspector, AWS Config와 같은 여러 AWS 서비스의 결과를 단일 대시보드로 통합하여 가시성을 높입니다.

 

자동화를 위해 특정 이벤트가 발생할 때 사용자 지정 작업을 수행하도록 이벤트에 의해 시작되는 Lambda 함수를 설정할 수 있습니다. 그러면 사용자 지정 로그 분석을 수행하고 로그 패턴에 따라 자동화된 작업을 수행할 수 있으므로 특정 이벤트에 대한 응답을 자동화하고 추적 기능을 향상시킬 수 있습니다.

 

애플리케이션 동작에 대한 인사이트를 확보하고, 문제를 식별하고, 시스템 성능을 향상하려면 로그 분석이 필수적이므로 여기서 잠시 멈추겠습니다. 특히 지난 강의에서 이러한 서비스를 여럿 소개했습니다. 또 반복하지는 않을 테니 필요하면 지난 강의를 다시 시청하십시오.

 

여기서 질문이 있습니다. 여러분은 Macie를 사용하여 S3 데이터의 보안을 관리하고 있습니다. 현재, Macie는 S3 버킷의 인벤토리를 제공하고 보안 및 액세스 제어를 위해 버킷을 자동으로 평가하고 모니터링합니다. Macie는 공개적으로 액세스할 수 있는 버킷과 같이 데이터 보안 또는 프라이버시와 관련된 잠재적인 문제를 감지하는 경우 사용자가 검토하고 필요에 따라 해결할 수 있도록 검색 결과를 생성합니다. Macie가 찾은 퍼블릭 S3 버킷에 대한 자동 수정을 어떻게 설계하시겠습니까?

 

CloudWatch Logs 또는 EventBridge로 전송된 로그에서 작업을 생성하시겠습니까 아니면 CloudTrail을 사용하시겠습니까? Macie는 CloudTrail과 통합되어 정책 및 민감한 데이터 검색 결과를 자동으로 EventBridge에 이벤트로 푸시합니다. 또한 게시 설정을 구성하면 Macie가 검색 결과를 Security Hub에 푸시할 수도 있습니다. 현재는 Macie가 CloudWatch Logs로 로그를 전송하지 않습니다. AWS 사용자 알림을 Macie와 통합하면 EventBridge 이벤트를 사용하여 자동으로 알림을 생성할 수도 있습니다. 이러한 서비스는 모두 보안 인시던트 탐지, 변경 사항 추적, 규정 준수 유지, 신속한 문제 해결에 도움이 됩니다.

 

성능 문제를 해결하는 데 도움이 되는 모범 사례 접근 방식을 살펴보겠습니다. CloudWatch와 같은 모니터링 도구를 사용하여 CPU 사용률, 메모리 사용량, 네트워크 트래픽, 디스크 입력/출력, 데이터베이스 성능과 같은 주요 성능 지표를 추적합니다. 기준 지표를 설정하고 이를 현재 성능과 비교하여 이상 상태를 식별합니다. 애플리케이션 문제 해결을 시작하기 전에 AWS Health Dashboard를 확인하여 애플리케이션이 사용하는 AWS 서비스에서 서비스 중단 또는 성능 저하가 발생하지 않는지 확인합니다.

 

애플리케이션 로그를 분석하여 오류 메시지, 예외 또는 경고를 식별합니다. CloudWatch Logs를 사용하여 애플리케이션 로그를 수집, 저장, 분석합니다. 성능 병목 현상을 나타낼 수 있는 패턴을 찾습니다. 애플리케이션이 Lambda, Amazon RDS, Amazon EC2와 같은 AWS 서비스를 사용하는 경우 서비스별 로그를 확인하여 리소스 사용량, 오류 또는 시간 초과와 관련된 잠재적인 문제를 찾습니다.

 

여기서 질문이 있습니다. 여러분은 피크 시간 동안 지연 시간이 증가한 애플리케이션 문제를 해결해야 합니다. 이 애플리케이션은 Kinesis Data Streams를 사용하여 데이터를 로깅합니다. 트래픽 증가를 처리하기 위해 40개의 샤드로 데이터 스트림을 프로비저닝하여 높은 데이터 처리량을 보장했습니다. 또한 스트림을 사용하고, 데이터를 분석하고, 결과를 Amazon DynamoDB 테이블에 저장하도록 Auto Scaling 그룹에서 호스팅되는 KCL 애플리케이션을 구성했습니다. 높은 지연 시간 문제를 해결하기 위한 접근 방식은 무엇입니까?

 

첫 번째 단계는 CloudWatch를 확인하여 피크 시간 동안 인스턴스의 평균 CPU 사용률을 파악하는 것입니다. Auto Scaling 그룹 내 인스턴스의 평균 CPU 사용률이 35% 미만인지 확인합니다. 또한 Kinesis Data Stream의 프로비저닝된 처리량 초과 예외 오류에 대한 로그가 없는지 확인합니다. 로그 유무로 데이터 스트림 및 인스턴스가 피크 시간 동안 로드를 처리할 수 있는지 확인합니다. 애플리케이션의 다른 구성 요소는 DynamoDB 테이블입니다.

 

저라면 프로비저닝된 쓰기 용량을 확인하고 해당 쓰기 처리량 용량을 늘리겠습니다. 애플리케이션의 다양한 구성 요소 간의 네트워크 트래픽 패턴을 모니터링하고 시각화하는 것도 중요합니다.

높은 네트워크 지연 시간 또는 패킷 손실로 인해 성능 문제가 발생할 수 있습니다.

 

VPC 흐름 로그 및 Network Load Balancer 액세스 로그는 네트워크 성능에 대한 인사이트를 얻는 데 도움이 될 수 있습니다.

자동 크기 조정을 사용하는 경우 애플리케이션의 수요 패턴에 따라 크기 조정 정책이 올바르게 구성되었는지 확인한 다음 필요에 따라 자동 크기 조정 파라미터를 조정합니다.

 

여기서 질문이 있습니다. 장기 실행 쿼리가 있는 Amazon Redshift 클러스터에서 쿼리를 최적화하고 지연 시간을 방지해야 합니다. 단기 실행 쿼리가 영향을 받지 않도록 하려면 어떻게 해야 합니까? 이전 강의에서 파라미터 그룹에 대해 이야기했습니다. Amazon Redshift에서는 클러스터와 연결된 파라미터 그룹을 생성하고 워크로드 관리 대기열에 대한 크기 조정을 구성할 수 있습니다. 이렇게 하면 더 빠르게 실행되는 쿼리가 대기열에서 장기 실행 쿼리에 막히는 일이 발생하지 않습니다. 또한 부하 테스트 및 스트레스 테스트를 수행하여 애플리케이션에서 과도한 트래픽과 높은 부하를 시뮬레이션하면서 성능 제한과 잠재적인 개선 영역을 식별할 수 있습니다. 성능 문제를 식별 또는 해결할 수 없는 경우 AWS Support를 통해 지원 및 지침을 받는 것을 고려해 보십시오.

 

관리형 AWS 서비스는 많은 오류 조건을 사용자가 해당 동작을 프로그래밍해야 하는 Amazon EMR 및 Kinesis Data Streams 같은 서비스와는 다른 방식으로 처리한다는 점을 기억해야 합니다. 예를 들어 Amazon EMR에서 호스팅되는 클러스터에 문제가 있는 경우 해당 문제로 인해 클러스터가 실패하거나 완료하는 데 예상보다 오래 걸릴 수 있습니다.

이유는 무엇입니까?

Amazon EMR에서 호스팅되는 클러스터는 여러 유형의 오픈 소스 소프트웨어, 사용자 지정 애플리케이션 코드, AWS 서비스로 구성된 복잡한 에코시스템에서 실행됩니다. 이러한 부분 중 하나라도 문제가 발생하면 클러스터가 실패하거나 완료하는 데 예상보다 오랜 시간이 걸릴 수 있습니다. 예를 들어 새 Hadoop 애플리케이션을 개발하는 경우 디버깅을 추가하고 대표적인 소규모 데이터 하위 집합을 처리하여 애플리케이션을 테스트하는 것이 모범 사례입니다. 애플리케이션을 단계적으로 실행하여 각 단계를 별도로 테스트해도 좋습니다.

 

Kinesis Data Streams의 경우 문제를 해결해야 하는 구성 요소는 생산자 및 소비자 두 가지가 있습니다. 생산자와 관련된 문제에는 연결할 수 없음 또는 스트림에 쓸 때의 느린 응답이 포함됩니다. 소비자와 관련된 문제에는 수신된 레코드와 관련된 문제 또는 소비자가 정보를 느리게 수신하는 문제가 포함될 수 있습니다.

Amazon Managed Service for Apache Flink의 경우 스트림에 액세스할 수 없음, 데이터 손실, 데이터 처리할 수 없음, 애플리케이션 오류 등 여러 가지 일반적인 문제가 발생할 수 있습니다. CloudWatch Logs를 쿼리하여 애플리케이션 관련 문제를 조사하십시오.

 

여기에서 잠시 멈추고 지식의 심도를 확인해 보겠습니다. Amazon Managed Service for Apache Flink를 사용하여 데이터 스트림에서 이상을 탐지하고 알림을 보낼 수도 있습니까? 데이터 파이프라인이 센서 데이터를 실시간으로 수집하여 데이터 센터의 온도 수준을 모니터링하도록 오케스트레이션되었다고 가정해 보겠습니다. 비정상적인 고온 판독값이 탐지되면 알림을 받기 위해 무엇을 사용할 수 있습니까? 이상 상태를 탐지하고 알림을 보내기 위해 Random_Cut_Forest 함수를 사용할 수 있습니다. 그런 다음 데이터 센터의 온도를 낮추기 위해 에어컨을 켜도록 Lambda 함수를 구성할 수도 있습니다.

 

Kinesis Data Firehose는 데이터를 제공하거나 처리하는 동안 오류가 발생하면 구성된 재시도 기간이 만료될 때까지 재시도합니다. 데이터가 성공적으로 전달되기 전에 재시도 기간이 만료되면 Kinesis Data Firehose는 구성된 S3 백업 버킷에 데이터를 백업합니다. 대상이 Amazon S3이고 전송이 실패하거나 백업 AmazonS3 버킷으로의 전송이 실패하는 경우 Kinesis Data Firehose는 보존 기간이 끝날 때까지 재시도합니다. 일반적인 오류에는 특정 서비스에 데이터를 전달할 수 없음 및 기타 스트림 오류가 있습니다.

 

다음에는 영역 3의 네 번째 작업 설명을 시작하겠습니다.

## 4. 데이터 품질 보장 

네 번째 작업 설명을 시작하고 데이터 품질 보장에 대해 알아보겠습니다. 이번 작업 설명에서는 신뢰를 이야기하고 싶습니다. 여러분의 데이터를 사용하는 사람들은 여러분이 제공하는 데이터를 신뢰해야 합니다. 그러므로 사용 사례 및 사용자 이해, 생성될 데이터 이해, 해당 데이터를 제공하는 방법 이해 등의 기본 사항을 다시 생각해야 합니다.

 

이해 관계자와 최종 사용자가 데이터에 대한 신뢰를 잃으면 최종 목표를 달성하지 못할 가능성이 높습니다. 그러므로 전체 데이터 수명 주기에서 데이터 품질 및 관측 기능 프로세스 보장이 왜 중요한지 다시 생각하게 됩니다.

 

AWS에서는 고객에서부터 거꾸로 일합니다. 데이터 엔지니어로서 여러분도 그렇게 해야 합니다.

 

다음은 새로운 데이터 수명 주기를 시작할 때 고려해야 할 몇 가지 질문입니다. 이 데이터는 누가 사용합니까? 이 데이터를 어떻게 사용합니까? 그리고 필요한 데이터를 제공하기 위해 어떻게 협업합니까?

 

저는 데이터 엔지니어들이 좋지 않은 데이터를 게시하는 것보다는 데이터를 게시하지 않는 것이 낫다고 얘기하는 것을 들었습니다. 또한 이상 또는 오류가 있는 경우 파이프라인을 중지하고 문제를 해결하도록 파이프라인 전반에 걸쳐 자동화된 검사를 수행하는 것에 대해 이미 언급했습니다.

 

파이프라인에서 데이터 품질 검사를 수행하고 있는데 기본 키가 되어야 하는 열에 중복 값이 있다는 것을 발견했다고 가정해 보겠습니다. 어떻게 해야 합니까? 알림이 전송된 것은 확인할 수 있습니다. 그리고 문제에 따라 전체 프로세스를 중지할 필요가 없을 수도 있습니다.

 

데이터 품질 검사는 파이프라인에 중요합니다. 검사를 어디에 추가합니까? 데이터 품질 검사를 변환 이전, 변환 도중, 변환 이후에 추가하는 것이 모범 사례이지만 이를 위해서는 데이터 수명 주기 및 파이프라인을 설계하고 구현해야 합니다. 검사가 많을수록 항상 더 좋은 것도 아닙니다. 검사가 너무 많으면 오탐의 늪에 빠질 수도 있습니다. 검사가 너무 적으면 데이터 품질에 심각한 문제가 생길 수 있습니다. 어떻게 이 문제를 해결할 수 있을까요?

데이터 프로파일링은 데이터 관리, 운영 및 준비의 일부로, 데이터를 분석하고 조사하여 구조, 콘텐츠, 품질에 대한 인사이트를 얻는 프로세스입니다. 여기에는 데이터의 특성, 구조, 패턴 및 통계를 이해하기 위해 데이터를 체계적으로 검사하는 과정이 포함됩니다.

 

예를 들어, 데이터 프로파일링은 누락된 값 또는 Null 값의 존재뿐 아니라 다양한 필드에 걸친 데이터 분포도 파악합니다. 또한 데이터 이상, 불일치, 잠재적인 데이터 품질 문제를 식별하여 데이터의 품질을 평가하고 데이터의 카디널리티를 평가합니다. 데이터 패턴을 검사하고 필드 또는 엔터티 간의 관계를 식별합니다. 이를 통해 공통 값, 빈도 분포, 데이터 요소 간의 상관 관계를 확인할 수 있습니다. 그리고 데이터를 요약하여 최소값, 최대값, 평균값, 중앙값, 표준 편차 등을 제공합니다.

 

데이터 프로파일링은 데이터 품질을 개선하고, 데이터를 이해하고, 거버넌스 및 규정 준수를 지원하는 데 도움이 됩니다. 데이터 분산 및 패턴을 이해하면 쿼리 성능을 향상하고 데이터 스토리지를 최적화할 수 있습니다. 그렇다면 AWS에서 데이터 프로파일링을 수행하는 데 사용할 수 있는 AWS 서비스 및 도구는 무엇입니까? 이러한 서비스 및 도구는 이미 언급했지만 이 작업 설명에서 다시 이야기해 보겠습니다.

 

AWS Glue DataBrew를 데이터 프로파일링 태스크에 사용하면 복잡한 코드 데이터를 작성하지 않고도 데이터를 탐색하고 프로파일링할 수 있습니다. DataBrew는 사용자가 데이터를 더 잘 이해할 수 있도록 데이터 통계, 빈도 분포 및 데이터 품질 인사이트를 자동으로 생성합니다.

 

Athena는 통계 요약을 생성하고 데이터 패턴을 분석하는 SQL 쿼리를 실행하여 기본 데이터 프로파일링을 수행하는 데에도 사용할 수 있습니다.

 

Amazon Redshift SQL 기능을 사용하면 데이터 분산 계산, 데이터 이상 식별, 데이터 품질 지표 생성 등의 데이터 프로파일링 태스크를 수행할 수 있습니다.

 

Amazon EMR에서 Spark를 사용하면 사용자 지정 데이터 프로파일링 스크립트를 개발하여 데이터를 분석하고, 통계를 계산하고, 데이터 요약을 생성할 수 있습니다.

 

AWS Marketplace에는 AWS 환경과 통합되어 포괄적인 데이터 프로파일링을 수행할 수 있는 서드 파티 데이터 프로파일링 및 데이터 품질 도구도 있습니다.

 

이제 데이터 프로파일링을 완료했지만, 데이터 관리 프로세스에서 데이터의 품질 및 신뢰성을 보장하기 위한 데이터 검증은 어떻게 수행합니까? 데이터의 완전성, 일관성, 정확성, 무결성을 확인하고 검증해야 하기 때문입니다.

 

첫째, 데이터가 완전한지 확인합니다. 예를 들어 데이터세트에 누락된 값, Null 값 또는 빈 필드가 있습니까? 둘째, 데이터의 일관성을 확인합니다. 예를 들어 사전 정의된 규칙 또는 제약 조건에 따라 데이터를 검증하여 비일관성 또는 불일치를 식별합니다. 셋째, 데이터의 정확성을 확인합니다. 넷째, 데이터 무결성을 확인하여 생성부터 삭제까지 전체 수명 주기에서 데이터가 변경되지 않고 일관성을 유지하는지 확인합니다. 데이터 엔지니어가 데이터 검증을 수행하는 방법과 사용할 수 있는 기술 및 도구는 다양합니다.

복습이 필요할 경우를 대비해 플래시 카드를 추가했습니다.

 

데이터를 처리하는 동안 데이터 품질 검사를 실행해야 한다면 어떻게 합니까? 한 가지 예는 빈 필드를 확인하고 모든 필수 필드에 Null이 아닌 값이 있는지 확인하는 것입니다. AWS Glue DataBrew 레시피를 사용하여 빈 필드, 데이터 형식, 고유성 등에 대한 규칙 및 유효성 검사를 정의할 수 있습니다.

 

방금 Amazon EMR에서 Spark를 사용하여 데이터 처리 워크플로의 일부로 사용자 지정 데이터 품질 검사를 구축할 수 있다고 했습니다. 복잡한 데이터 품질 검사의 경우 Lambda 함수를 사용하여 품질 검사를 실행하고 문제 해결을 시작하고 Step Functions를 사용하여 검증 결과에 따라 처리 태스크를 오케스트레이션할 수 있습니다. SageMaker Data Wrangler는 필터 및 변환을 사용하여 데이터 품질 검사를 적용해 누락된 값, 중복 항목 등을 식별하고 해결할 수 있는 기능을 제공합니다.

 

여기서 질문이 있습니다. 여러분은 Amazon S3를 사용하여 데이터를 CSV 형식으로 저장하고 있습니다. AWS Glue 크롤러가 데이터 카탈로그를 채우고 테이블과 스키마를 생성하는 데 사용됩니다. AWS Glue 작업을 시작하여 테이블의 데이터를 처리하고 해당 데이터를 Amazon Redshift 테이블에 기록합니다. 작업을 몇 번 실행한 후 Amazon Redshift 테이블에 중복 레코드가 있음을 알게 되었습니다. 작업을 다시 실행할 때 더 이상 중복이 없도록 하려면 어떻게 합니까?

 

작업 설명 1.2에서 테스트 또는 스테이징 환경 생성에 대해 설명했습니다. 새 데이터를 유지하기 전에 AWS Glue 작업에서 스테이징 테이블을 설정하여 Amazon Redshift 테이블의 기존 행을 바꿀 수 있습니다.

 

더 자세히 살펴보면서 지식의 심도를 테스트해 보겠습니다. 병합 작업을 수행할 수 있습니까? 예. 데이터를 스테이징 테이블에 로드한 다음 UPDATE 문 및 INSERT 문을 위해 스테이징 테이블을 대상 테이블과 조인합니다. 또는 Amazon Redshift에서 AWS Glue 작업을 사용하여 UPSERT 또는 병합을 구현할 수 있습니다.

 

먼저 데이터를 스테이징 테이블에 로드한 다음 UPSERT 작업을 사용하여 스테이징 테이블을 대상 테이블과 조인할 수 있습니다. UPSERT 기능은 삽입되는 행이 이미 테이블에 있는 경우 데이터를 삽입하거나 업데이트합니다. 이 작업을 사용하면 Amazon Redshift 테이블에 더 이상 중복 레코드가 없습니다.

 

이 작업 설명에서는 데이터 샘플링도 다룹니다. 데이터 샘플링은 분석이 인사이트를 제공할 수 있도록 더 큰 데이터세트에서 데이터 하위 집합 또는 샘플을 선택하는 기술로, 처리에 필요한 컴퓨팅 리소스 및 시간을 줄여줍니다. 다양한 데이터 샘플링 기술이 있으며 각각 장점 및 사용 사례가 있습니다.

 

이번에도 이에 대한 플래시 카드를 추가하겠습니다. 그런데 데이터 샘플링에 도움을 주기 위해 사용할 수 있는 AWS 서비스에는 무엇이 있습니까? Amazon S3 Select는 SQL 유사 쿼리를 사용하여 Amazon S3에 저장된 객체에서 직접 데이터의 특정 하위 집합을 검색하는 기능을 제공합니다.

 

Athena와 Amazon Redshift를 사용하면 SQL 쿼리에서 LIMIT 절을 지정하여 제한된 수의 행을 검색하는 방법으로 데이터를 샘플링할 수 있습니다.

 

Amazon EMR과 Spark를 사용하면 사용자 지정 Spark 스크립트를 작성하여 데이터에 대해 체계적 샘플링, 무작위 샘플링 또는 층화 샘플링을 수행할 수 있습니다.

 

SageMaker Data Wrangler는 모델 훈련 및 테스트를 위한 균형 잡힌 데이터세트를 생성하는 데이터 샘플링을 지원합니다.

AWS Glue DataBrew의 경우 DataBrew 변환을 사용하여 데이터의 하위 집합을 필터링하고 선택할 수 있습니다.

 

이 작업 설명에서는 일부 파티션 또는 키에 다른 파티션 또는 키보다 많은 데이터가 있을 때 데이터 편향 메커니즘을 구현하는 방법도 알아야 합니다. 데이터 편향은 파티션 또는 노드 간의 데이터 분산 불균형으로, 이로 인해 성능 병목 현상이 발생하고 태스크 처리 시간이 증가합니다. 동적 로드 밸런싱, 데이터 복제 또는 기타 전략을 사용하여 편향을 해결하고 워크로드를 재분산할 수 있습니다.

 

AWS에서 실시간으로 또는 데이터를 처리하는 동안 데이터 편향을 감지하는 메커니즘을 어떻게 구현합니까? 실시간으로 또는 데이터 처리 중에 데이터 편향을 감지하려면 데이터 분산 불균형을 식별하고 해결하기 위한 모니터링, 경고 및 프로파일링 메커니즘을 구현해야 합니다. 지난 강의에서 이미 모니터링 및 관찰 기능을 다루었습니다. CloudWatch에서는 사용자 지정 지표를 생성하여 데이터 처리 작업 또는 애플리케이션을 모니터링하고 파티션 또는 키당 레코드 수, 데이터 분산, 처리 시간과 같은 지표를 수집하고 추적할 수 있습니다.

 

그런 다음 정의된 지표를 기반으로 경보를 구성하면 데이터 편향이 사전 정의된 임계값을 초과할 때 실시간 알림을 받을 수 있습니다. AWS Glue ETL 작업의 경우 CloudWatch 지표와 AWS Glue 작업 북마크를 사용하여 작업 진행 상황과 데이터 처리 상태를 모니터링합니다. AWS Glue 작업 북마크를 사용하면 Glue는 마지막으로 처리된 레코드에서 다시 시작하여 후속 실행에서 데이터 편향 문제를 더 잘 추적할 수 있습니다.

 

Amazon EMR을 Spark와 함께 사용하는 경우 자세한 Spark 애플리케이션 로깅 및 모니터링을 선택할 수 있습니다. 그런 다음 CloudWatch Logs를 사용하여 Spark 애플리케이션 로그를 캡처하고 로그에서 장기 실행 태스크, 불균일한 태스크 분산 또는 편향된 데이터 파티션과 같은 데이터 편향 징후를 검토합니다. 데이터 프로파일링 및 품질 검사로 돌아가서, AWS Glue DataBrew 또는 사용자 지정 Spark 스크립트를 사용하면 데이터 분산을 분석하고, 불균형한 파티션을 확인하고, 편향된 키 또는 속성을 식별하는 논리를 포함할 수 있습니다.

 

실시간 데이터 처리를 위해서는 Kinesis 또는 Apache Kafka와 함께 Spark Streaming을 데이터 소스로 사용하는 것이 좋습니다. Spark Streaming 작업을 구현하면 수신 데이터를 지속적으로 처리하고 데이터 편향 발생을 실시간으로 모니터링할 수 있습니다.

 

복잡한 데이터 처리 워크플로의 경우 Lambda 함수 및 Step Functions를 사용하여 데이터 처리 태스크를 오케스트레이션하고 실시간 편향 감지를 수행할 수 있습니다. Lambda 함수는 데이터 스트림 또는 데이터세트의 편향을 분석할 수 있으며, Step Functions는 이러한 태스크를 조정하고 문제 해결 태스크를 시작할 수 있습니다. Spark 또는 Amazon EMR을 사용한 데이터 처리 작업에서는 관찰된 데이터 편향을 기반으로 동적 데이터 재파티셔닝 기술을 구현할 수 있습니다.

 

고급 사용 사례의 경우 Lambda, EventBridge, Amazon SNS와 같은 AWS 서비스를 사용하여 사용자 지정 모니터링 및 알림 솔루션을 개발할 수 있습니다. 실시간으로 또는 데이터 처리 중에 데이터 편향을 감지하면 AWS 환경에서 데이터 정확성을 보장하고 성능을 최적화하며 신뢰할 수 있는 데이터 처리를 유지하는 데 도움이 됩니다.

 

분산 처리 시스템의 노드 또는 태스크 전체에서 데이터가 균일하게 분할되도록 하는 방법에 대해서도 이야기해 보겠습니다. 파티션 키와 해시 함수를 사용하여 데이터를 균일하게 분산시킬 수 있습니다. 그리고 로드 밸런싱 메커니즘을 구현하여 각 노드의 워크로드를 모니터링하고 그에 따라 태스크를 재분산합니다.

 

자주 액세스하는 데이터 또는 핫스팟의 경우 데이터를 여러 노드에 복제하여 동일한 데이터를 동시에 처리하는 여러 노드에서 데이터 편향의 영향을 줄이는 것이 좋습니다 .또한 map-reduce 작업과 같이 필요한 경우 셔플링 기술을 사용하여 데이터를 재분산합니다. 일반적인 데이터 파티셔닝 전략은 무엇입니까?

가장 먼저 떠오르는 생각은 날짜와 관련된 열로 파일을 분할하는 것입니다. 예를 들어 Sales 테이블이 있는 경우 판매 거래의 연도, 월, 일을 반영하는 YEAR 열, MONTH 열, DAY 열을 설정할 수 있습니다. S3 버킷을 사용하는 경우 각 특정 날짜와 관련된 모든 날짜 데이터는 동일한 S3 접두사 경로에 기록됩니다.

 

파티셔닝은 하나 이상의 분할된 열을 기반으로 쿼리 결과를 필터링할 때 성능상의 이점을 제공합니다. 예를 들어 특정 날짜에 대해 ‘WHERE’ 절을 사용하는 경우 쿼리는 단일 S3 접두사에 있는 파일만 읽으면 됩니다. 그리고 1개월 전체 또는 심지어 1년 동안의 데이터를 쿼리하는 경우 스캔해야 하는 파일 수가 줄어듭니다.

 

데이터 버킷팅은 파티셔닝과 관련된 개념입니다. 파티셔닝과 버킷팅은 쿼리를 실행할 때 Athena가 스캔해야 하는 데이터의 양을 줄이는 두 가지 방법입니다. 버킷팅은 데이터세트의 레코드를 버킷이라는 범주로 구성하는 방법입니다. 이러한 버킷 및 버킷팅의 의미는 S3 버킷과 다릅니다. 데이터 버킷팅에서는 동일한 속성 값을 갖는 레코드가 동일한 버킷에 들어갑니다.

 

각 버킷에 대략 동일한 양의 데이터가 포함되도록 레코드는 버킷 간에 최대한 균일하게 분배됩니다. 버킷은 파일이며 해시 함수에서 레코드가 들어가는 버킷을 결정합니다. 버킷팅된 데이터세트에는 파티션별로 버킷당 하나 이상의 파일이 있습니다. 파일이 속한 버킷은 파일 이름에 인코딩됩니다.

 

Athena 엔진 버전 2는 Hive 버킷 알고리즘을 사용하여 버킷팅된 데이터세트를 지원하고, Athena 엔진 버전 3은 Spark 버킷팅 알고리즘도 지원합니다. Hive 버킷팅이 기본값입니다. 데이터세트가 Spark 알고리즘을 사용하여 버킷팅된 경우 TBLPROPERTIES 절을 사용하여 bucketing_format 속성 값을 Spark로 설정합니다.

 

방금 순차 샘플링에 대해 이야기했습니다. 경우에 따라 전체 데이터세트를 처리하지 않고도 샘플링 또는 근사치 계산 기법을 사용하여 결과를 추정할 수 있습니다. 샘플링은 데이터 편향의 영향을 줄이고 데이터 처리의 효율성을 개선할 수 있습니다.

 

JOIN 작업의 경우 편향 조인 최적화 기술(Bloom 필터, 편향 조인 알고리즘 또는 기타 방법)을 사용하여 조인 키의 데이터 편향을 처리하고 편향된 데이터가 포함된 조인 작업을 최적화하는 것을 고려하십시오. AWS는 편향된 데이터 및 JOIN 작업을 처리하기 위한 내장 알고리즘과 최적화를 제공합니다.

 

예를 들어, 조인 키에서 데이터 편향을 자동으로 감지하고 처리하는 Amazon EMR의 편향 조인 최적화 기능을 들 수 있습니다. 이 기능은 편향된 키를 식별하고 데이터를 재분산하여 조인 중에 로드 밸런싱을 수행함으로써 작업 성능을 개선합니다.

 

Amazon Redshift와 같은 일부 AWS 서비스는 데이터 분포에 따라 동작을 조정할 수 있는 적응형 조인 알고리즘을 제공합니다. 이러한 알고리즘은 해시 조인과 같은 다양한 조인 전략 간에 동적으로 전환하고 데이터 편향 및 기타 요인에 따라 병합 조인을 정렬할 수 있습니다.

 

여기서 질문이 있습니다. 현재, 과거 트랜잭션 기록이 데이터 카탈로그와 통합된 S3 버킷에 저장되어 있습니다. 매일 밤 판매 실적 보고서가 Amazon Redshift 클러스터로 전송되어 저장됩니다. 판매 실적 보고서를 처리하려면 Amazon S3의 과거 데이터와 Amazon Redshift 클러스터의 판매 실적 보고서를 결합해야 합니다. 여러분의 솔루션은 무엇입니까?

Redshift Spectrum을 사용하여 Amazon S3에서 과거 트랜잭션 데이터를 위한 외부 테이블을 생성하고 Amazon Redshift SQL을 사용하여 테이블을 조인할 수 있습니다. 데이터를 처리하기 전에 전처리 및 데이터 변환을 수행하여 데이터 비닝, 특성 확장, 로그 변환 등의 기술을 사용하여 데이터 분산을 정규화하고 데이터 편향을 처리합니다.

또한 분산 처리 시스템에서 태스크의 세분성을 조정하는 것을 고려할 수도 있습니다. 태스크를 세분화하면 워크로드를 더욱 균일하게 분산하여 데이터 편향의 영향을 줄일 수 있습니다.

 

데이터 처리 시스템을 정기적으로 모니터링하고 프로파일링하면 데이터 편향 문제를 식별하고 이를 처리하기 위한 적절한 메커니즘을 구현하는 데 도움이 될 수 있습니다.

 

이번 강의는 AWS에서 데이터 품질 규칙을 정의하는 방법을 살펴보면서 마무리하겠습니다. 데이터 품질 규칙은 특정 품질 표준을 충족하지 않는 데이터를 식별하고 플래그를 지정하는 데 도움이 되도록 데이터세트의 데이터 품질을 평가하고 적용하는 데 사용하는 사전 정의된 기준 또는 조건입니다. 이를 통해 데이터 분석가와 데이터 엔지니어는 데이터 정리, 수정 또는 검증을 위해 적절한 조치를 취할 수 있습니다.

 

AWS Glue DataBrew 콘솔에서 DataBrew 프로젝트를 생성하는 경우 포함할 수 있는 데이터 품질 규칙의 예는 무엇입니까? 특정 열에서 누락된 값 또는 Null 값이 있는 레코드에 플래그를 지정하는 완전성 규칙이 있습니다. 형식 규칙은 열의 값이 특정 형식 또는 패턴과 일치하는지 확인합니다. 예를 들어 이메일 주소 형식, 전화번호 형식이 있습니다. 범위 규칙은 숫자 값이 지정된 범위 내에 속하는지 확인합니다. 또한 열에서 중복 레코드 또는 값을 식별하는 고유성 규칙도 있습니다.

 

데이터 품질 규칙이 정의된 데이터세트에 적용되면 DataBrew는 정의된 규칙에 따라 데이터를 분석하고 규정 미준수 레코드에 대한 품질 통계 및 플래그를 생성합니다. 그러면 데이터 품질 분석 결과를 검토하여 사전 정의된 품질 규칙을 충족하지 않는 레코드를 식별할 수 있습니다.

 

DataBrew는 데이터 품질 문제를 이해하는 데 도움이 되는 시각화 및 요약을 제공합니다. 그런 다음 식별된 데이터 품질 문제에 따라 조치를 취하고 데이터 정리, 변환 또는 검증을 수행하여 데이터 품질 문제를 해결합니다. 프로세스를 자동화하여 지속적인 데이터 품질 모니터링을 보장할 수 있습니다.

 

다음에는 다섯 번째 연습 문제를 시작하겠습니다.

# 영역 4. 데이터 보안 및 거버넌스

영역 4, 데이터 보안 및 거버넌스를 시작하겠습니다. 이 과정 시작 부분의 기본 사항 강의에서 데이터 엔지니어링은 원시 데이터를 수집하고 고품질의 일관된 데이터를 생성하는 프로세스 및 시스템을 개발, 구현, 유지 관리하는 것이라고 이야기했습니다.

 

또한 데이터 엔지니어는 보안, 데이터 관리, 오케스트레이션, 데이터 아키텍처, 소프트웨어 엔지니어링, 운영을 통합하여 수명 주기를 관리해야 합니다. 이 영역의 초점은 보안입니다. 암호화, 토큰화, 데이터 마스킹, 난독화 및 액세스 제어를 사용하여 전송 중 데이터와 저장 시 데이터를 보호해야 하기 때문입니다.

 

영역 4는 5개의 작업 설명으로 나뉘며, 다음 몇 번의 강의에서 이를 살펴보겠습니다. 작업 설명 4.1: 인증 메커니즘 적용, 작업 설명 4.2: 권한 부여 메커니즘 적용, 작업 설명 4.3: 데이터 암호화 및 마스킹 보장, 작업 설명 4.4: 감사 로그 준비, 작업 설명 4.5: 데이터 프라이버시 및 거버넌스 이해입니다.

 

첫 번째 작업 설명에서는 네트워크, 보안 그룹, 네트워크 액세스 제어 등 Amazon VPC를 보호하는 방법을 알아야 합니다. AWS 관리형 정책 및 고객 관리형 정책을 포함하여 암호, 인증서, 역할과 같은 인증 방법을 다룰 것입니다.

 

두 번째 작업 설명에서는 무단 액세스로부터 데이터를 보호하는 방법을 알아야 합니다. 역할 기반 액세스 제어, 액세스 패턴을 포함하여 역할, 정책, 태그, 속성과 같은 권한 부여 방법을 다를 것입니다.

 

세 번째 작업 설명에서는 암호화 및 다양한 AWS 서비스에서 데이터를 암호화하는 방법을 이해해야 합니다. 민감한 데이터 보호, 데이터 익명화, 마스킹, 키 솔팅을 다룰 것입니다.

 

네 번째 작업 설명에서는 애플리케이션 데이터를 로깅하는 방법에 대해 다시 설명합니다. 작업 설명 3.3에 관한 강의에서 로그에 대해 이야기한 것을 기억하십시오. 하지만 이 작업 설명에서는 AWS 서비스에 대한 액세스를 로깅하는 방법과 AWS 로그를 중앙 집중화하는 방법에 더 중점을 둡니다.

 

다섯 번째 작업 설명에서는 개인 식별 정보(PII)와 데이터 주권을 보호하는 방법을 다룹니다. 이 영역에서는 IAM 역할, 정책, 그룹, 네트워크 보안, 암호 정책, 암호화를 이해해야 합니다.

 

다음 몇 개의 강의에서 각 작업 설명을 개별적으로 다루면서 실제 직무에서 성공하기 위해 익혀야 하는 지식과 기술을 분석해 보겠습니다.

 

영역 4의 첫 번째 작업 설명을 살펴볼 다음 강의에서 시험 준비 상태 평가를 시작하겠습니다.

## 1. 인증 메커니즘 적용

영역 4의 첫 번째 작업 설명인 인증 메커니즘 적용을 시작하겠습니다. 모든 데이터 엔지니어는 자신이 구축하고 유지 관리하는 시스템의 보안을 책임집니다.

 

이 작업 설명으로 넘어가기 전에 기본 사항을 잠깐 알아보고 인증 및 권한 부여를 정의하겠습니다. 이 작업 설명은 인증에 관한 것이고, 다음 작업 설명은 권한 부여에 관한 것입니다. 인증은 사용자를 식별하고 본인이 맞는지 확인합니다. 권한 부여는 인증된 자격 증명이 시스템 내에서 액세스할 수 있는 항목을 결정합니다.

 

AWS에서는 AWS 서비스 및 리소스에 대한 액세스 권한을 부여하기 전에 인증 방법을 통해 사용자 또는 리소스의 신원을 확인합니다. AWS는 암호 기반 인증, 인증서 기반 인증, 역할 기반 인증 등 다양한 인증 방법을 지원합니다.

 

이 시험에서는 다양한 사용 사례 및 각 사례가 제공하는 다양한 보안 수준을 알아야 합니다.

그렇다면 AWS에서는 어떻게 인증합니까? IAM은 AWS 리소스에 대한 액세스를 안전하게 제어하는 데 도움이 되는 서비스입니다. IAM을 활용하여 리소스를 사용하도록 인증 및 권한 부여된 대상을 제어합니다. 여기에는 ID 페더레이션 지원이 포함됩니다. 데이터 엔지니어는 IAM을 사용하여 각 사용자 및 사용되는 AWS 서비스에 대해 최소 권한 원칙을 따르는 역할을 설계할 수 있습니다.

AWS에는 인증 방법이 몇 가지 있습니다. 먼저, AWS 계정 루트 사용자로, IAM 사용자로 또는 IAM 역할을 수임하여 인증(AWS에 로그인)할 수 있습니다. 자격 증명 소스를 통해 제공된 자격 증명을 사용하여 페더레이션형 ID로 로그인할 수도 있습니다.

 

AWS IAM Identity Center를 사용하면 다중 계정 권한으로 직원 자격 증명에 대한 로그인 보안을 관리하여 직원 사용자에게 AWS 계정에 대한 액세스 권한을 할당하거나, 애플리케이션 할당으로 사용자에게 SAML 2.0 (Security Assertion Markup Language) 애플리케이션에 대한 SSO 액세스 권한을 할당하거나, 자격 증명 센터로서 자동으로 연결할 수 있습니다.

 

AWS에서 사용되는 다양한 자격 증명은 무엇입니까? IAM 사용자는 단일 개인 또는 애플리케이션에 대한 특정 권한을 가진 AWS 계정 내의 자격 증명입니다. IAM 그룹은 동일한 권한이 필요한 사용자의 그룹입니다. 그룹으로 로그인할 수는 없지만 그룹을 사용하여 한 번에 여러 사용자에 대한 권한을 지정할 수 있습니다.

 

사용자는 역할과 다릅니다. 사용자는 한 사람 또는 애플리케이션에게 고유하게 연결되지만 역할은 필요하면 누구나 수임할 수 있도록 설계되었습니다. 사용자는 영구 장기 자격 증명이 있지만 역할은 임시 자격 증명을 제공합니다. 또한 IAM 역할을 사용하여 다른 계정의 신뢰할 수 있는 보안 주체가 내 계정의 리소스에 액세스하도록 교차 계정 액세스 권한을 부여할 수도 있습니다.

 

일부 AWS 서비스는 다른 AWS 서비스의 기능을 사용합니다. 예를 들어, 한 서비스에서 호출을 생성하면 해당 서비스가 Amazon EC2에서 애플리케이션을 실행하거나 Amazon S3에 객체를 저장하는 것이 일반적입니다. 서비스는 호출 보안 주체의 권한, 서비스 역할 또는 서비스 연결 역할을 사용하여 이 작업을 수행할 수 있습니다.

 

이 시험에서는 페더레이션 사용자 액세스 권한, 교차 서비스 액세스 권한 등과 같은 임시 자격 증명으로 IAM 역할을 사용하는 사용 사례 및 시나리오를 이해해야 합니다. 보안 주체 권한, 서비스 역할, 서비스 연결 역할 간의 차이점도 알아야 합니다.

 

AWS에서는 인증서 기반 인증도 사용할 수 있습니다. AWS Certificate Manager(ACM)는 퍼블릭 및 프라이빗 SSL/TLS X.509 인증서 및 키의 생성, 저장, 갱신을 처리합니다. ACM을 사용하여 직접 발급하거나 서드 파티 인증서를 ACM 관리 시스템으로 가져오는 방식으로 통합 AWS 서비스의 인증서를 제공할 수 있습니다. AWS Private CA에서 서명한 ACM 인증서를 내부 퍼블릭 키 인프라의 어디에서나 사용할 수 있도록 내보낼 수도 있습니다.

 

리소스에 대한 액세스를 관리하기 위해 IAM 정책을 사용할 수 있습니다. 정책은 AWS의 리소스에 대해 수행할 수 있는 작업을 정의하는 문서입니다. 여러 유형의 정책을 사용할 수 있습니다. 여기에는 IAM 자격 증명 기반 정책, 신뢰 정책, 리소스 기반 정책이 포함됩니다.

 

IAM 자격 증명 기반 정책은 IAM 사용자, IAM 그룹 또는 IAM 역할에 연결되어야 합니다. 리소스 기반 정책은 S3 버킷, AWS KMS와 같은 AWS 리소스와 직접 연결됩니다. 실제로는 IAM 역할에 2개의 정책이 필요하다는 것을 이해하는 것이 중요합니다. 첫 번째 정책은 역할을 수임할 수 있는 사용자 또는 서비스를 정의하는 신뢰 정책입니다.

 

두 번째 정책은 역할을 수임하는 사용자 또는 서비스가 어떤 작업을 수행할 수 있는지 정의하는 자격 증명 기반 정책입니다. 또한 리소스에 대한 작업이 정책에서 구체적으로 허용되지 않은 경우 AWS의 기본 동작은 해당 작업을 거부하는 것임을 이해해야 합니다.

 

추가 리소스에 링크를 추가하겠습니다.

 

예를 들어 IAM 자격 증명에 대한 권한을 찾는 데 도움이 되는 IAM 자격 증명 기반 정책을 들 수 있습니다. IAM에서 자격 증명에 대한 권한을 설정할 때 AWS 관리형 정책, 고객 관리형 정책, 인라인 정책 중 무엇을 사용할지 결정해야 합니다.

 

 

AWS 관리형 정책은 사용자, 그룹 및 역할에 적절한 권한을 할당하는 데 도움이 되도록 AWS에서 생성하고 관리하는 정책입니다. AWS 관리형 정책에서 정의된 권한은 변경할 수 없습니다.

고객 관리형 정책은 고객이 생성하는 자격 증명 기반 정책으로 AWS 계정의 여러 사용자, 그룹 또는 역할에 연결할 수 있습니다. 인라인 정책은 단일 IAM 자격 증명 즉, 사용자, 그룹 또는 역할에 대해 생성된 정책입니다.

인라인 정책은 정책과 자격 증명 간에 엄격한 일대일 관계를 유지합니다. 즉, 자격 증명을 삭제하면 인라인도 삭제됩니다.

 

역할, 엔드포인트 및 서비스에 IAM 정책을 적용하는 방법에 대해서도 이야기해 보겠습니다. 예를 들어, S3 액세스 포인트는 리소스, 사용자 또는 기타 조건을 기준으로 액세스 포인트 사용을 제어하는 기능을 제공하는 IAM 리소스 기반 정책을 지원합니다. 애플리케이션 또는 사용자가 액세스 포인트를 통해 객체에 액세스할 수 있으려면 액세스 포인트와 기본 버킷 모두 요청을 허용해야 합니다. 따라서 액세스 포인트 정책에 부여된 권한은 기본 버킷도 동일한 액세스를 허용하는 경우에만 유효합니다. 이에 대한 예는 곧 나옵니다.

 

액세스 포인트를 통해 제출된 요청을 제어하기 위해 IAM 정책을 생성하는 방법을 살펴보겠습니다. AWS에서는 이 태스크를 수행할 때 두 가지 방법을 권장합니다.

 

첫째, 버킷에서 액세스 포인트로 액세스 제어를 할당할 수 있습니다. 둘째, 액세스 포인트 정책에 포함된 동일한 권한을 기본 버킷의 리소스 기반 정책에 추가할 수 있습니다. 이 버킷 정책 예제는 버킷 소유자의 계정이 소유한 모든 액세스 포인트에 대한 전체 액세스를 허용합니다. 이 버킷에 대한 모든 액세스는 해당 액세스 포인트에 연결된 정책에 의해 제어됩니다. 두 번째 예제에서 액세스 정책은 계정 123456789012의 IAM 사용자 Julie에게 계정 123456789012의 액세스 포인트 my-access-point를 통해 접두사 Julie/가 지정된 GET 및 PUT 객체에 대한 권한을 부여합니다.

 

이 액세스 포인트 정책이 Julie에게 효과적으로 액세스 권한을 부여하려면 본 버킷도 Julie에게 동일한 액세스 권한을 부여해야 합니다. 첫 번째 예제에 설명한 대로 버킷에서 액세스 포인트로 액세스 제어를 위임할 수 있습니다. 또는 다음 정책을 기본 버킷에 추가하여 Julie에게 필요한 권한을 부여할 수 있습니다.

 

AWS PrivateLink의 IAM 정책을 적용하는 방법에 대해서도 이야기해 보겠습니다. 여기서 질문이 있습니다. 이 정책은 어떤 권한을 부여합니까? 기본적으로 사용자에게는 엔드포인트 사용 권한이 없습니다. 이 예제와 같이 사용자에게 엔드포인트를 생성, 수정, 설명, 삭제할 수 있는 권한을 부여하는 자격 증명 기반 정책을 생성할 수 있습니다.

 

다른 질문도 살펴보겠습니다. 지정된 프라이빗 DNS 이름으로 VPC 엔드포인트를 생성할 수 있는 권한을 부여하려면 어떻게 합니까? ec2:VpceServicePrivateDnsName 조건 키를 사용하면 VPC 엔드포인트 서비스와 연결된 프라이빗 DNS 이름을 기반으로 어떤 VPC 엔드포인트 서비스를 수정 또는 생성할 수 있는지 제어할 수 있습니다. PrivateLink와 함께 사용할 수 있는 IAM 기능에 대해 더 자세한 내용이 필요한 경우 링크를 추가할 테니 참조하십시오.

 

개략적으로, IAM은 자격 증명 공급자(IDP) 역할을 하며 AWS 계정 내의 자격 증명을 관리합니다. AWS 계정에 로그인하도록 이러한 자격 증명을 인증한 다음, 연결된 정책에 따라 리소스에 액세스하거나 리소스에 대한 액세스를 거부할 수 있도록 해당 자격 증명에 권한을 부여합니다.

 

액세스를 위한 IAM 역할 설정에 대해 이야기해 보겠습니다. 여기서 질문이 있습니다. IAM 권한을 사용하여 API Gateway에 대한 액세스를 제어하고 있습니다. API Gateway에서 API를 생성, 배포, 관리하려면 API 개발자에게 API Gateway의 API 관리 구성 요소에서 지원하는 필수 작업을 수행할 수 있는 권한을 부여합니다. 배포된 API를 호출하거나 API 캐싱을 새로 고치려면 API 직접 호출자에게 API Gateway의 API 실행 구성 요소에서 지원하는 필수 IAM 작업을 수행할 수 있는 권한을 부여합니다. 이 두 프로세스의 액세스 제어에는 서로 다른 권한 모델이 포함됩니다. 어떻게 작동하는지 이해할 수 있습니까?

 

API 개발자에게 API Gateway에서 API를 생성 및 관리할 수 있도록 허용하려면 지정된 API 개발자에게 필요한 API 엔터티를 생성, 업데이트, 배포, 보기 또는 삭제할 수 있도록 허용하는 IAM 권한 정책을 생성해야 합니다. 사용자, 역할 또는 그룹에 권한 정책을 연결합니다. API 직접 호출자가 API를 호출하거나 해당 캐싱을 새로 고치도록 허용하려면 지정된 API 직접 호출자가 사용자 인증이 활성화된 API 메서드를 호출할 수 있도록 허용하는 IAM 정책을 생성해야 합니다. API 개발자는 메서드의 authorizationType 속성을 AWS_IAM으로 설정하여 호출자가 인증을 위해 사용자의 자격 증명을 제출하도록 요구합니다. 그런 다음 사용자, 그룹 또는 역할에 정책을 연결합니다.

 

API가 백엔드에서 Lambda와 통합되어 있다고 가정해 보겠습니다. API Gateway에도 통합 AWS 리소스에 액세스할 수 있는 권한이 있어야 합니다. 우리의 예제에서는 해당 권한이 API 직접 호출자를 대신하여 Lambda 함수를 시작하는 기능을 제공합니다. 이러한 권한을 부여하려면 API Gateway 유형에 대한 IAM 역할을 생성합니다. IAM에서 이 역할을 생성하면 결과 역할에는 다음과 같이 API Gateway를 해당 역할을 수임하도록 허용된 신뢰할 수 있는 엔터티로 선언하는 IAM 신뢰 정책이 포함됩니다.

 

API Gateway가 통합 AWS 서비스를 호출하려면 이 역할에도 통합 AWS 서비스를 호출하기 위한 적절한 IAM 권한 정책을 연결해야 합니다. 예를 들어 Lambda 함수를 호출하려면 IAM 역할에 다음 IAM 권한 정책을 포함해야 합니다. lambda:InvokeFunction. IAM을 CloudFormation과 함께 사용하여 사용자가 CloudFormation으로 수행할 수 있는 작업을 제어할 수 있습니다. 또한 각 사용자가 사용할 수 있는 AWS 서비스 및 리소스를 관리할 수 있습니다.

 

요약하자면 인터페이스 VPC 엔드포인트를 생성하여 VPC와 CloudFormation 사이에 프라이빗 연결을 설정할 수 있습니다. CloudFormation에 대한 액세스를 제어하는 엔드포인트 정책을 VPC 엔드포인트에 연결할 수 있습니다. 이 정책은 작업을 수행할 수 있는 보안 주체, 수행할 수 있는 작업, 작업을 수행할 수 있는 리소스를 지정할 수 있습니다.

 

다음은 CloudFormation에 대한 엔드포인트 정책의 예입니다. 이 정책은 엔드포인트에 연결되면 모든 리소스의 모든 보안 주체에게 나열된 CloudFormation 작업에 대한 액세스 권한을 부여합니다. 또한 이 예제는 모든 사용자에게 CloudFormation 서비스에서 VPC 엔드포인트를 통해 스택을 생성할 수 있는 권한을 거부하고 다른 모든 작업에 대한 전체 액세스를 허용합니다.

 

또한 이 시험에서는 인프라를 보호하는 방법과 Amazon VPC에서 데이터 파이프라인을 지원하는 리소스에 대한 네트워크 트래픽을 제어하는 방법을 이해해야 합니다. 보안 그룹 및 네트워크 액세스 제어 목록을 사용할 수 있습니다. 프라이빗 서브넷, 배스천 호스트 또는 NAT 게이트웨이, 라우팅 테이블, AWS 가상 프라이빗 네트워크(AWS VPN), AWS Direct Connect, VPC 흐름 로그, Security Hub, AWS Network Firewall을 사용할 수도 있습니다.

 

이번 강의를 마무리하면서 암호 관리를 위한 자격 증명 생성 및 회전에 대해 이야기해 보겠습니다. 비밀의 수명 주기는 생성, 저장, 사용, 삭제의 4단계로 구성됩니다. 비밀 관리 솔루션은 이러한 각 단계에서 비밀을 무단 액세스로부터 보호합니다. AWS Secrets Manager는 애플리케이션, 서비스, 리소스에 대한 액세스를 보호하는 데 도움이 되는 비밀 관리 서비스입니다.

 

Secrets Manager가 IAM과의 통합을 기본 제공하므로 리소스 기반 정책을 사용하여 IAM 보안 주체 또는 비밀 자체에 액세스 제어 정책을 연결할 수 있습니다. Secrets Manager는 AWS Key Management Service (AWS KMS)와도 통합됩니다. 비밀은 AWS 관리형 키 또는 고객 관리형 키를 사용하여 저장 시 암호화됩니다. 그리고 Secrets Manager는 안전한 암호 회전을 지원하며 Amazon RDS, Amazon Redshift 및 Amazon DocumentDB에 대한 자동 데이터베이스 자격 증명 회전을 예약할 수 있습니다. 또한 사용자 지정된 Lambda 함수를 사용하여 Secrets Manager 회전 기능을 API 키, OAuth 토큰과 같은 다른 비밀 유형으로 확장할 수 있습니다.

 

여기서 질문이 있습니다. 결과를 데이터 카탈로그와 Amazon S3에 저장하고 있습니다. 분석 및 데이터 시각화를 위해 Athena 및 QuickSight를 사용하고 있습니다. 데이터 카탈로그가 새 S3 버킷에 지표를 저장하는 새 데이터 프로파일러를 포함하도록 업데이트되었습니다. 그리고 새 S3 버킷을 쿼리하고 참조하기 위해 새 Athena 테이블이 생성되었습니다. QuickSight에서 새 데이터를 시각화하려고 하면 가져오기가 실패합니다. 먼저, 무엇이 문제입니까? 오류 메시지가 무엇입니까? 권한이 부족하다는 오류 메시지를 받았다고 가정해 보겠습니다.

 

데이터 프로파일러가 데이터 카탈로그에 추가되기 전과 Athena에서 새 테이블이 생성되기 전에는 분명히 아무 문제가 없었습니다. 하지만 이제 Athena를 QuickSight와 함께 사용하려고 하니 권한이 부족합니다. QuickSight를 Athena에서 사용하는 S3 버킷에 성공적으로 연결하려면 QuickSight에게 S3 계정에 액세스할 수 있는 권한이 부여되어야 합니다. 사용자에게 권한이 부여된 것만으로는 충분하지 않습니다. 여기서 필요한 것은 이 강의의 앞부분에서 언급한 교차 서비스 액세스입니다. QuickSight에게 별도로 권한이 부여되어야 합니다. 이 문제를 해결하려면 QuickSight 콘솔에서 새 S3 버킷에 대한 권한을 구성해야 합니다.

 

다음에는 영역 4의 두 번째 작업 설명을 시작하겠습니다.

## 2. 권한부여 메커니즘

영역 4의 두 번째 작업 설명인 권한 부여 메커니즘 적용을 시작하겠습니다. 지난 강의를 시작하며 언급했듯이, 모든 데이터 엔지니어는 자신이 구축하고 유지 관리하는 시스템의 보안을 책임집니다. 그리고 인증은 사용자를 식별하고 본인이 맞는지 확인합니다. 권한 부여는 인증된 자격 증명이 시스템 내에서 액세스할 수 있는 항목을 결정합니다.

 

목표는 인증된 보안 주체가 최소한의 권한으로 대상 리소스에 액세스하도록 하는 것입니다. AWS에서는 보안 정책을 프로그래밍 방식으로 구축하고 파이프라인에서 코드로 관리할 수 있습니다. 그런 다음 소프트웨어 개발에서와 마찬가지로 파이프라인에서 애플리케이션 코드와 함께 정책 자동화를 테스트하여 정책이 예상대로 작동하는지 확인하고 검증할 수도 있습니다.

 

예를 들어 IAM Access Analyzer를 사용하면 정책에 부여된 권한을 지속적으로 평가하여 고객의 AWS 계정 외부에서 액세스할 수 있는 리소스를 식별할 수 있습니다. 또한 AWS Organizations를 사용하여 여러 계정에 정책을 적용할 수 있습니다.

 

이 작업 설명은 권한 부여, 그리고 정책 기반 인증 메커니즘을 사용하여 AWS 서비스의 리소스에 대한 액세스를 제어하는 방법에 초점이 있습니다. 각 정책은 AWS 또는 고객이 생성하고 관리합니다. 역할 기반 액세스 제어(RBAC) 방법, 속성 기반 액세스 제어(ABAC) 방법 또는 둘 다를 사용하여 해당 자격 증명을 인증하는 방법에 관계없이 계정 내부 및 외부 모두에서 자격 증명 간에 정책을 공유하거나 재사용할 수 있습니다. 각 모델은 장단점이 있으며 어떤 모델을 사용할지는 특정 사용 사례에 따라 다릅니다.

 

RBAC는 역할에 따라 리소스에 대한 액세스를 결정합니다. 예를 들어 개발자 역할은 사용자에게 데이터 파이프라인 또는 애플리케이션 내에서 개발을 수행할 수 있는 권한을 부여합니다. 이 액세스 제어 모델은 권한이 역할과 부합하므로 관리하기가 더 쉽습니다. 하지만 여러 역할에서 권한이 필요한 사용자가 있거나, 역할을 정의하기 어려운 복잡한 비즈니스 로직이 있거나, 권한 부여가 동적 파라미터를 기반으로 하는 경우에는 어떻게 될까요? 속성을 기반으로 권한을 정의하고 IAM 사용자 또는 역할을 포함한 IAM 리소스는 물론 AWS 리소스, 환경 또는 심지어 애플리케이션 상태에도 태그를 연결하는 권한 부여 전략인 ABAC를 사용할 수 있습니다. IAM 보안 주체에 대한 ABAC 정책을 하나 또는 여러 개 생성할 수 있습니다.

 

기본적인 예를 살펴보겠습니다. access-project 태그 키를 사용하여 세 가지 역할을 생성할 수 있습니다. 첫 번째 역할의 태그 값을 Heart로 설정하고 두 번째 역할은 Star, 세 번째 역할은 Bolt로 설정합니다. 그런 다음 역할 및 리소스에 access-project와 동일한 값의 태그가 지정될 때 액세스 권한을 부여하는 단일 정책을 사용할 수 있습니다. ABAC 모델은 동적, 컨텍스트 기반 및 세분화된 권한 부여 결정을 추가하지만 처음에는 구현하기 어려울 수 있습니다. 규칙 및 정책뿐만 아니라 모든 관련 액세스에 대한 속성도 정의해야 합니다. 권한 부여 방법을 결정할 때 세분화 계층을 추가하기 위해 ABAC와 RBAC를 결합할 수 있습니다. 이 하이브리드 접근 방식은 사용자의 역할 및 할당된 권한을 추가 속성과 결합하여 액세스 결정을 내리는 방식으로 액세스 권한을 부여합니다.

 

계속해서 데이터베이스에서 데이터베이스 사용자, 그룹, 역할 액세스 및 권한을 제공하는 방법에 대해 이야기해 보겠습니다. 예를 들어 Amazon Redshift에서는 사용자를 어떻게 생성하고 관리합니까? Amazon Redshift SQL 명령 CREATE USER 및 ALTER USER를 사용하여 데이터베이스 사용자를 생성하고 관리할 수 있습니다. 또한 사용자 지정 Amazon Redshift JDBC 또는 ODBC 드라이버를 사용하여 SQL 클라이언트를 구성하고 데이터베이스 사용자의 임시 암호를 생성하는 프로세스를 관리하여 데이터베이스 사용자를 생성하고 관리할 수도 있습니다. Amazon Redshift 사용자는 데이터베이스 슈퍼 사용자만 생성 및 삭제할 수 있습니다.

 

사용자는 Amazon Redshift에 로그인할 때 인증됩니다. 데이터베이스, 테이블과 같은 데이터베이스 객체를 소유할 수 있습니다. 또한 사용자, 그룹, 스키마에 해당 객체에 대한 권한을 부여하여 누가 어떤 객체에 액세스할 수 있는지 제어할 수 있습니다. CREATE DATABASE 권한이 있는 사용자는 데이터베이스를 생성하고 해당 데이터베이스에 권한을 부여할 수 있습니다.

 

슈퍼 사용자는 모든 데이터베이스에 대한 데이터베이스 소유권 권한을 갖습니다. 또한 Amazon Redshift에서 역할 기반 액세스 제어(RBAC)를 사용하여 데이터베이스 권한을 관리하면 광범위한 수준에서 세분화된 수준까지 사용자가 수행할 수 있는 작업을 제어하여 민감한 데이터에 대한 액세스를 보호할 수 있습니다. 할당된 역할이 있는 사용자는 할당된 역할에서 권한이 부여된 태스크만 수행할 수 있습니다.

 

예를 들어 CREATE TABLE 및 DROP TABLE 권한이 있는 역할이 할당된 사용자에게는 해당 태스크를 수행할 수 있는 권한만 부여됩니다. 다양한 사용자가 업무에 필요한 데이터에 액세스할 수 있도록 다양한 수준의 보안 권한을 부여하여 사용자 액세스를 제어할 수 있습니다. 관련된 객체의 유형에 관계없이 역할 요구 사항을 기반으로 하는 모든 사용자에 대한 모범 사례인 최소 권한 원칙을 상기해 보십시오. 권한 부여 및 취소는 개별 데이터베이스 객체에 대한 권한을 업데이트할 필요 없이 역할 수준에서 처리됩니다.

 

이제 방향을 바꿔 Lake Formation을 통한 권한 관리에 대해 이야기해 보겠습니다. Lake Formation은 데이터 레이크의 데이터에 대한 액세스 제어를 관리할 수 있는 단일 위치를 제공합니다. 데이터베이스, 테이블, 열, 행, 셀 수준에서 데이터에 대한 액세스를 제한하는 보안 정책을 정의할 수 있습니다. 이러한 정책은 IAM 사용자 및 역할에 적용되고, 외부 자격 증명 공급자를 통해 페더레이션하는 경우에는 사용자 및 그룹에 적용됩니다. 세분화된 제어를 사용하여 Redshift Spectrum, Athena, AWS Glue ETL, Amazon EMR for Spark 내에서 Lake Formation으로 보호되는 데이터에 액세스할 수 있습니다.

 

Lake Formation 태그 기반 액세스 제어를 사용하면 LF 태그라는 사용자 지정 레이블을 생성하여 수백 또는 수천 개의 데이터베이스 권한을 관리할 수 있습니다. LF 태그를 정의하여 데이터베이스, 테이블 또는 열에 연결할 수 있습니다. 그런 다음 소비를 위한 분석, 기계 학습 및 ETL 서비스 전반에 걸쳐 제어된 액세스를 공유할 수 있습니다. LF 태그는 수천 개의 리소스에 대한 정책 정의를 몇 개의 논리적 태그로 대체하여 데이터 거버넌스를 확장할 수 있도록 합니다. 예를 들어, 데이터 또는 메타데이터를 Amazon AmazonS3 또는 데이터 카탈로그로 마이그레이션하지 않고도 Amazon Redshift와 같은 다양한 데이터 소스에 저장된 데이터세트에 대한 권한을 설정할 수 있습니다.

 

Lake Formation을 Amazon Redshift 데이터 공유와 통합하여 Amazon Redshift 데이터 공유의 데이터베이스, 테이블, 열 및 행 수준 액세스 권한을 중앙에서 관리하고 데이터 공유 내의 객체에 대한 사용자 액세스를 제한하는 경우입니다. 지난 강의에서는 Secrets Manager를 사용하는 방법을 다루었지만 AWS Systems Manager의 기능인 Parameter Store를 사용하여 구성 데이터 관리 및 비밀 관리를 저장할 수도 있습니다. 암호, 데이터베이스 문자열, 라이선스 코드와 같은 데이터를 파라미터 값으로 저장할 수 있습니다. 그러나 Parameter Store는 저장된 암호에 대한 자동 회전 서비스를 제공하지 않습니다. 대신, Parameter Store를 사용하면 Secrets Manager에 비밀을 저장한 다음 해당 비밀을 Parameter Store 파라미터로 참조할 수 있습니다.

 

Secrets Manager를 사용하여 Parameter Store를 구성하는 경우 비밀 ID Parameter Store의 이름 문자열 앞에 슬래시(/)가 필요합니다. 이 시험에서는 무단 액세스를 방지하는 방법과 영향 범위를 제한하는 방법을 이해해야 합니다. 다음 강의에서는 AWS 또는 다른 사람들이 콘텐츠를 이해할 수 없도록 만드는 암호화, 토큰화, 데이터 분해, 사이버 디셉션과 같은 다양한 기술에 대해 설명하겠습니다.

 

다음에는 영역 4의 세 번째 작업 설명을 시작하고 데이터 암호화 및 마스킹에 대해 자세히 알아보겠습니다.

## 3. 데이터 암호화 및 마스킹 보장

영역 4의 세 번째 작업 설명을 시작하겠습니다. 이 작업 설명은 데이터 암호화 및 마스킹에 초점이 있습니다. AWS는 확장 가능하고 효율적인 암호화 기능을 제공하면서 저장 시 데이터와 전송 중 데이터에 보안 계층을 추가할 수 있는 기능을 제공합니다.

 

모든 데이터가 합법적이고 공정하며 투명한 방식으로 처리되도록 설계상 기본 데이터 보호가 중요합니다. 모든 AWS 서비스는 저장 데이터와 전송 데이터를 암호화하는 기능을 제공합니다. AWS KMS는 데이터를 보호하는 데 사용되는 암호화 키를 생성 및 제어하는 관리형 서비스입니다.

 

AWS KMS는 데이터를 암호화하는 다른 AWS 서비스와 대부분 통합됩니다. 예를 들어 AWS KMS는 CloudTrail과 통합되어 감사, 규제 및 규정 준수를 위한 AWS KMS 키 사용을 로깅합니다. 또한 AWS KMS API를 사용하여 사용자 지정 키 스토어, 암호화 작업에서 AWS KMS 키 사용과 같은 AWS KMS 키 및 기능을 생성하고 관리할 수 있습니다. 여기서 질문이 있습니다. 암호화 키를 생성하고, 저장하고, 사용하는 AWS CloudHSM 디바이스를 직접 관리해야 하는 경우 어떤 AWS 서비스를 사용할 수 있습니까?

 

CloudHSM은 AWS에서 하드웨어 보안 모듈을 제공합니다. AWS는 하드웨어 프로비저닝, 소프트웨어 패치, 네트워크 라우팅, 키 스토어의 암호화된 백업 생성을 자동화합니다. CloudHSM 환경을 확장하고 HSM 내에서 암호화 계정 및 자격 증명을 관리하는 것은 고객의 책임입니다. AWS KMS와 마찬가지로 CloudHSM은 누구도 HSM 외부에서 일반 텍스트 키를 사용할 수 없도록 설계되었습니다.

 

AWS에서는 AWS 데이터 센터 간의 모든 네트워크 트래픽이 물리적 계층에서 투명하게 암호화됩니다. Amazon VPC 내부의 모든 트래픽 그리고 여러 리전에 걸쳐 피어링된 Amazon VPC 간의 모든 트래픽은 네트워크 계층에서 투명하게 암호화됩니다. 애플리케이션 계층에서는 TLS(Transport Layer Security) 같은 프로토콜을 사용할 수 있습니다. 모든 AWS 서비스 엔드포인트는 API 요청을 위한 보안 HTTPS 연결을 생성하도록 TLS를 지원합니다.

 

여기서 질문이 있습니다. AWS 내에서 TLS를 종단해야 하는 고객 관리형 인프라의 경우 어떤 옵션을 사용할 수 있습니까? AWS는 Network Load Balancer, Application Load Balancer, CloudFront, API Gateway와 같은 서비스를 제공합니다. TLS 연결을 구현하기 위해 이러한 엔드포인트 서비스는 각각 고객의 자체 디지털 인증서를 업로드하여 암호화 자격 증명을 엔드포인트에 바인딩하는 기능을 제공합니다.

 

이전 강의에서 ACM을 사용하여 디지털 인증서를 생성, 배포, 회전하는 프로세스를 다룬 적이 있습니다. AWS KMS, CloudHSM, ACM과 같은 서비스를 사용하면 고객은 저장 시 및 전송 중 데이터 암호화 전략을 구현하여 특정 분류의 모든 데이터가 동일한 보안 태세를 공유하도록 할 수 있습니다.

 

이 시험에서는 Amazon EBS, Amazon S3, Amazon RDS, Amazon Redshift, ElastiCache, Lambda, Amazon EMR, AWS Glue, SageMaker와 같은 AWS 서비스를 사용한 저장 시 데이터 암호화를 이해해야 합니다. 아직 언급하지 않은 한 가지는 Amazon SQS용 서버 측 암호화(SSE)를 사용하여 민감한 데이터를 전송하기 위한 암호화된 메시지 대기열도 있다는 것입니다.

 

다음은 이 시험에 필요한 암호화 기본 사항에 대한 이해를 테스트하기 위한 또 다른 질문입니다. 한 리전에서 Amazon EMR 클러스터를 생성하고 Amazon EBS 볼륨이 암호화되면 로컬 디스크도 암호화됩니까?

 

아니요, 그렇지 않습니다. 하지만 보안 구성에 로컬 디스크 암호화를 추가하도록 선택할 수 있습니다. 운영 체제와 Amazon EBS 볼륨 사이에는 3개의 수준이 있습니다. 각각 로컬 디스크, Amazon EBS에서 관리하는 Amazon EBS 볼륨, 가상 머신에 제공된 볼륨입니다.

 

데이터 자체는 네트워크를 통해 기본 Amazon EBS 볼륨으로 전송되기 전에 EC2 하이퍼바이저에 의해 암호화됩니다. 따라서 Amazon EBS 볼륨이 암호화되고, 가상 머신에 제공된 볼륨도 암호화됩니다. 그러나 운영 체제에 노출되는 내용은 암호화되지 않습니다. 엔드투엔드 암호화를 추가하지 않는 한 EC2 호스트는 운영 체제에 암호화된 볼륨을 제공하지 않는다는 점을 이해해야 합니다. 암호화 및 복호화는 EC2 호스트에서 실행됩니다. 이 질문에 대한 여러분의 생각을 듣고 싶습니다. 동의하십니까, 동의하지 않으십니까? 이 주제는 몇 달 전에 AWS 커뮤니티에서 격론이 있었습니다.

 

이 시험에서는 사용 사례 및 요구 사항에 따라 암호화 방법, 암호화 키 저장, 데이터에 사용되는 암호화 키 관리를 사용하는 방법 및 시기를 이해해야 합니다. 또한 클라이언트 측 암호화와 서버 측 암호화의 차이점도 알아야 합니다.

 

여기서 질문이 있습니다. Amazon EMR 클러스터의 모든 노드에 루트 디바이스 볼륨 암호화를 적용해야 하는 새로운 요구 사항이 주어졌습니다. 여러분의 솔루션은 무엇입니까? 사용자 지정 Amazon Machine Image(AMI)를 사용하여 Amazon EMR 클러스터를 생성할 수 있습니다. AWS KMS 키를 사용하여 AMI의 Amazon EBS 루트 볼륨을 암호화할 수도 있습니다. 또한 Amazon EMR 클러스터의 인스턴스에서 Amazon EBS 루트 볼륨 크기를 조정할 수 있습니다. 그런 다음, Amazon EMR용 Amazon Linux AMI의 Amazon EBS 루트 디바이스를 암호화하기 위해 암호화되지 않은 AMI의 스냅샷 이미지를 암호화된 대상으로 복사합니다. 스냅샷의 소스 AMI는 기본 Amazon Linux AMI일 수도 있고, 사용자 지정한 기본 Amazon Linux AMI에서 파생된 AMI로부터 스냅샷을 복사할 수도 있습니다. 그런 다음 CloudFormation 템플릿의 CustomAmiID 속성 아래에서 암호화된 루트 디바이스 볼륨이 있는 사용자 지정 AMI를 사용하는 Amazon EMR 클러스터를 정의합니다.

 

계속해서 데이터 익명화, 마스킹, 키 솔팅에 대해 이야기해 보겠습니다. 암호화와 토큰화는 차이가 있습니다. 암호화는 알고리즘을 사용하여 일반 텍스트를 암호 텍스트로 변환하는 프로세스입니다. 원본 일반 텍스트를 복호화하려면 알고리즘과 암호화 키가 필요합니다.

 

토큰화는 데이터 조각을 토큰이라고 하는 임의의 문자열로 변환하는 프로세스입니다. 원본 데이터와 관련하여 직접적인 의미가 있는 값은 없습니다. 토큰은 원본 데이터에 대한 참조 역할을 하지만 해당 데이터를 파생하는 데 사용할 수는 없습니다. 암호화와 달리 토큰화는 민감한 정보를 토큰으로 변환하기 위해 수학적 프로세스를 사용하지 않습니다. 대신, 토큰화는 민감한 값과 토큰 사이의 관계를 저장하는 종종 토큰 볼트라고 하는 데이터베이스를 사용합니다. 그런 다음 볼트에 저장된 실제 데이터는 암호화를 통해 보호됩니다. 토큰 값은 원본 데이터 대신 다양한 애플리케이션에서 사용될 수 있습니다. 토큰은 그 자체로는 의미가 없으며 토큰화 시스템을 사용하지 않으면 토큰이 표현하는 데이터에 다시 매핑될 수 없습니다.

 

예를 들어, 반복적인 신용 카드 결제를 처리하기 위해 토큰이 볼트에 제출됩니다. 권한 부여 프로세스에 사용할 원본 데이터를 검색하는 데 인덱스가 사용됩니다. 토큰은 다른 유형의 민감한 정보 또는 개인 식별 정보를 보호하는 데에도 사용될 수 있습니다.

 

기본 사항으로 돌아가서 데이터 익명화, 키 솔팅, 데이터 마스킹 간의 차이점을 이해해야 합니다.

 

데이터 익명화는 데이터세트에서 기밀 정보, 개인 정보 또는 민감 정보를 제거합니다.반면, 데이터 마스킹은 변경된 값으로 기밀 데이터를 모호하게 만듭니다.

 

또한 솔트가 무엇인지 이해하십시오. 솔트는 데이터, 암호 또는 패스프레이즈를 해시하는 단방향 함수에 대한 추가 입력으로 제공되는 임의 데이터입니다. 솔팅은 각 암호에 대해 새로운 솔트를 임의로 생성하는 방식으로 작동합니다. 솔트와 암호가 연결되어 암호화 해시 함수에 제공되고, 출력 해시 값이 솔트와 함께 데이터베이스에 저장됩니다. 솔트는 암호화할 필요가 없습니다. 솔트를 알아도 공격자에게 도움이 되지 않기 때문입니다.

 

AWS에서의 토큰화에 대해 이야기해 보겠습니다. 토큰 볼트를 Amazon VPC에서 구성하여 민감한 정보를 암호화 형식으로 저장하는 동시에 난독화된 데이터 전송을 위해 승인된 서비스에 토큰을 공유할 수 있습니다. 또한 AWS에는 데이터베이스 및 스토리지 서비스와 통합되는 토큰화 서비스를 전문으로 제공하는 여러 파트너가 있습니다.

 

여기서 질문이 있습니다. AWS에서 안전하고, 신뢰할 수 있고, 확장 가능하고, 비용 최적화된 토큰화 솔루션을 어떻게 설계할 수 있습니까? 고객 데이터베이스에 액세스해야 한다고 가정해 보겠습니다. 이 데이터베이스에는 고객 이름, 주민등록번호, 신용카드, 주문 내역, 기본 설정이 저장됩니다. 이 정보에는 민감한 데이터와 PII가 포함되어 있으므로 열 수준 액세스, 역할 기반 제어, 열 수준 암호화, 무단 액세스 방지 등의 방법을 적용해야 합니다. 토큰화를 사용하면 민감한 데이터를 고유한 임의의 토큰으로 바꿔 애플리케이션 데이터베이스에 저장할 수 있습니다. 이는 데이터 보호에 도움이 되는 동시에 액세스 관리의 복잡성 및 비용을 줄여줍니다.

 

API Gateway, Lambda, Amazon Cognito, DynamoDB, AWS KMS를 사용하는 서버리스 애플리케이션이 한 가지 설계가 될 수 있습니다. 이 설계의 경우 클라이언트는 Amazon Cognito로 인증하고 인증 토큰을 받습니다. 이 토큰은 Customer Order Lambda 함수에 대한 호출을 검증하는 데 사용됩니다. 이 함수는 토큰화 계층을 호출하여 요청에서 민감한 정보를 제공합니다. 이 계층에는 고유한 임의의 토큰을 생성하고 암호화된 텍스트를 암호 데이터베이스에 저장하는 논리가 포함되어 있습니다. Lambda는 AWS KMS를 호출하여 암호화 키를 얻습니다. 그런 다음 DynamoDB 클라이언트 측 암호화 라이브러리를 사용하여 원본 텍스트를 암호화하고 암호 텍스트를 암호 데이터베이스에 저장합니다. Lambda 함수는 토큰화 계층의 응답에서 생성된 토큰을 검색합니다. 이 토큰은 나중에 참조할 수 있도록 애플리케이션 데이터베이스에 저장됩니다. AWS KMS는 암호화 키 생성 및 관리를 관리합니다. 규제 및 규정 준수 요구 사항을 충족하는 데 도움이 되도록 모든 키 사용의 로그를 제공합니다.

 

보안 강화를 위해 연결된 보안 그룹이 프라이빗 IP에서 들어오는 HTTPS 트래픽만 허용하도록 두 번째 Lambda 함수를 구성할 수 있습니다. Lambda 함수는 게이트웨이 엔드포인트를 사용하여 DynamoDB에 연결하며, AWS KMS는 퍼블릭 인터넷을 거치지 않고 인터페이스 엔드포인트를 사용합니다.

 

VPC 엔드포인트는 엔드포인트 정책을 사용하여 이 연결을 통해 AWS KMS 및 DynamoDB에 대해 허용된 작업만 적용할 수 있습니다. 암호화 키 관리를 추가로 제어할 수 있도록 AWS KMS 기본 키에는 리소스 기반 정책이 있습니다. 이 정책은 Lambda 계층에 암호화/복호화를 위한 데이터 키를 생성하고 기본 키에 대한 모든 관리 활동을 제한할 수 있는 권한을 부여합니다.

 

다른 질문도 살펴보겠습니다. 데이터 웨어하우스에서 민감한 데이터를 어떻게 보호합니까? Amazon Redshift에서 동적 데이터 마스킹(DDM)을 사용하여 데이터베이스에서 민감한 데이터를 변환하지 않고 Amazon Redshift가 쿼리 시 민감한 데이터를 사용자에게 표시하는 방법을 조작할 수 있습니다. 마스킹 표현식이 테이블에 연결되면 해당 열 하나 이상에 표현식이 적용됩니다. 특정 사용자에게만 적용되거나 RBAC를 사용하여 생성할 수 있는 사용자 정의 역할에만 적용되도록 마스킹 정책을 추가로 수정할 수 있습니다. 마스킹 정책을 생성할 때 조건부 열을 사용하여 셀 레벨에서 DDM을 적용할 수도 있습니다. 또한 테이블의 동일한 열에 다양한 난독화 수준의 여러 마스킹 정책을 적용하고 이를 다른 역할에 할당할 수 있습니다.

 

다음에는 영역 4의 네 번째 작업 설명을 시작하겠습니다.


## 4. 감사를 위한 로그 준비
영역 4의 네 번째 작업 설명인 감사를 위해 로그 준비를 시작하겠습니다. 작업 설명 2.1에서는 데이터 스토리지, 데이터 스토리지 수명 주기, 데이터 보존을 다루었습니다.

 

이전 강의에서는 모니터링하고 조치를 취하는 데 필요한 데이터를 확보하기 위해 데이터를 로깅하는 방법을 설명했습니다. AWS에서 애플리케이션을 모니터링하고 문제를 해결하기 위해 애플리케이션 데이터를 로깅하는 데 사용할 수 있는 AWS 서비스에 대해서도 알아보았습니다. 미국 의료 정보 보호법(HIPAA), 신용카드 업계(PCI) 등 일부 규정에서는 특정 기간 동안 데이터를 보관하도록 요구합니다. 또한 요청 시 데이터를 사용할 수 있어야 합니다. 따라서 데이터 검색 기능을 갖춘 저장 및 보관 데이터 프로세스가 필요합니다. 그러나 데이터를 특정 기간 동안만 보관해야 할 수도 있으므로 규정 준수 지침 내에서 해당 데이터를 삭제하는 기능도 필요합니다.

 

작업 설명 3.3에서 이미 애플리케이션 데이터를 로깅하도록 CloudWatch Logs를 구성하고 자동화를 추가하는 방법을 설명했습니다. 또 반복하지는 않을 테니 복습이 필요하면 해당 강의를 다시 시청하십시오. 로그 그룹을 생성하면 로그 스트림의 컨테이너가 되며, 로그에 대한 액세스를 구성하고 제어하는 데 도움이 됩니다. 기본적으로 CloudWatch Logs의 로그 데이터는 무기한 보존됩니다. 로그 보존 정책을 구성하여 로그 데이터를 보존할 기간을 지정할 수 있습니다.

 

또한 CloudTrail을 사용하여 API 직접 호출을 추적하는 방법에 대해서도 설명했습니다. CloudTrail 로그는 Amazon S3 또는 CloudWatch Logs로 전송되며, 여기에서 알림을 설정하고 규정 준수 및 감사 목적으로 로그를 분석 및 보관할 수 있습니다. 이 작업 설명에서는 AWS 계정 및 리전 전체의 CloudTrail 이벤트에 대한 중앙 집중식 로깅 쿼리에 CloudTrail Lake를 사용하는 방법을 살펴보겠습니다.

 

CloudTrail Lake 통합을 사용하여 AWS 외부의 사용자 활동 데이터를 로깅하고 저장할 수도 있습니다. 이벤트 데이터 스토어를 생성할 때 이벤트 데이터 스토어에 포함할 이벤트 유형을 선택합니다. CloudTrail 이벤트, AWS Config 구성 항목, AWS Audit Manager 증거 또는 AWS 외부의 이벤트를 포함하는 이벤트 데이터 스토어를 생성할 수 있습니다.

 

이벤트 스키마는 이벤트 범주마다 고유하므로 각 이벤트 데이터 스토어에는 AWS Config 구성 항목, CloudTrail 이벤트와 같은 특정 이벤트 범주만 포함될 수 있습니다. 그런 다음, 지원되는 SQL JOIN 키워드를 사용하여 여러 이벤트 데이터 스토어에 걸쳐 SQL 쿼리를 실행할 수 있습니다. CloudTrail은 추적을 위한 태그 기반 인증을 지원하지 않습니다. 그러나 태그 기반 인증을 사용하여 이벤트 데이터 스토어의 작업에 대한 액세스를 제어할 수 있습니다.

 

AWS CloudTrail Lake는 이전 1시간 및 보존 기간 동안 이벤트 데이터 스토어에 수집된 데이터의 양에 대한 정보를 볼 수 있는 CloudWatch 지표를 지원합니다.

 

여기서 질문이 있습니다. Amazon EMR에서 클러스터를 설계할 때 로깅, 로그 및 감사와 관련하여 고려해야 할 사항은 무엇입니까? 제가 생각하는 한 가지는 디버깅 지원이 얼마나 필요한가 또는 원하는가입니다. Amazon EMR은 로그 파일을 Amazon S3에 보관할 수 있습니다. 기본적으로 각 클러스터는 로그 파일을 프라이머리 노드에 기록합니다. /mnt/var/log/ 디렉터리에 저장됩니다. 또한 Amazon EMR은 CloudTrail과 통합됩니다. CloudTrail은 Amazon EMR에 대한 모든 API 호출을 이벤트로 캡처합니다. 캡처되는 호출에는 Amazon EMR 콘솔에서 수행한 호출과 Amazon EMR API 작업에 대한 코드 호출이 포함됩니다. 추적을 생성하면 S3 버킷에 대한 CloudTrail 이벤트 (Amazon EMR 이벤트 포함)의 지속적 전달을 시작할 수 있습니다. 추적을 구성하지 않은 경우에도 CloudTrail 콘솔의 이벤트 기록에서 최신 이벤트를 볼 수 있습니다.

 

로그를 분석하려면 특정 설계 및 요구 사항에 따라 사용할 AWS 서비스를 알아야 합니다. CloudWatch 로그 인사이트를 사용하여 CloudWatch Logs에서 로그 데이터를 검색 및 분석할 수 있습니다. 운영 문제에 대응하기 위해 쿼리를 수행할 수 있습니다. 문제가 발생하면 CloudWatch 로그 인사이트를 사용하여 잠재적인 원인을 식별하고 배포된 수정 사항이 유효한지 검사할 수 있습니다. CloudWatch 로그 인사이트는 Amazon Route 53, Lambda, CloudTrail, Amazon VPC 등의 AWS 서비스 로그와 로그 이벤트를 JSON으로 출력하는 모든 애플리케이션 또는 사용자 지정 로그에서 필드를 자동으로 검색합니다.

 

또한 Amazon Athena CloudWatch 커넥터를 사용하면 Athena가 CloudWatch와 통신할 수 있으므로 SQL로 로그 데이터를 쿼리할 수 있습니다. 이 커넥터는 로그 그룹을 스키마로 매핑하고 각 로그 스트림을 테이블로 매핑합니다. 또한 커넥터는 로그 그룹의 모든 로그 스트림을 포함하는 특수 all_log_streams 뷰를 매핑합니다. 이 뷰는 로그 스트림을 개별적으로 검색하는 대신 로그 그룹의 모든 로그를 한 번에 쿼리하는 데 도움이 됩니다.

 

또한 CloudWatch Logs 구독을 통해 수신된 데이터를 거의 실시간으로 OpenSearch Service 클러스터에 스트리밍하도록 CloudWatch Logs 로그 그룹을 구성할 수 있습니다. CloudWatch를 사용하면 모든 시스템, 애플리케이션 및 AWS 서비스 로그를 확장성이 뛰어난 단일 서비스로 통합할 수 있습니다. 데이터 엔지니어는 CloudWatch에서 실행하는 서비스에 대한 로그를 검색하고 개발 과정에서 디버그 로그를 유지할 수 있습니다. CloudWatch Events를 사용하여 특정 기간 동안 시작하려는 서비스를 예약할 수도 있습니다.

 

다음에는 영역 4의 마지막 작업 설명을 시작하겠습니다.



## 5. 데이터 프라이버시 및 거버넌스 이해
영역 4의 마지막 작업 설명인 데이터 프라이버시 및 거버넌스를 시작하겠습니다. 이 과정 전반에 걸쳐 데이터 보호의 중요성을 강조했습니다.

 

이 작업 설명에서는 개인 식별 정보(PII) 데이터 및 데이터 주권을 보호하는 방법을 자세히 다룹니다. 이 과정의 기본 사항 강의에서는 전체 데이터 수명 주기에서 어떻게 보안이 포함되어야 하는지 설명했습니다.

 

AWS의 전체 수명 주기 동안 민감한 데이터를 보호하도록 설계된 솔루션부터 시작해 보겠습니다. TLS(Transport Layer Security)를 사용하여 개별 통신 채널을 통과하는 전송 중 데이터를 보호하고, 볼륨 암호화, 객체 암호화 또는 데이터베이스 테이블 암호화를 사용하여 개별 스토리지 솔루션에서 저장 시 데이터를 보호할 수 있습니다.

 

그러나 민감한 워크로드가 있는 경우 전체 데이터 수명 주기를 통해 데이터가 이동하는 동안 데이터를 따라갈 수 있는 추가 보호가 필요할 수 있습니다. 필드 수준 암호화와 같은 세분화된 데이터 보호 기술을 사용하면 대규모 애플리케이션 페이로드에서 민감하지 않은 필드는 일반 텍스트로 남겨 두면서 민감한 데이터 필드를 보호할 수 있습니다.

 

필드 수준 암호화는 애플리케이션 페이로드의 구조를 유지하면서 민감한 데이터 필드를 개별적으로 보호하는 데 사용됩니다. 대체 방법은 전체 페이로드 암호화로, 전체 애플리케이션 페이로드가 바이너리 BLOB으로 암호화되어 전체가 복호화될 때까지 페이로드를 사용할 수 없게 됩니다. 민감한 데이터를 보호하여 전체 수명 주기에서 노출을 줄이는 것이 모범 사례입니다. 이는 수집된 데이터를 최대한 조기에 보호하고 승인된 사용자 및 애플리케이션만 반드시 필요할 때 데이터에 액세스할 수 있도록 보장하는 것을 의미합니다.

 

다음은 예입니다. CloudFront는 Lambda@Edge와 결합되면 AWS에서 수집 시 민감한 데이터를 보호하기 위해 AWS 네트워크의 엣지에 환경을 제공합니다. 다운스트림 시스템은 민감한 데이터에 액세스할 수 없으므로 데이터 노출이 줄어들어 감사 목적의 규정 준수 공간을 최소화하는 데 도움이 됩니다.

 

다음 그림은 애플리케이션에서 민감한 것으로 간주하는 JSON 구성의 PII 데이터 필드가 필드 수준 암호화 메커니즘을 사용하여 일반 텍스트에서 암호 텍스트로 변환될 수 있는 방식을 보여줍니다. 여기서 질문이 있습니다. 데이터를 마스킹, 암호화하고 Lake Formation을 통해 세분화된 액세스를 활성화하여 데이터를 처리함으로써 안전한 데이터 레이크를 생성하려면 어떻게 합니까?

 

민감한 데이터를 Amazon S3에 안전하게 저장하기 전에 식별, 마스킹, 암호화하기 위해 서버리스 파이프라인을 통해 수집되는 데이터가 있다고 가정해 보겠습니다. 데이터가 처리 및 저장되면 Lake Formation을 사용하여 세분화된 액세스 권한을 정의하고 적용하여 데이터 분석가와 데이터 과학자에게 안전한 액세스를 제공합니다. 사용 사례는 무엇입니까?

 

여러분의 솔루션이 MQTT(Message Queuing Telemetry Transport) 메시지를 AWS IoT Core 주제로 보내는 진단 장치용으로 설계되었을 수 있습니다. Kinesis Data Firehose를 사용하여 Amazon S3에서 원시 데이터를 전처리하고 준비할 수 있습니다. 그런 다음 ETL용 AWS Glue에서 Amazon Comprehend를 호출하여 민감한 정보를 식별하는 방법으로 데이터를 추가로 처리합니다. 그런 다음 Lake Formation을 사용하여 Athena를 사용해 데이터를 쿼리하는 비즈니스 분석가 및 데이터 과학자로 액세스를 제한하는 세분화된 권한을 정의합니다. 다음은 또 다른 예입니다.

 

애플리케이션이 예약된 일정에 따라 실행된다고 가정해 보겠습니다. 예를 들어, 기본적으로 6시간마다 하루 네 번 실행됩니다. 원시 데이터 S3 버킷에 추가된 데이터를 처리하기 위해 이 파이프라인의 어느 단계에서든 민감한 데이터 검색 스캔을 수행하도록 애플리케이션을 사용자 지정할 수 있습니다. 대부분의 고객은 ETL을 매일 실행하기 때문에 애플리케이션은 크롤러 작업이 실행되어 데이터를 카탈로그화하기 전 그리고 일반적인 검증 및 데이터 수정 또는 토큰화 프로세스가 완료된 후에 예약된 일정에 따라 중요한 데이터를 검색합니다.

 

하지만 이로 인해 파이프라인 실행 시간이 증가할 수 있습니다. 이 추가 검증으로 인해 파이프라인 실행에 최소한 5~10분이 추가될 것으로 예상할 수 있습니다. 검증 처리 시간은 객체 크기에 따라 선형적으로 증가하지만 작업당 존재하는 시작 시간은 일정합니다. 객체에서 민감한 데이터가 발견되면 승인 결정을 요청하는 이메일이 관리자에게 발송되며 관리자는 파이프라인의 다음 단계를 승인하거나 거부할 수 있습니다. 대부분의 경우 검토자는 민감한 데이터 정리 프로세스를 조정하여 민감한 데이터를 제거하고, 파일 진행을 거부하고, 파이프라인에서 파일을 다시 수집하도록 선택합니다.

 

따라서 IAM 관리형 정책을 사용하여 Lambda 함수가 애플리케이션의 일부인 AWS 리소스에 액세스할 수 있는 권한을 부여하도록 애플리케이션을 설계할 수 있습니다. S3 버킷은 처리의 다양한 단계에서 데이터를 저장할 수 있습니다. 데이터 파이프라인을 위해 객체를 업로드하기 위한 원시 데이터 버킷, 객체에서 민감한 데이터를 스캔하는 스캔 버킷, 민감한 데이터가 발견된 객체를 보관하는 수동 검토 버킷, 데이터 파이프라인의 다음 수집 단계를 시작하기 위한 스캔된 데이터 버킷이 있을 수 있습니다. Lambda 함수는 민감한 데이터 스캔 및 워크플로를 실행하는 논리를 시작합니다.

 

Step Functions는 비즈니스 로직에 따라 Lambda 함수를 오케스트레이션합니다. Macie 민감한 데이터 검색 작업은 스캔 단계 S3 버킷에서 민감한 데이터를 스캔할 수 있습니다. EventBridge 규칙은 반복 일정에 따라 Step Functions 워크플로 실행을 시작합니다. Amazon SNS는 파이프라인에서 검색된 민감한 데이터를 검토하라는 알림을 보냅니다. 그러면 2개의 리소스가 포함된 Amazon API Gateway REST API는 수동 워크플로의 일부로 민감한 데이터 검토자의 결정을 수신합니다.

 

이 계획에서 누락된 사항은 무엇입니까? 가장 먼저 떠오르는 생각은 백업 계획, 재해 복구 계획, 데이터 프라이버시 전략 계획입니다. 데이터 프라이버시 전략 계획이 필요한 이유는 무엇입니까?

 

앞서 언급한 데이터 주권과 관련하여 허용되지 않는 AWS 리전으로의 데이터 백업 또는 복제를 방지하는 것이 중요합니다. 다음은 재해 복구에도 사용할 수 있는 백업 전략이 추가된 시나리오입니다.

 

데이터 레이크 관리자, 데이터 엔지니어, 스토리지 관리자, 백업 관리자 등 여러 사용자가 있는 일반적인 데이터 레이크 환경을 가정해 보겠습니다. 데이터 레이크에 필요한 S3 리소스의 구성 및 관리는 스토리지 관리자 역할이 담당합니다.

 

스토리지 관리자는 S3 버전 관리, 동일 리전 복제, 교차 리전 복제, S3 객체 잠금 등 버킷에 저장된 데이터를 보호하는 데 도움이 되는 S3 기능을 구성할 수 있습니다.

 

백업 관리자 역할은 여러 AWS 서비스 간에 애플리케이션의 백업, 복원 및 관련 규정 준수를 담당합니다. AWS Backup을 사용하여 백업 관리자는 단일 AWS 계정에서 또는 AWS Organizations를 사용하여 여러 계정 간에 AWS 리소스 백업의 빈도 및 보존을 정의하는 백업 계획을 생성할 수 있습니다. 데이터는 Amazon S3의 객체 또는 버킷 수준에서 정의된 것과는 별도로 자체 액세스 정책, 암호화 키 및 잠금 정책을 사용하는 백업 볼트에 저장되고 구성됩니다.

 

IAM Identity Center를 사용하면 다양한 데이터 레이크 사용자의 자격 증명을 사용자 권한에 매핑할 수 있습니다.

 

복수의 소스와 다양한 개발 단계의 데이터가 포함된 데이터 레이크는 어떻습니까? 예를 들어 소스 시스템의 원시 데이터는 하나의 버킷에 수집되고, 변환을 거쳐 큐레이팅된 버킷에 저장되고, 다른 버킷에서 분석 처리의 결과로 집계된 데이터의 일부가 됩니다. 변환 및 집계된 데이터는 원시 데이터에서 재현될 수 있으므로 원시 데이터가 포함된 버킷을 백업하는 것이 최선의 백업 옵션이 될 수 있습니다.

 

그러나 PII 정보가 포함된 원시 데이터와 같은 데이터의 경우 대신 익명화된 데이터를 백업하는 것이 선호될 수 있습니다. 민감한 데이터를 보관하는 S3 버킷에는 필요한 데이터 보호 수준을 나타내는 키/값 페어 스키마로 태그를 지정할 수 있습니다. 예를 들어 태그 키 ‘DataProtectionLevel’과 값 ‘Critical’은 백업이 필요한 S3 버킷에만 적용됩니다. 그런 다음 이러한 태그 값을 사용하여 필요한 S3 버킷만 선택할 수 있습니다.

 

지정된 백업 빈도 및 보존 기간에 따라 작업 실행 시기와 데이터 저장 기간이 결정됩니다. Amazon S3용 AWS Backup을 사용하는 경우 백업 관리자는 연속 백업, 정기 백업 또는 두 백업 유형 모두를 생성하는 백업 계획을 정의할 수 있습니다. 연속 백업은 특정 시점 복원(PITR)에 유용합니다. 정기 백업 규칙은 PITR 규칙과 동일한 대상 볼트를 사용하여 규정 준수 요구 사항에 필요한 빈도 및 보존을 정의합니다. 데이터를 추가로 격리 및 보호하려면 대상에 복사 기능을 사용하여 다른 AWS 계정 또는 리전에 상주할 수 있는 대상 볼트에 스냅샷을 복사할 수 있습니다.

 

백업은 백업 볼트와 연결된 AWS KMS 키를 사용하여 암호화됩니다. AWS Backup Audit Manager를 사용하면 데이터 보호 정책 준수 여부를 감사하고 보고하여 비즈니스 및 규정 준수 요구 사항을 충족할 수 있습니다. 앞서 식별한 S3 리소스의 백업 활동을 모니터링하려면 AWS Backup Audit Manager에서 대상 제어 세트가 구성된 사용자 지정 프레임워크를 생성할 수 있습니다. 다른 시나리오를 살펴보고 AWS 서비스에서 데이터 공유 권한을 부여하는 방법에 대해 이야기해 보겠습니다. 예를 들어 Amazon Redshift의 데이터 공유입니다.

 

사용자를 삭제하려고 시도했는데 다음 오류가 발생했다고 가정해 보겠습니다. ERROR: user "username" can't be dropped because the user owns some object. ERROR: user "username" can't be dropped because the user has a privilege on some object. 이유는 무엇입니까?

 

데이터 공유 객체의 수명 주기는 생성부터 삭제까지 모든 Amazon Redshift 객체와 동일한 패턴을 따릅니다. 이 수명 주기의 일부로 Amazon Redshift는 데이터 공유 객체에 대한 사용자 삭제 확인을 적용합니다. 사용자가 데이터 공유 객체를 소유하거나 데이터 공유 객체에 대한 권한이 있는 경우 해당 사용자를 삭제할 수 없습니다.

 

다른 질문도 살펴보겠습니다. Amazon Redshift에 기밀 데이터가 포함된 새 테이블이 있으면 어떻게 됩니까? 이 테이블은 여러 팀에서 쿼리합니다. 권한을 보유한 사용자만 기밀 데이터가 포함된 열을 읽을 수 있도록 테이블을 보호하려면 어떻게 해야 합니까? 앞서 Lake Formation과 Amazon Redshift Spectrum이 하나의 솔루션이 될 수 있다고 했습니다. 그러나 Amazon Redshift에도 열 수준 액세스 제어 기능이 있으므로 열 수준 GRANT 및 REVOKE 문을 사용하여 보안 및 규정 준수 요구 사항을 충족할 수 있습니다. 이번 강의를 마무리하면서 마지막 시나리오 하나를 다루겠습니다.

 

방금 권한 관리에 대해 이야기했습니다. 하지만 계정에서 발생한 구성 변경 사항은 어떻게 관리합니까? AWS Config는 활성화하면 먼저 계정에 존재하는 지원되는 AWS 리소스를 검색하고 각 리소스의 구성 항목을 생성합니다. AWS Config는 계정의 각 리소스에 대해 Describe 또는 List API 직접 호출을 호출하여 리소스에 대한 모든 변경 사항을 추적합니다. 이 서비스는 동일한 API 직접 호출을 사용하여 모든 관련 리소스에 대한 구성 세부 정보를 캡처합니다.

 

예를 들어 VPC 보안 그룹에서 송신 규칙을 제거하면 AWS Config가 보안 그룹에 대한 Describe API 직접 호출을 간접적으로 호출하게 됩니다. 그런 다음, AWS Config는 보안 그룹과 연결된 모든 인스턴스에 대해 Describe API 직접 호출을 간접적으로 호출합니다. 보안 그룹(리소스) 및 각 인스턴스(관련 리소스)의 구성 업데이트는 구성 항목으로 기록되고 구성 스트림을 통해 S3 버킷에 전달됩니다. AWS Config 규칙을 사용하는 경우 각 규칙은 해당 규칙에 대한 평가 논리가 포함된 Lambda 함수와 연결됩니다. 구성 변경 사항 및 알림을 Amazon SNS 주제로 스트림하도록 AWS Config를 구성할 수 있습니다.

 

다음에는 일곱 번째 연습 문제를 시작하겠습니다.
