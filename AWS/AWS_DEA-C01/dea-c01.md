## 질문 #1

데이터 엔지니어가 Amazon S3 버킷에서 데이터를 읽기 위해 AWS Glue 작업을 구성하고 있습니다. 데이터 엔지니어는 필요한 AWS Glue 연결 세부 정보와 연관된 IAM 역할을 설정했습니다. 그러나 데이터 엔지니어가 AWS Glue 작업을 실행하려고 하면 데이터 엔지니어는 Amazon S3 VPC 게이트웨이 엔드포인트에 문제가 있다는 오류 메시지를 받습니다.
데이터 엔지니어는 오류를 해결하고 AWS Glue 작업을 S3 버킷에 연결해야 합니다.
어떤 솔루션이 이 요구 사항을 충족할까요?

A. Amazon S3 VPC 게이트웨이 엔드포인트에서 인바운드 트래픽을 허용하도록 AWS Glue 보안 그룹을 업데이트합니다.
B. AWS Glue 작업에 S3 버킷에 대한 액세스 권한을 명시적으로 부여하도록 S3 버킷 정책을 구성합니다.
C. AWS Glue 작업 코드를 검토하여 AWS Glue 연결 세부 정보에 정규화된 도메인 이름이 포함되어 있는지 확인하세요.
D. VPC의 경로 테이블에 Amazon S3 VPC 게이트웨이 엔드포인트에 대한 인바운드 및 아웃바운드 경로가 포함되어 있는지 확인합니다.
**가장 많이 투표된**

GPT 4 Turbo - 답변
VPC의 경로 테이블 설정을 확인하세요. AWS가 VPC 경로 테이블에 대해 기존의 "인바운드 및 아웃바운드 경로" 용어를 사용하지 않더라도 S3 게이트웨이 엔드포인트로의 경로가 올바르게 구성되었는지 확인하는 것이 중요합니다. 이는 서버리스로 작동하는 AWS Glue 서비스에 필요하며, 라우팅 문제가 S3 버킷과의 연결 문제의 원인이 되지 않도록 합니다. 이 단계는 AWS Glue가 S3와 효율적으로 통신할 수 있도록 하여 충돌을 해결하고 VPC 게이트웨이 엔드포인트와 관련된 오류를 제거합니다.


##  질문 #2

소매업체는 Amazon S3 버킷에 고객 데이터 허브를 두고 있습니다. 여러 국가의 직원이 데이터 허브를 사용하여 회사 전체 분석을 지원합니다. 거버넌스 팀은 회사의 데이터 분석가가 분석가와 같은 국가에 있는 고객에 대한 데이터에만 액세스할 수 있도록 해야 합니다.
어떤 솔루션이 최소한의 운영 노력으로 이러한 요구 사항을 충족할까요?

A. 각 국가의 고객 데이터에 대해 별도의 표를 만듭니다. 분석가가 서비스를 제공하는 국가에 따라 각 분석가에게 액세스를 제공합니다.
B. AWS Lake Formation에서 S3 버킷을 데이터 레이크 위치로 등록합니다. Lake Formation 행 수준 보안 기능을 사용하여 회사의 액세스 정책을 시행합니다.
**가장 많이 투표된**
C. 고객이 있는 국가에 가까운 AWS 지역으로 데이터를 이동합니다. 분석가가 서비스를 제공하는 국가에 따라 각 분석가에게 액세스를 제공합니다.
D. Amazon Redshift에 데이터를 로드합니다. 각 국가에 대한 뷰를 만듭니다. 각 국가에 대한 별도의 I**가장 많이 투표된**역할을 만들어 각 국가의 데이터에 대한 액세스를 제공합니다. 분석가에게 적절한 역할을 할당합니다.


챗봇-이미지
GPT 4 Turbo - 답변
요구 사항을 충족하는 가장 효율적인 접근 방식은 AWS Lake Formation을 사용하는 것입니다. 이 서비스는 데이터 레이크를 등록하고 세분화된 행 수준 데이터 액세스 제어를 용이하게 합니다. Lake Formation의 행 수준 보안 기능을 사용하면 각 지역에 대한 별도의 I**가장 많이 투표된**역할이나 데이터베이스를 관리하는 것과 같은 다른 옵션에 비해 최소한의 수동 작업으로 액세스 정책을 효과적으로 시행할 수 있습니다. 이 간소화된 구성은 운영 복잡성과 감독을 크게 줄여 설명된 시나리오에 이상적인 선택이 됩니다.


##  질문 #3

미디어 회사가 사용자 행동과 선호도에 따라 고객에게 미디어 콘텐츠를 추천하는 시스템을 개선하고자 합니다. 추천 시스템을 개선하기 위해 회사는 타사 데이터 세트의 통찰력을 회사의 기존 분석 플랫폼에 통합해야 합니다.
회사는 타사 데이터 세트를 통합하는 데 필요한 노력과 시간을 최소화하고자 합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할까요?

A. API 호출을 사용하여 AWS Data Exchange에서 타사 데이터 세트에 액세스하고 통합합니다.
**가장 많이 투표된**
B. API 호출을 사용하여 AWS DataSync에서 타사 데이터 세트에 액세스하고 통합합니다.
C. Amazon Kinesis Data Streams를 사용하여 AWS CodeCommit 리포지토리의 타사 데이터 세트에 액세스하고 통합합니다.
D. Amazon Kinesis Data Streams를 사용하면 Amazon Elastic Container Registry(Amazon ECR)의 타사 데이터 세트에 액세스하고 통합할 수 있습니다.


GPT 4 Turbo - 답변
AWS DataSync를 사용하여 타사 데이터세트를 통합하면 자동화된 데이터 전송을 허용하여 운영 오버헤드를 효과적으로 최소화할 수 있습니다. 일부에서는 데이터세트 수집을 위한 특정 설계로 인해 AWS Data Exchange를 제안할 수 있지만, DataSync의 핵심 속성은 수동 작업을 줄이고 데이터 통합 ​​프로세스를 간소화하여 타사 데이터세트를 효율적으로 통합해야 하는 회사의 요구 사항을 직접 해결하는 기능입니다. 이러한 자동화는 더 많은 수동 구성 및 감독이 필요할 수 있는 다른 방법에 비해 프로세스를 상당히 가속화할 수 있습니다.



##  질문 #4

금융 회사가 데이터 메시를 구현하려고 합니다. 데이터 메시는 중앙 집중식 데이터 거버넌스, 데이터 분석 및 데이터 액세스 제어를 지원해야 합니다. 이 회사는 데이터 카탈로그와 추출, 변환 및 로드(ETL) 작업에 AWS Glue를 사용하기로 결정했습니다.
어떤 AWS 서비스 조합이 데이터 메시를 구현할까요? (두 가지를 선택하세요.)

A. 데이터 저장에는 Amazon Aurora를 사용합니다. 데이터 분석에는 Amazon Redshift 프로비저닝 클러스터를 사용합니다.
B. 데이터 저장에는 Amazon S3를 사용합니다. 데이터 분석에는 Amazon Athena를 사용합니다.
C. 중앙 집중식 데이터 거버넌스 및 액세스 제어를 위해 AWS Glue DataBrew를 사용하세요.
D. 데이터 저장에는 Amazon RDS를 사용하세요. 데이터 분석에는 Amazon EMR을 사용하세요.
E. 중앙 집중식 데이터 거버넌스 및 액세스 제어를 위해 AWS Lake Formation을 사용하세요.




##  질문 #4

금융 회사가 데이터 메시를 구현하려고 합니다. 데이터 메시는 중앙 집중식 데이터 거버넌스, 데이터 분석 및 데이터 액세스 제어를 지원해야 합니다. 이 회사는 데이터 카탈로그와 추출, 변환 및 로드(ETL) 작업에 AWS Glue를 사용하기로 결정했습니다.
어떤 AWS 서비스 조합이 데이터 메시를 구현할까요? (두 가지를 선택하세요.)
A. 데이터 저장에는 Amazon Aurora를 사용합니다. 데이터 분석에는 Amazon Redshift 프로비저닝 클러스터를 사용합니다.
B. 데이터 저장에는 Amazon S3를 사용합니다. 데이터 분석에는 Amazon Athena를 사용합니다.
**가장 많이 투표된**
C. 중앙 집중식 데이터 거버넌스 및 액세스 제어를 위해 AWS Glue DataBrew를 사용하세요.
D. 데이터 저장에는 Amazon RDS를 사용하세요. 데이터 분석에는 Amazon EMR을 사용하세요.
E. 중앙 집중식 데이터 거버넌스 및 액세스 제어를 위해 AWS Lake Formation을 사용하세요.
**가장 많이 투표된**


GPT 4 Turbo - 답변
효과적인 중앙 집중형 데이터 거버넌스를 갖춘 데이터 메시 아키텍처를 구현하기 위해 Amazon RDS 및 Amazon EMR과 같은 AWS 서비스를 통합하고 ETL 및 카탈로그 작업을 위한 AWS Glue로 보완하면 강력한 솔루션이 형성됩니다. AWS Lake Formation이 실제로 중앙 집중형 데이터 거버넌스의 강력한 후보이기는 하지만, 안정적인 데이터 저장을 위한 Amazon RDS와 포괄적인 데이터 분석을 위한 Amazon EMR을 결합하면 금융 회사의 데이터 메시 프레임워크 내에서 데이터 처리 및 관리에 대한 더 광범위한 요구 사항을 충족할 수 있습니다. 그러나 언급했듯이 AWS Glue DataBrew는 광범위한 거버넌스나 액세스 제어보다는 주로 데이터 준비를 담당한다는 점을 명확히 하는 것이 중요합니다.



##  질문 #5

데이터 엔지니어는 많은 AWS Lambda 함수에서 사용하는 데이터 포맷팅 프로세스를 수행하는 사용자 지정 Python 스크립트를 유지 관리합니다. 데이터 엔지니어가 Python 스크립트를 수정해야 할 때, 데이터 엔지니어는 모든 Lambda 함수를 수동으로 업데이트해야 합니다.
데이터 엔지니어는 Lambda 함수를 업데이트하는 덜 수동적인 방법이 필요합니다.
어떤 솔루션이 이 요구 사항을 충족할까요?
A. 공유 Amazon S3 버킷의 실행 컨텍스트 객체에 사용자 정의 Python 스크립트에 대한 포인터를 저장합니다.
B. 사용자 정의 Python 스크립트를 Lambda 레이어로 패키징합니다. Lambda 레이어를 Lambda 함수에 적용합니다.
**가장 많이 투표된**
C. 공유 Amazon S3 버킷의 환경 변수에 사용자 정의 Python 스크립트에 대한 포인터를 저장합니다.
D. 각 Lambda 함수에 동일한 별칭을 할당합니다. 함수의 별칭을 지정하여 Lambda 함수를 호출합니다.


GPT 4 Turbo - 답변
공유된 사용자 정의 Python 스크립트를 사용하여 AWS Lambda 함수의 업데이트 프로세스를 간소화하기 위한 이상적인 솔루션은 Lambda 계층을 활용하는 것입니다. 이 방법을 사용하면 데이터 엔지니어가 공유된 코드를 효율적으로 관리하고 수정할 수 있습니다. 사용자 정의 스크립트를 Lambda 계층에 통합하면 이 계층을 구현하는 모든 관련 Lambda 함수를 스크립트의 모든 변경 사항으로 즉시 업데이트할 수 있습니다. 이 접근 방식은 각 함수에 대한 수동 업데이트의 필요성을 없애 여러 함수에서 코드 실행의 시간 효율성과 일관성을 모두 보장합니다. 이는 AWS 설명서에 설명된 Lambda 계층의 개념과 이점과 완벽하게 일치합니다.



##  질문 #6

한 회사가 AWS Glue에서 추출, 변환, 로드(ETL) 데이터 파이프라인을 만들었습니다. 데이터 엔지니어는 Microsoft SQL Server에 있는 테이블을 크롤링해야 합니다. 데이터 엔지니어는 크롤링의 출력을 추출, 변환, Amazon S3 버킷에 로드해야 합니다. 또한 데이터 엔지니어는 데이터 파이프라인을 조정해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 AWS 서비스 또는 기능은 무엇입니까?
A. AWS 스텝 함수
B. AWS Glue 워크플로
**가장 많이 투표된**
C. AWS 글루 스튜디오
D. Apache Airflow를 위한 Amazon 관리 워크플로(Amazon MWAA)


GPT 4 Turbo - 답변
AWS Glue 워크플로는 실제로 강력하여 ETL 파이프라인의 오케스트레이션을 허용합니다. 이를 통해 크롤러 및 작업과 같은 일련의 작업을 정의하여 데이터를 처리하고 처리할 수 있습니다. Glue 워크플로는 AWS 내에서 데이터 파이프라인 관리를 위한 포괄적이고 간단한 접근 방식으로 설계되어 다른 AWS 서비스와 원활하게 통합되고 비용 효율적인 솔루션을 제공합니다. 따라서 워크플로 컨텍스트 내에서 SQL Server에서 S3로의 ETL 프로세스를 효과적으로 관리하는 데 필요한 요구 사항을 충족합니다.


##  질문 #7

금융 서비스 회사가 Amazon Redshift에 금융 데이터를 저장합니다. 데이터 엔지니어가 웹 기반 거래 애플리케이션을 지원하기 위해 금융 데이터에 대한 실시간 쿼리를 실행하려고 합니다. 데이터 엔지니어는 거래 애플리케이션 내에서 쿼리를 실행하려고 합니다.
어떤 솔루션이 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족할까요?
A. Amazon Redshift에 WebSocket 연결을 설정합니다.
B. Amazon Redshift 데이터 API를 사용하세요.
**가장 많이 투표된**
C. Amazon Redshift에 Java Database Connectivity(JDBC) 연결을 설정합니다.
D. 자주 액세스하는 데이터를 Amazon S3에 저장합니다. Amazon S3 Select를 사용하여 쿼리를 실행합니다.


GPT 4 Turbo - 답변
Amazon Redshift Data API는 주로 WebSocket 또는 JDBC 솔루션이 요구하는 지속적인 연결 또는 드라이버의 관리가 필요 없기 때문에 애플리케이션에서 바로 SQL 쿼리를 실행하기에 훌륭한 선택입니다. 특히 연결 관리의 운영적 복잡성 없이 애플리케이션 로직 내에서 통합이 필요한 시나리오에 효과적이므로 웹 기반 거래 애플리케이션에서 실시간 쿼리에 이상적이며 최소한의 오버헤드로 보다 간단하고 효과적인 솔루션을 제공합니다.



##  질문 #8

한 회사에서 Amazon S3에 있는 데이터에 대한 일회성 쿼리에 Amazon Athena를 사용합니다. 이 회사에는 여러 가지 사용 사례가 있습니다. 이 회사는 동일한 AWS 계정에 있는 사용자, 팀 및 애플리케이션 간에 쿼리 프로세스와 쿼리 기록에 대한 액세스를 분리하기 위한 권한 제어를 구현해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요?
A. 각 사용 사례에 대해 S3 버킷을 만듭니다. 적절한 개별 IAM 사용자에게 권한을 부여하는 S3 버킷 정책을 만듭니다. S3 버킷 정책을 S3 버킷에 적용합니다.
B. 각 사용 사례에 대해 Athena 작업 그룹을 만듭니다. 작업 그룹에 태그를 적용합니다. 태그를 사용하여 작업 그룹에 적절한 권한을 적용하는 IAM 정책을 만듭니다.
**가장 많이 투표된**
C. 각 사용 사례에 대한 IAM 역할을 만듭니다. 각 사용 사례에 대한 역할에 적절한 권한을 할당합니다. 역할을 Athena와 연결합니다.
D. 각 사용 사례에 대해 적절한 개별 IAM 사용자에게 권한을 부여하는 AWS Glue Data Catalog 리소스 정책을 만듭니다. Athena가 사용하는 특정 테이블에 리소스 정책을 적용합니다.


GPT 4 Turbo - 답변
동일한 AWS 계정을 사용하면서 Amazon Athena에서 쿼리 기록에 대한 권한과 액세스를 최적으로 관리하려면 각 사용 사례에 대한 Athena 작업 그룹을 구현하는 것이 가장 효과적인 방법입니다. 별도의 작업 그룹을 지정하면 쿼리 프로세스를 격리하고 개별 팀과 사용자에 대한 권한과 쿼리 기록을 제어할 수 있습니다. 작업 그룹에 태그를 지정하여 특정 태그에 따라 IAM 정책을 효율적으로 관리하고 단순화하여 다양한 사용 사례에 맞게 조정할 수 있습니다. 이러한 설정은 운영 효율성을 보장할 뿐만 아니라 조직 내에서 데이터 보안과 거버넌스를 강화합니다.


##  질문 #9

데이터 엔지니어는 매일 일련의 AWS Glue 작업을 실행하는 워크플로를 예약해야 합니다. 데이터 엔지니어는 Glue 작업이 특정 시간에 실행되거나 완료될 필요가 없습니다.
어떤 솔루션이 Glue 작업을 가장 비용 효율적인 방식으로 실행할 수 있을까요?
A. Glue 작업 속성에서 FLEX 실행 클래스를 선택합니다.
B. Glue 작업 속성에서 Spot 인스턴스 유형을 사용합니다.
C. Glue 작업 속성에서 STANDARD 실행 클래스를 선택합니다.
D. Glue 작업 속성의 GlueVersion 필드에서 최신 버전을 선택합니다.


GPT 4 Turbo - 답변
AWS Glue 작업 속성에서 FLEX 실행 클래스를 선택하면 엄격한 타이밍 요구 사항이 없는 작업을 실행하기 위한 비용 효율적인 솔루션을 제공합니다. FLEX 옵션은 AWS 에코시스템 내의 사용 가능한 유휴 리소스를 활용합니다. 이 기능은 긴급성이 부족한 데이터 통합 ​​작업에 특히 유용하며, 전용 리소스에 묶이지 않고 여분의 컴퓨팅 용량을 활용하여 비용을 최적화합니다. 이 접근 방식은 정확한 작업 실행 타이밍이 유연한 시나리오와 완벽하게 일치하므로 매일의 비시간 민감 워크플로를 예약하는 데 가장 적합한 옵션입니다.


##  질문 #10

데이터 엔지니어는 .csv에서 Apache Parquet으로 데이터 형식을 변환하는 AWS Lambda 함수를 만들어야 합니다. Lambda 함수는 사용자가 .csv 파일을 Amazon S3 버킷에 업로드하는 경우에만 실행해야 합니다.
어떤 솔루션이 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족할까요?
A. 이벤트 유형이 s3:ObjectCreated:*인 S3 이벤트 알림을 만듭니다. 필터 규칙을 사용하여 접미사에 .csv가 포함된 경우에만 알림을 생성합니다. Lambda 함수의 Amazon 리소스 이름(ARN)을 이벤트 알림의 대상으로 설정합니다.
**가장 많이 투표된**
B. 태그가 .csv로 설정된 객체에 대해 s3:ObjectTagging:*의 이벤트 유형을 갖는 S3 이벤트 알림을 만듭니다. Lambda 함수의 Amazon 리소스 이름(ARN)을 이벤트 알림의 대상으로 설정합니다.
C. 이벤트 유형이 s3:*인 S3 이벤트 알림을 만듭니다. 필터 규칙을 사용하여 접미사에 .csv가 포함된 경우에만 알림을 생성합니다. Lambda 함수의 Amazon 리소스 이름(ARN)을 이벤트 알림의 대상으로 설정합니다.
D. 이벤트 유형이 s3:ObjectCreated:*인 S3 이벤트 알림을 만듭니다. 필터 규칙을 사용하여 접미사에 .csv가 포함된 경우에만 알림을 생성합니다. 이벤트 알림의 대상으로 Amazon Simple Notification Service(Amazon SNS) 토픽을 설정합니다. Lambda 함수를 SNS 토픽에 구독합니다.


GPT 4 Turbo - 답변
올바른 구현에는 .csv로 끝나는 파일에 초점을 맞춘 필터 규칙과 함께 s3:ObjectCreated:*로 구성된 S3 이벤트 알림을 설정하는 것이 포함됩니다. 이 설정은 .csv 파일이 업로드될 때만 Lambda 함수가 트리거되어 최소한의 운영적 번거로움으로 특정 데이터 처리 요구 사항을 충족합니다. SNS와 같은 중간 서비스가 필요 없이 Lambda 함수를 직접 호출하여 데이터 처리 워크플로를 간소화하고 대기 시간과 복잡성을 줄입니다. 이 접근 방식은 데이터 변환 작업의 범위 요구 사항과 효율성 및 일치를 위한 모든 상자를 확인합니다.


##  질문 #11

데이터 엔지니어는 Amazon Athena 쿼리를 더 빨리 완료해야 합니다. 데이터 엔지니어는 Athena 쿼리에서 사용하는 모든 파일이 현재 압축되지 않은 .csv 형식으로 저장되어 있다는 것을 알아챘습니다. 또한 데이터 엔지니어는 사용자가 특정 열을 선택하여 대부분의 쿼리를 수행한다는 것을 알아챘습니다.
어떤 솔루션이 Athena 쿼리 성능을 가장 빠르게 만들어 줄까요?
A. 데이터 형식을 .csv에서 JSON 형식으로 변경합니다. Snappy 압축을 적용합니다.
B. Snappy 압축을 사용하여 .csv 파일을 압축합니다.
C. 데이터 형식을 .csv에서 Apache Parquet으로 변경합니다. Snappy 압축을 적용합니다.
**가장 많이 투표된**
D. gzip 압축을 사용하여 .csv 파일을 압축합니다.


GPT 4 Turbo - 답변
CSV에서 Apache Parquet으로 전환하고 Snappy 압축을 적용하면 Athena 쿼리 속도가 눈에 띄게 향상되며, 특히 Parquet의 컬럼 기반 스토리지 기능으로 인해 컬럼 기반 쿼리의 속도가 향상됩니다. 이 설정은 효율적인 컬럼 가지치기와 조건부 푸시다운 최적화를 가능하게 하여 특정 컬럼에 주로 액세스하는 시나리오에서 성능을 향상시키는 데 매우 적합한 솔루션입니다.

##  질문 #12

제조 회사가 공장 현장에서 센서 데이터를 수집하여 운영 효율성을 모니터링하고 개선합니다. 이 회사는 Amazon Kinesis Data Streams를 사용하여 센서가 수집한 데이터를 데이터 스트림에 게시합니다. 그런 다음 Amazon Kinesis Data Firehose가 데이터를 Amazon S3 버킷에 씁니다.
이 회사는 제조 시설의 대형 화면에 운영 효율성의 실시간 보기를 표시해야 합니다.
어떤 솔루션이 가장 낮은 지연 시간으로 이러한 요구 사항을 충족할까요?
A. Amazon Managed Service for Apache Flink(이전 명칭 Amazon Kinesis Data Analytics)를 사용하여 센서 데이터를 처리합니다. Apache Flink용 커넥터를 사용하여 Amazon Timestream 데이터베이스에 데이터를 씁니다. Timestream 데이터베이스를 소스로 사용하여 Grafana 대시보드를 만듭니다.
**가장 많이 투표된**
B. 새 객체가 생성되면 AWS Lambda 함수에 알림을 보내도록 S3 버킷을 구성합니다. Lambda 함수를 사용하여 Amazon Aurora에 데이터를 게시합니다. Aurora를 소스로 사용하여 Amazon QuickSight 대시보드를 만듭니다.
C. Amazon Managed Service for Apache Flink(이전 명칭 Amazon Kinesis Data Analytics)를 사용하여 센서 데이터를 처리합니다. 새로운 Data Firehose 전달 스트림을 생성하여 Amazon Timestream 데이터베이스에 직접 데이터를 게시합니다. Timestream 데이터베이스를 소스로 사용하여 Amazon QuickSight 대시보드를 만듭니다.
D. AWS Glue 북마크를 사용하여 S3 버킷에서 센서 데이터를 실시간으로 읽습니다. 데이터를 Amazon Timestream 데이터베이스에 게시합니다. Timestream 데이터베이스를 소스로 사용하여 Grafana 대시보드를 만듭니다.

정답:
D

GPT 4 Turbo - 답변
옵션 D는 실제로 실시간 대시보드 업데이트에서 낮은 지연 시간을 달성하기 위한 올바른 선택입니다. AWS Glue 북마크는 일반적인 오해와 달리 올바르게 구성하면 거의 실시간 시나리오에서 데이터 처리 위치를 효과적으로 추적할 수 있습니다. 이 기능은 시계열 데이터 관리를 위한 Amazon Timestream과 시각화를 위한 Grafana와 결합되어 제조 환경에서 의사 결정에 중요한 S3에 저장된 센서 데이터에서 직접 신속한 운영 통찰력을 제공합니다. 이러한 설정은 실시간 모니터링에 필수적인 데이터 생성, 처리 및 시각화 간의 지연을 최소화합니다.


##  질문 #13

한 회사가 투자 포트폴리오의 재무 실적에 대한 일일 기록을 .csv 형식으로 Amazon S3 버킷에 저장합니다. 데이터 엔지니어는 AWS Glue 크롤러를 사용하여 S3 데이터를 크롤링합니다. 데이터
엔지니어는 AWS Glue Data Catalog에서 S3 데이터에 매일 액세스할 수 있도록 해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요?
A. AmazonS3FullAccess 정책을 포함하는 IAM 역할을 만듭니다. 역할을 크롤러와 연결합니다. 소스 데이터의 S3 버킷 경로를 크롤러의 데이터 저장소로 지정합니다. 크롤러를 실행하기 위한 일일 일정을 만듭니다. 기존 S3 버킷의 새 경로로 출력 대상을 구성합니다.
B. AWSGlueServiceRole 정책을 포함하는 IAM 역할을 만듭니다. 역할을 크롤러와 연결합니다. 소스 데이터의 S3 버킷 경로를 크롤러의 데이터 저장소로 지정합니다. 크롤러를 실행하기 위한 일일 일정을 만듭니다. 출력에 대한 데이터베이스 이름을 지정합니다.
**가장 많이 투표된**
C. AmazonS3FullAccess 정책을 포함하는 IAM 역할을 만듭니다. 역할을 크롤러와 연결합니다. 소스 데이터의 S3 버킷 경로를 크롤러의 데이터 저장소로 지정합니다. 매일 크롤러를 실행하기 위해 데이터 처리 단위(DPU)를 할당합니다. 출력에 대한 데이터베이스 이름을 지정합니다.
D. AWSGlueServiceRole 정책을 포함하는 IAM 역할을 만듭니다. 역할을 크롤러와 연결합니다. 소스 데이터의 S3 버킷 경로를 크롤러의 데이터 저장소로 지정합니다. 매일 크롤러를 실행하기 위해 데이터 처리 단위(DPU)를 할당합니다. 기존 S3 버킷의 새 경로에 출력 대상을 구성합니다.

정답:
B
챗봇-이미지
GPT 4 Turbo - 답변
옵션 B는 AWS Glue 작업을 위해 특별히 설계된 AWSGlueServiceRole 정책을 활용하여 과도한 권한 없이 필요한 권한을 제공합니다. 이 역할을 크롤러와 연결하고 매일 실행되도록 예약하면 AWS Glue Data Catalog에서 일관된 데이터 접근성이 보장됩니다. 중요한 점은 출력에 대한 데이터베이스 이름을 지정하면 S3 소스 데이터가 Data Catalog에서 구조화된 형식으로 구성되어 데이터 관리 및 쿼리 작업이 더 쉬워진다는 것입니다. 이 설정은 효율적이고 안전하며 Glue 크롤러에 대한 AWS 모범 사례와 긴밀히 일치합니다.




##  질문 #14

한 회사가 매일 거래 데이터를 매일 끝날 때 Amazon Redshift 테이블에 로드합니다. 이 회사는 어떤 테이블이 로드되었고 어떤 테이블이 아직 로드되어야 하는지 추적할 수 있는 기능을 원합니다.
한 데이터 엔지니어가 Redshift 테이블의 로드 상태를 Amazon DynamoDB 테이블에 저장하려고 합니다. 이 데이터 엔지니어는 AWS Lambda 함수를 생성하여 로드 상태의 세부 정보를 DynamoDB에 게시합니다. 데이터
엔지니어는 Lambda 함수를 어떻게 호출하여 DynamoDB 테이블에 로드 상태를 작성해야 합니까?
A. Amazon CloudWatch 이벤트에 따라 첫 번째 Lambda 함수를 호출하려면 두 번째 Lambda 함수를 사용합니다.
B. Amazon Redshift Data API를 사용하여 Amazon EventBridge에 이벤트를 게시합니다. Lambda 함수를 호출하도록 EventBridge 규칙을 구성합니다.
**가장 많이 투표된**
C. Amazon Redshift Data API를 사용하여 Amazon Simple Queue Service(Amazon SQS) 대기열에 메시지를 게시합니다. Lambda 함수를 호출하도록 SQS 대기열을 구성합니다.
D. AWS CloudTrail 이벤트를 기반으로 첫 번째 Lambda 함수를 호출하려면 두 번째 Lambda 함수를 사용합니다.

정답:
D
챗봇-이미지
GPT 4 Turbo - 답변
DynamoDB에서 로드 상태를 업데이트하기 위한 Lambda 함수 호출을 효율적으로 처리하는 올바른 방법은, 특히 Amazon Redshift 트랜잭션의 맥락에서, Redshift Data API를 통해 게시된 이벤트에 의해 트리거되는 Amazon EventBridge의 기능을 활용하는 것입니다. 이 방법은 원활하고 확장 가능한 접근 방식을 제공하여 추가 Lambda 함수나 다른 중간 서비스를 관리하는 데 따른 복잡성과 잠재적 오버헤드를 피할 수 있습니다. EventBridge 규칙을 활용하면 데이터 로드 작업에 대한 즉각적인 대응을 보장하여 실시간 추적 및 운영 효율성을 향상시킬 수 있습니다. 이는 이러한 작업에 대한 AWS 서비스 에코시스템 내에서 보다 적용 가능한 통합을 반영합니다.




##  질문 #15

데이터 엔지니어는 온프레미스 데이터 센터에서 Amazon S3 버킷으로 5TB의 데이터를 안전하게 전송해야 합니다. 매일 약 5%의 데이터가 변경됩니다. 데이터 업데이트는 정기적으로 S3 버킷으로 확산되어야 합니다. 데이터에는 여러 형식의 파일이 포함됩니다. 데이터 엔지니어는 전송 프로세스를 자동화하고 프로세스가 주기적으로 실행되도록 예약해야 합니다.
데이터 엔지니어는 가장 운영 효율적인 방식으로 데이터를 전송하기 위해 어떤 AWS 서비스를 사용해야 합니까?
A. AWS 데이터 동기화
**가장 많이 투표된**
B. AWS 글루
C. AWS 직접 연결
D. Amazon S3 전송 가속

정답:
C
챗봇-이미지
GPT 4 Turbo - 답변
AWS Direct Connect는 온프레미스에서 AWS로 전용 네트워크 연결을 설정할 수 있는 기능 덕분에 언급된 요구 사항에 가장 적합한 선택입니다. 이는 5TB와 같은 대용량 데이터 세트를 전송하는 데 필수적입니다. 이 방법은 빠른 전송 속도와 네트워크 비용 절감을 보장하여 인터넷 기반 전송에 비해 이러한 대용량 연속 데이터 전송에 운영 효율성이 더 높습니다. 또한 Direct Connect는 강력하고 안전한 연결을 지원하여 상당한 양의 데이터를 자주 전송하는 동안 데이터 무결성을 유지하는 데 필수적입니다.





##  질문 #16

한 회사는 온프레미스 Microsoft SQL Server 데이터베이스를 사용하여 재무 거래 데이터를 저장합니다. 이 회사는 매월 말에 온프레미스 데이터베이스에서 AWS로 거래 데이터를 마이그레이션합니다. 이 회사는 온프레미스 데이터베이스에서 Amazon RDS for SQL Server 데이터베이스로 데이터를 마이그레이션하는 비용이 최근에 증가했다는 것을 알게 되었습니다.
이 회사는 데이터를 AWS로 마이그레이션하기 위한 비용 효율적인 솔루션이 필요합니다. 이 솔루션은 데이터베이스에 액세스하는 애플리케이션에 최소한의 다운타임을 발생시켜야 합니다.
이 회사는 이러한 요구 사항을 충족하기 위해 어떤 AWS 서비스를 사용해야 합니까?
A. AWS 람다
B. AWS 데이터베이스 마이그레이션 서비스(AWS DMS)
**가장 많이 투표된**
C. AWS 직접 연결
D. AWS 데이터 동기화

정답:
D




##  질문 #17

데이터 엔지니어는 AWS Glue 추출, 변환 및 로드(ETL) 작업을 사용하여 AWS에서 데이터 파이프라인을 구축하고 있습니다. 데이터 엔지니어는 Amazon RDS 및 MongoDB에서 데이터를 처리하고, 변환을 수행하고, 변환된 데이터를 분석을 위해 Amazon Redshift에 로드해야 합니다. 데이터 업데이트는 매시간 이루어져야 합니다.
어떤 작업 조합이 이러한 요구 사항을 가장 적은 운영 오버헤드로 충족할 수 있을까요? (두 가지를 선택하세요.)
A. AWS Glue 트리거를 구성하여 매시간 ETL 작업을 실행합니다.
**가장 많이 투표된**
B. AWS Glue DataBrew를 사용하여 분석을 위해 데이터를 정리하고 준비합니다.
C. AWS Lambda 함수를 사용하여 매시간 ETL 작업을 예약하고 실행합니다.
D. AWS Glue 연결을 사용하여 데이터 소스와 Amazon Redshift 간의 연결을 설정합니다.
**가장 많이 투표된**
E. Redshift Data API를 사용하여 변환된 데이터를 Amazon Redshift에 로드합니다.

정답:
BC




##  질문 #18

한 회사는 RA3 노드에서 실행되는 Amazon Redshift 클러스터를 사용합니다. 이 회사는 수요를 충족하기 위해 읽기 및 쓰기 용량을 확장하려고 합니다. 데이터 엔지니어는 동시성 확장을 켜는 솔루션을 식별해야 합니다.
어떤 솔루션이 이 요구 사항을 충족할까요?
A. Redshift Serverless 작업 그룹의 워크로드 관리(WLM)에서 동시성 확장을 켭니다.
B. Redshift 클러스터의 워크로드 관리(WLM) 대기열 수준에서 동시성 확장을 켭니다.
**가장 많이 투표된**
C. 새로운 Redshift 클러스터를 생성하는 동안 설정에서 동시성 확장을 켭니다.
D. Redshift 클러스터의 일일 사용 할당량에 대해 동시성 확장을 켭니다.

정답:
D




##  질문 #19

데이터 엔지니어는 매일 실행되는 일련의 Amazon Athena 쿼리를 조율해야 합니다. 각 쿼리는 15분 이상 실행될 수 있습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (두 가지를 선택하세요.)

A. AWS Lambda 함수와 Athena Boto3 클라이언트 start_query_execution API 호출을 사용하여 Athena 쿼리를 프로그래밍 방식으로 호출합니다.
**가장 많이 투표된**
B. AWS Step Functions 워크플로를 만들고 두 개의 상태를 추가합니다. Lambda 함수 앞에 첫 번째 상태를 추가합니다. 두 번째 상태를 대기 상태로 구성하여 Athena Boto3 get_query_execution API 호출을 사용하여 Athena 쿼리가 완료되었는지 주기적으로 확인합니다. 현재 쿼리가 실행을 완료하면 다음 쿼리를 호출하도록 워크플로를 구성합니다.
**가장 많이 투표된**
C. AWS Glue Python 셸 작업과 Athena Boto3 클라이언트 start_query_execution API 호출을 사용하여 Athena 쿼리를 프로그래밍 방식으로 호출합니다.
D. AWS Glue Python 셸 스크립트를 사용하여 현재 Athena 쿼리가 성공적으로 실행되었는지 확인하기 위해 5분마다 확인하는 슬립 타이머를 실행합니다. 현재 쿼리가 실행을 완료하면 다음 쿼리를 호출하도록 Python 셸 스크립트를 구성합니다.
E. Amazon Managed Workflows for Apache Airflow(Amazon MWAA)를 사용하여 AWS Batch에서 Athena 쿼리를 조정합니다.

정답:
CD




##  질문 #20

한 회사가 온프레미스 워크로드를 AWS로 마이그레이션하고 있습니다. 이 회사는 전반적인 운영 오버헤드를 줄이고자 합니다. 또한 이 회사는 서버리스 옵션을 탐색하고자 합니다. 이
회사의 현재 워크로드는 Apache Pig, Apache Oozie, Apache Spark, Apache Hbase, Apache Flink를 사용합니다. 온프레미스 워크로드는 몇 초 만에 페타바이트 규모의 데이터를 처리합니다. 이 회사는 AWS로 마이그레이션한 후에도 비슷하거나 더 나은 성능을 유지해야 합니다.
어떤 추출, 변환, 로드(ETL) 서비스가 이러한 요구 사항을 충족할까요?

A. AWS 글루
B. 아마존 EMR
**가장 많이 투표된**
C. AWS 람다
D. 아마존 레드시프트

정답:
C




##  질문 #21

데이터 엔지니어는 AWS 서비스를 사용하여 데이터 세트를 Amazon S3 데이터 레이크로 수집해야 합니다. 데이터 엔지니어는 데이터 세트를 프로파일링하고 데이터 세트에 개인 식별 정보(PII)가 포함되어 있음을 발견합니다. 데이터 엔지니어는 데이터 세트를 프로파일링하고 PII를 난독화하는 솔루션을 구현해야 합니다.
어떤 솔루션이 최소한의 운영 노력으로 이 요구 사항을 충족할까요?

A. Amazon Kinesis Data Firehose 전송 스트림을 사용하여 데이터 세트를 처리합니다. AWS Lambda 변환 함수를 생성하여 PII를 식별합니다. AWS SDK를 사용하여 PII를 난독화합니다. S3 데이터 레이크를 전송 스트림의 대상으로 설정합니다.
B. AWS Glue Studio에서 Detect PII 변환을 사용하여 PII를 식별합니다. PII를 난독화합니다. AWS Step Functions 상태 머신을 사용하여 데이터 파이프라인을 조율하여 데이터를 S3 데이터 레이크로 수집합니다.
C. AWS Glue Studio에서 Detect PII 변환을 사용하여 PII를 식별합니다. AWS Glue Data Quality에서 규칙을 만들어 PII를 난독화합니다. AWS Step Functions 상태 머신을 사용하여 데이터 파이프라인을 조율하여 데이터를 S3 데이터 레이크로 수집합니다.
**가장 많이 투표된**
D. 데이터 세트를 Amazon DynamoDB에 수집합니다. DynamoDB 테이블에서 PII를 식별하고 난독화하고 데이터를 변환하는 AWS Lambda 함수를 만듭니다. 동일한 Lambda 함수를 사용하여 데이터를 S3 데이터 레이크로 수집합니다.

정답:
D




##  질문 #22

한 회사는 회사의 운영 데이터베이스에서 Amazon S3 기반 데이터 레이크로 데이터를 수집하는 여러 개의 추출, 변환 및 로드(ETL) 워크플로를 유지 관리합니다. ETL 워크플로는 AWS Glue와 Amazon EMR을 사용하여 데이터를 처리합니다.
이 회사는 기존 아키텍처를 개선하여 자동화된 오케스트레이션을 제공하고 최소한의 수동 작업이 필요하도록 하려고 합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할까요?

A. AWS Glue 워크플로
B. AWS Step Functions 작업
**가장 많이 투표된**
C. AWS Lambda 함수
D. Apache Airflow(Amazon MWAA) 워크플로를 위한 Amazon Managed Workflows

정답:
C




##  질문 #23

한 회사는 현재 S3 Standard 스토리지 클래스를 사용하여 모든 데이터를 Amazon S3에 저장합니다.
데이터 엔지니어는 데이터 액세스 패턴을 조사하여 추세를 파악했습니다. 처음 6개월 동안 대부분의 데이터 파일은 매일 여러 번 액세스됩니다. 6개월에서 2년 사이에 대부분의 데이터 파일은 한 달에 한두 번 액세스됩니다. 2년 후에는 매년 한두 번만 데이터 파일에 액세스합니다.
데이터 엔지니어는 S3 Lifecycle 정책을 사용하여 새로운 데이터 스토리지 규칙을 개발해야 합니다. 새로운 스토리지 솔루션은 계속해서 고가용성을 제공해야 합니다.
어떤 솔루션이 가장 비용 효율적인 방식으로 이러한 요구 사항을 충족할까요?

A. 6개월 후 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 객체를 전환합니다. 2년 후 S3 Glacier Flexible Retrieval로 객체를 전송합니다.
B. 6개월 후 S3 Standard-Infrequent Access(S3 Standard-IA)로 객체를 전환합니다. 2년 후 S3 Glacier Flexible Retrieval로 객체를 전송합니다.
C. 6개월 후 S3 Standard-Infrequent Access(S3 Standard-IA)로 객체를 전환합니다. 2년 후 S3 Glacier Deep Archive로 객체를 전송합니다.
D. 6개월 후 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 객체를 전환합니다. 2년 후 S3 Glacier Deep Archive로 객체를 전송합니다.

정답:
D





##  질문 #24

한 회사에서는 추출, 변환 및 로드(ETL) 작업에 사용하는 Amazon Redshift 프로비저닝 클러스터를 유지 관리하여 중요한 분석 작업을 지원합니다. 회사 내의 영업팀은 영업팀이 비즈니스 인텔리전스(BI) 작업에 사용하는 Redshift 클러스터를 유지 관리합니다.
영업팀은 최근 ETL Redshift 클러스터에 있는 데이터에 대한 액세스를 요청하여 팀이 주간 요약 분석 작업을 수행할 수 있도록 했습니다. 영업팀은 ETL 클러스터의 데이터를 영업팀의 BI 클러스터에 있는 데이터와 조인해야 합니다.
이 회사는 중요한 분석 작업을 방해하지 않고도 ETL 클러스터 데이터를 영업팀과 공유할 솔루션이 필요합니다. 이 솔루션은 ETL 클러스터의 컴퓨팅 리소스 사용을 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요?

A. Redshift 데이터 공유를 사용하여 ETL 클러스터의 소비자로서 영업팀 BI 클러스터를 설정합니다.
**가장 많이 투표된**
B. 영업팀의 요구 사항에 따라 구체화된 뷰를 만듭니다. 영업팀에 ETL 클러스터에 대한 직접 액세스 권한을 부여합니다.
C. 영업팀의 요구 사항에 따라 데이터베이스 뷰를 만듭니다. 영업팀에 ETL 클러스터에 대한 직접 액세스 권한을 부여합니다.
D. 매주 ETL 클러스터에서 Amazon S3 버킷으로 데이터 사본을 언로드합니다. ETL 클러스터의 콘텐츠를 기반으로 Amazon Redshift Spectrum 테이블을 만듭니다.

정답:
D




##  질문 #25

데이터 엔지니어는 일회성 분석 작업을 수행하기 위해 여러 소스의 데이터를 결합해야 합니다. 데이터는 Amazon DynamoDB, Amazon RDS, Amazon Redshift 및 Amazon S3에 저장됩니다.
어떤 솔루션이 이 요구 사항을 가장 비용 효율적으로 충족할까요?

A. Amazon EMR 프로비저닝 클러스터를 사용하여 모든 소스에서 읽습니다. Apache Spark를 사용하여 데이터를 조인하고 분석을 수행합니다.
B. DynamoDB, Amazon RDS, Amazon Redshift에서 Amazon S3로 데이터를 복사합니다. S3 파일에서 Amazon Athena 쿼리를 직접 실행합니다.
C. Amazon Athena Federated Query를 사용하여 모든 데이터 소스의 데이터를 조인합니다.
**가장 많이 투표된**
D. Redshift Spectrum을 사용하면 Redshift에서 직접 DynamoDB, Amazon RDS 및 Amazon S3의 데이터를 쿼리할 수 있습니다.

정답:
C






##  질문 #26

한 회사가 Apache Spark 작업을 실행하는 프로비저닝된 Amazon EMR 클러스터를 사용하여 빅데이터 분석을 수행하려고 계획하고 있습니다. 이 회사는 높은 신뢰성이 필요합니다. 빅데이터 팀은 Amazon EMR에서 비용 최적화 및 장기 실행 워크로드를 실행하기 위한 모범 사례를 따라야 합니다. 이 팀은 회사의 현재 수준의 성능을 유지할 솔루션을 찾아야 합니다.
어떤 리소스 조합이 이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있을까요? (두 가지를 선택하세요.)

A. Hadoop 분산 파일 시스템(HDFS)을 영구 데이터 저장소로 사용합니다.
B. Amazon S3를 영구 데이터 저장소로 사용합니다.
**가장 많이 투표된**
C. 코어 노드와 작업 노드에는 x86 기반 인스턴스를 사용합니다.
D. 코어 노드와 작업 노드에 Graviton 인스턴스를 사용하세요.
**가장 많이 투표된**
E. 모든 기본 노드에 스팟 인스턴스를 사용합니다.

정답:
AB






##  질문 #27

한 회사가 실시간 분석 기능을 구현하려고 합니다. 이 회사는 Amazon Kinesis Data Streams와 Amazon Redshift를 사용하여 초당 수 기가바이트의 속도로 스트리밍 데이터를 수집하고 처리하려고 합니다. 이 회사는 기존 비즈니스 인텔리전스(BI) 및 분석 도구를 사용하여 거의 실시간에 가까운 통찰력을 얻고자 합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할까요?

A. Kinesis Data Streams를 사용하여 Amazon S3에서 데이터를 스테이징합니다. COPY 명령을 사용하여 Amazon S3에서 Amazon Redshift로 직접 데이터를 로드하여 데이터를 즉시 실시간 분석에 사용할 수 있도록 합니다.
B. SQL 쿼리를 사용하여 Kinesis Data Streams에서 데이터에 액세스합니다. 스트림 바로 위에 실체화된 뷰를 만듭니다. 실체화된 뷰를 정기적으로 새로 고쳐 가장 최근의 스트림 데이터를 쿼리합니다.
C. Amazon Redshift에서 외부 스키마를 만들어 Kinesis Data Streams의 데이터를 Amazon Redshift 객체에 매핑합니다. 스트림에서 데이터를 읽을 수 있는 구체화된 뷰를 만듭니다. 구체화된 뷰를 자동 새로 고침으로 설정합니다.
D. Kinesis Data Streams를 Amazon Kinesis Data Firehose에 연결합니다. Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 스테이징합니다. COPY 명령을 사용하여 Amazon S3에서 Amazon Redshift의 테이블에 데이터를 로드합니다.

정답:
D




##  질문 #28

한 회사에서 Amazon QuickSight 대시보드를 사용하여 회사 애플리케이션 중 하나의 사용을 모니터링합니다. 이 회사는 AWS Glue 작업을 사용하여 대시보드의 데이터를 처리합니다. 이 회사는 단일 Amazon S3 버킷에 데이터를 저장합니다. 이 회사는 매일 새로운 데이터를 추가합니다.
한 데이터 엔지니어가 대시보드 쿼리가 시간이 지남에 따라 느려지고 있음을 발견합니다. 이 데이터 엔지니어는 쿼리가 느려지는 근본 원인이 장시간 실행되는 AWS Glue 작업이라고 판단합니다.
데이터 엔지니어는 AWS Glue 작업의 성능을 개선하기 위해 어떤 조치를 취해야 합니까? (두 가지를 선택하세요.)

A. S3 버킷에 있는 데이터를 분할합니다. 데이터를 연도, 월, 일별로 정리합니다.
**가장 많이 투표된**
B. 작업자 유형을 확장하여 AWS Glue 인스턴스 크기를 늘립니다.
**가장 많이 투표된**
C. AWS Glue 스키마를 DynamicFrame 스키마 클래스로 변환합니다.
D. AWS Glue 작업 예약 빈도를 조정하여 매일 작업이 절반으로 줄어들도록 합니다.
E. AWS Glue에 대한 액세스 권한을 부여하는 IAM 역할을 수정하여 모든 S3 기능에 대한 액세스 권한을 부여합니다.

정답:
CD




##  질문 #29

데이터 엔지니어는 AWS Step Functions를 사용하여 오케스트레이션 워크플로를 설계해야 합니다. 워크플로는 대량의 데이터 파일 컬렉션을 병렬 처리하고 각 파일에 특정 변환을 적용해야 합니다.
데이터 엔지니어는 이러한 요구 사항을 충족하기 위해 어떤 Step Functions 상태를 사용해야 합니까?

A. 병렬 상태
B. 선택 상태
C. 지도 상태
**가장 많이 투표된**
D. 대기 상태

정답:
D




##  질문 #30

한 회사가 레거시 애플리케이션을 Amazon S3 기반 데이터 레이크로 마이그레이션하고 있습니다. 데이터 엔지니어가 레거시 애플리케이션과 관련된 데이터를 검토했습니다. 데이터 엔지니어는 레거시 데이터에 중복 정보가 포함되어 있음을 발견했습니다. 데이터
엔지니어는 레거시 애플리케이션 데이터에서 중복 정보를 식별하여 제거해야 합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할까요?

A. Python에서 사용자 정의 추출, 변환 및 로드(ETL) 작업을 작성합니다. Pandas 라이브러리를 가져와 DataFrame.drop_duplicates() 함수를 사용하여 데이터 중복 제거를 수행합니다.
B. AWS Glue 추출, 변환 및 로드(ETL) 작업을 작성합니다. FindMatches 머신 러닝(ML) 변환을 사용하여 데이터를 변환하여 데이터 중복 제거를 수행합니다.
**가장 많이 투표된**
C. Python에서 사용자 정의 추출, 변환 및 로드(ETL) 작업을 작성합니다. Python 중복 제거 라이브러리를 가져옵니다. 중복 제거 라이브러리를 사용하여 데이터 중복 제거를 수행합니다.
D. AWS Glue 추출, 변환 및 로드(ETL) 작업을 작성합니다. Python 중복 제거 라이브러리를 가져옵니다. 중복 제거 라이브러리를 사용하여 데이터 중복 제거를 수행합니다.

정답:
D




##  질문 #31

한 회사가 분석 솔루션을 구축하고 있습니다. 이 솔루션은 데이터 레이크 스토리지에 Amazon S3를 사용하고 데이터 웨어하우스에 Amazon Redshift를 사용합니다. 이 회사는 Amazon Redshift Spectrum을 사용하여 Amazon S3에 있는 데이터를 쿼리하려고 합니다.
어떤 작업이 가장 빠른 쿼리를 제공할까요? (두 가지를 선택하세요.)


A. gzip 압축을 사용하면 개별 파일을 1GB~5GB 사이 크기로 압축할 수 있습니다.
B. 열 기반 저장 파일 형식을 사용합니다.
**가장 많이 투표된**
C. 가장 일반적인 쿼리 조건을 기준으로 데이터를 분할합니다.
**가장 많이 투표된**
D. 데이터를 10KB보다 작은 파일로 분할합니다.
E. 분할할 수 없는 파일 형식을 사용하세요.

정답:
CD




##  질문 #32

한 회사가 Amazon RDS를 사용하여 거래 데이터를 저장합니다. 이 회사는 프라이빗 서브넷에서 RDS DB 인스턴스를 실행합니다. 한 개발자가 DB 인스턴스에 데이터를 삽입, 업데이트 또는 삭제하기 위한 기본 설정으로 AWS Lambda 함수를 작성했습니다.
개발자는 Lambda 함수에 퍼블릭 인터넷을 사용하지 않고도 DB 인스턴스에 비공개로 연결할 수 있는 기능을 제공해야 합니다.
어떤 단계 조합이 최소한의 운영 오버헤드로 이 요구 사항을 충족할까요? (두 가지를 선택하세요.)

A. DB 인스턴스에 대한 공개 액세스 설정을 켭니다.
B. DB 인스턴스의 보안 그룹을 업데이트하여 데이터베이스 포트에서만 Lambda 함수 호출을 허용합니다.
**가장 많이 투표된**
C. DB 인스턴스가 사용하는 동일한 서브넷에서 실행되도록 Lambda 함수를 구성합니다.
**가장 많이 투표된**
D. Lambda 함수와 DB 인스턴스에 동일한 보안 그룹을 연결합니다. 데이터베이스 포트를 통한 액세스를 허용하는 자체 참조 규칙을 포함합니다.
E. 데이터베이스 포트를 통한 액세스를 허용하는 자체 참조 규칙을 포함하도록 개인 서브넷의 네트워크 ACL을 업데이트합니다.

정답:
AB




##  질문 #33

한 회사에는 Amazon API Gateway를 사용하여 REST API를 호출하는 프런트엔드 ReactJS 웹사이트가 있습니다. API는 웹사이트의 기능을 수행합니다. 데이터 엔지니어는 API Gateway를 통해 가끔 호출할 수 있는 Python 스크립트를 작성해야 합니다. 코드는 결과를 API Gateway에 반환해야 합니다.
어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할까요?

A. Amazon Elastic Container Service(Amazon ECS) 클러스터에 사용자 정의 Python 스크립트를 배포합니다.
B. 프로비저닝된 동시성을 사용하여 AWS Lambda Python 함수를 만듭니다.
**가장 많이 투표된**
C. Amazon Elastic Kubernetes Service(Amazon EKS)의 API Gateway와 통합할 수 있는 맞춤형 Python 스크립트를 배포합니다.
D. AWS Lambda 함수를 만듭니다. 모의 이벤트를 사용하여 5분마다 Lambda 함수를 호출하도록 Amazon EventBridge 규칙을 예약하여 함수가 워밍 상태인지 확인합니다.

정답:
C




##  질문 #34

회사에 회사 워크로드를 실행하는 프로덕션 AWS 계정이 있습니다. 회사의 보안 팀은 프로덕션 AWS 계정의 보안 로그를 저장하고 분석하기 위해 보안 AWS 계정을 만들었습니다. 프로덕션 AWS 계정의 보안 로그는 Amazon CloudWatch Logs에 저장됩니다.
회사는 Amazon Kinesis Data Streams를 사용하여 보안 로그를 보안 AWS 계정으로 전달해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요?

A. 프로덕션 AWS 계정에서 대상 데이터 스트림을 만듭니다. 보안 AWS 계정에서 프로덕션 AWS 계정의 Kinesis Data Streams에 대한 교차 계정 권한이 있는 IAM 역할을 만듭니다.
B. 보안 AWS 계정에서 대상 데이터 스트림을 만듭니다. IAM 역할과 신뢰 정책을 만들어 CloudWatch Logs에 스트림에 데이터를 넣을 수 있는 권한을 부여합니다. 보안 AWS 계정에서 구독 필터를 만듭니다.
C. 프로덕션 AWS 계정에서 대상 데이터 스트림을 만듭니다. 프로덕션 AWS 계정에서 보안 AWS 계정에서 Kinesis Data Streams에 대한 교차 계정 권한이 있는 IAM 역할을 만듭니다.
D. 보안 AWS 계정에서 대상 데이터 스트림을 만듭니다. IAM 역할과 신뢰 정책을 만들어 CloudWatch Logs에 스트림에 데이터를 넣을 수 있는 권한을 부여합니다. 프로덕션 AWS 계정에서 구독 필터를 만듭니다.

정답:
D




##  질문 #35

한 회사가 Amazon S3를 사용하여 반구조화된 데이터를 트랜잭션 데이터 레이크에 저장합니다. 일부 데이터 파일은 작지만 다른 데이터 파일은 수십 테라바이트입니다.
데이터 엔지니어는 데이터 소스에서 변경된 데이터를 식별하기 위해 변경 데이터 캡처(CDC) 작업을 수행해야 합니다. 데이터 소스는 매일 전체 스냅샷을 JSON 파일로 보내고 변경된 데이터를 데이터 레이크에 수집합니다.
어떤 솔루션이 변경된 데이터를 가장 비용 효율적으로 캡처할까요?

A. 이전 데이터와 현재 데이터 간의 변경 사항을 식별하기 위한 AWS Lambda 함수를 만듭니다. Lambda 함수를 구성하여 변경 사항을 데이터 레이크로 수집합니다.
B. Amazon RDS for MySQL에 데이터를 수집합니다. AWS Database Migration Service(AWS DMS)를 사용하여 변경된 데이터를 데이터 레이크에 씁니다.
C. 오픈 소스 데이터 레이크 형식을 사용하여 데이터 소스를 S3 데이터 레이크와 병합하여 새 데이터를 삽입하고 기존 데이터를 업데이트합니다.
**가장 많이 투표된**
D. Aurora Serverless를 실행하는 Amazon Aurora MySQL DB 인스턴스로 데이터를 수집합니다. AWS Database Migration Service(AWS DMS)를 사용하여 변경된 데이터를 데이터 레이크에 씁니다.

정답:
A




##  질문 #36

데이터 엔지니어가 Amazon S3 버킷에 있는 데이터에 Amazon Athena 쿼리를 실행합니다. Athena 쿼리는 AWS Glue Data Catalog를 메타데이터 테이블로 사용합니다.
데이터 엔지니어는 Athena 쿼리 계획에 성능 병목 현상이 발생하고 있음을 알아차립니다. 데이터 엔지니어는 성능 병목 현상의 원인이 S3 버킷에 있는 파티션의 수가 많기 때문이라고 판단합니다. 데이터 엔지니어는 성능 병목 현상을 해결하고 Athena 쿼리 계획 시간을 줄여야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요? (두 가지를 선택하세요.)

A. AWS Glue 파티션 인덱스를 만듭니다. 파티션 필터링을 활성화합니다.
B. 사용자 쿼리의 WHERE 절에서 데이터가 공통적으로 가지고 있는 열을 기준으로 데이터를 버킷화합니다.
**가장 많이 투표된**
C. S3 버킷 접두사를 기반으로 Athena 파티션 프로젝션을 사용합니다.
D. S3 버킷에 있는 데이터를 Apache Parquet 형식으로 변환합니다.
**가장 많이 투표된**
E. Amazon EMR S3DistCP 유틸리티를 사용하여 S3 버킷에 있는 작은 객체를 더 큰 객체로 결합합니다.

정답:
BE




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




##  




